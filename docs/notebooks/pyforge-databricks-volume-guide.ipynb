{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# PyForge CLI: Unity Catalog Volume Integration\n\nThis notebook demonstrates using PyForge CLI in Databricks environments with Unity Catalog Volume support, showing how to work with volumes and process data using command-line tools.\n\n## Introduction\n\nPyForge CLI works seamlessly in Databricks environments. This guide covers:\n\n- Installing and using PyForge sample datasets in Volumes\n- Working with Unity Catalog Volumes using CLI commands\n- Direct Volume-to-Volume file conversions\n- Working with multi-table databases in Volumes\n- Batch processing using shell commands\n- CLI integration with Databricks file systems\n\n### Key Features:\n- **Unity Catalog Volumes**: Direct read/write operations on Volume paths using CLI\n- **Shell Integration**: Use %sh magic commands for all file operations  \n- **Multi-format Support**: Convert between CSV, Excel, PDF, XML, Access databases and more\n- **Volume-Native**: Works directly with `/Volumes/` paths\n- **Batch Processing**: Process multiple files efficiently using shell commands",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Install PyForge CLI using Databricks %pip magic\n%pip install \"pyforge-cli\" --quiet\n\n# Restart Python to reload packages\ndbutils.library.restartPython()",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Verify PyForge CLI installation \n%sh\npyforge --version\n\n# Check available PyForge commands\n%sh \npyforge --help\n\n# Display system information\n%sh\necho \"üìç System Information:\"\necho \"======================\"  \npython --version\necho \"üè¢ Databricks Runtime: $(echo $DATABRICKS_RUNTIME_VERSION)\"\necho \"üíæ Available disk space:\"\ndf -h /tmp"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Databricks SDK\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service import catalog\n",
    "\n",
    "# PyForge imports\n",
    "from pyforge_core import PyForgeCore\n",
    "from pyforge_databricks import PyForgeDatabricks\n",
    "\n",
    "# Spark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, current_timestamp, count, sum as spark_sum\n",
    "\n",
    "print(\"üöÄ PyForge Databricks Integration loaded!\")\n",
    "print(f\"üìç Python version: {sys.version.split()[0]}\")\n",
    "print(f\"üè¢ Databricks Runtime: {spark.conf.get('spark.databricks.clusterUsageTags.sparkVersion')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Detection and Validation\n",
    "\n",
    "PyForge Databricks automatically detects your environment and optimizes accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyForge Databricks\n",
    "forge = PyForgeDatabricks()\n",
    "\n",
    "# Get detailed environment information\n",
    "env_info = forge.env.get_environment_info()\n",
    "\n",
    "print(\"üîç Environment Detection Results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display environment details\n",
    "env_details = [\n",
    "    (\"Databricks Environment\", env_info['is_databricks'], \"‚úÖ\" if env_info['is_databricks'] else \"‚ùå\"),\n",
    "    (\"Serverless Compute\", env_info['is_serverless'], \"‚ö°\" if env_info['is_serverless'] else \"üñ•Ô∏è\"),\n",
    "    (\"Environment Version\", env_info['environment_version'], f\"v{env_info['environment_version']}\"),\n",
    "    (\"Python Version\", env_info['python_version'], \"üêç\"),\n",
    "    (\"Spark Version\", env_info['spark_version'], \"‚ú®\"),\n",
    "    (\"Cluster Type\", env_info['cluster_type'], \"üè¢\")\n",
    "]\n",
    "\n",
    "for label, value, icon in env_details:\n",
    "    print(f\"{icon} {label:25}: {value}\")\n",
    "\n",
    "# Validate SDK connection\n",
    "print(\"\\nüîå Validating Databricks SDK connection...\")\n",
    "try:\n",
    "    current_user = forge.w.current_user.me()\n",
    "    print(f\"‚úÖ Connected as: {current_user.display_name}\")\n",
    "    print(f\"üìß Email: {current_user.user_name}\")\n",
    "    sdk_connected = True\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå SDK connection failed: {str(e)}\")\n",
    "    print(\"   Note: Some features may be limited without SDK access\")\n",
    "    sdk_connected = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Unity Catalog Volume Setup\n",
    "\n",
    "Configure Unity Catalog paths and verify Volume access using shell and file system magic commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install sample datasets to temporary location first\n",
    "%sh\n",
    "echo \"üì¶ Installing PyForge Sample Datasets...\"\n",
    "pyforge install sample-datasets /tmp/pyforge_samples --formats csv,excel,xml,access,dbf --sizes small,medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Volume paths for data governance\n",
    "volume_config = {\n",
    "    'catalog': 'main',\n",
    "    'schema': 'default', \n",
    "    'bronze_volume': 'bronze_data',\n",
    "    'silver_volume': 'silver_data',\n",
    "    'gold_volume': 'gold_data'\n",
    "}\n",
    "\n",
    "# Construct Volume paths\n",
    "bronze_path = f\"/Volumes/{volume_config['catalog']}/{volume_config['schema']}/{volume_config['bronze_volume']}\"\n",
    "silver_path = f\"/Volumes/{volume_config['catalog']}/{volume_config['schema']}/{volume_config['silver_volume']}\"\n",
    "gold_path = f\"/Volumes/{volume_config['catalog']}/{volume_config['schema']}/{volume_config['gold_volume']}\"\n",
    "\n",
    "print(\"üèóÔ∏è Unity Catalog Volume Configuration:\")\n",
    "print(f\"   Bronze Layer: {bronze_path}\")\n",
    "print(f\"   Silver Layer: {silver_path}\")\n",
    "print(f\"   Gold Layer:   {gold_path}\")\n",
    "\n",
    "# Verify Volume access\n",
    "try:\n",
    "    dbutils.fs.ls(bronze_path)\n",
    "    print(\"‚úÖ Volume access confirmed\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Volume access issue: {str(e)}\")\n",
    "    print(\"   Creating sample paths in /tmp for demo...\")\n",
    "    bronze_path = \"/tmp/bronze_data\"\n",
    "    silver_path = \"/tmp/silver_data\" \n",
    "    gold_path = \"/tmp/gold_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Copy Sample Data to Volume (Bronze Layer)\n",
    "\n",
    "Transfer sample datasets to Unity Catalog Volumes for governed data access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy sample datasets to Bronze Volume using file system magic\n",
    "%fs mkdirs $bronze_path/sample_datasets/\n",
    "\n",
    "# Copy different format samples\n",
    "%fs cp -r file:/tmp/pyforge_samples/csv/small/ $bronze_path/sample_datasets/csv/\n",
    "%fs cp -r file:/tmp/pyforge_samples/excel/small/ $bronze_path/sample_datasets/excel/\n",
    "%fs cp -r file:/tmp/pyforge_samples/access/small/ $bronze_path/sample_datasets/access/\n",
    "%fs cp -r file:/tmp/pyforge_samples/dbf/small/ $bronze_path/sample_datasets/dbf/\n",
    "\n",
    "# Verify copy completed\n",
    "%fs ls $bronze_path/sample_datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Volume-to-Volume Data Conversion\n",
    "\n",
    "Demonstrate direct Volume-to-Volume conversions using PyForge Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Silver layer directory\n",
    "%fs mkdirs $silver_path/converted_data/\n",
    "\n",
    "# Convert CSV data from Bronze to Silver using Volume paths\n",
    "bronze_csv = f\"{bronze_path}/sample_datasets/csv/titanic-dataset.csv\"\n",
    "silver_parquet = f\"{silver_path}/converted_data/titanic.parquet\"\n",
    "\n",
    "print(\"üîÑ Volume-to-Volume Conversion:\")\n",
    "print(f\"   Source (Bronze): {bronze_csv}\")\n",
    "print(f\"   Target (Silver): {silver_parquet}\")\n",
    "\n",
    "# Perform conversion using PyForge Databricks\n",
    "result = forge.convert_from_volume(\n",
    "    source_volume_path=bronze_csv,\n",
    "    target_volume_path=silver_parquet,\n",
    "    output_format='parquet'\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Conversion completed in {result['duration']:.2f}s\")\n",
    "print(f\"   Rows processed: {result['row_count']:,}\")\n",
    "\n",
    "# Verify converted file\n",
    "%fs ls $silver_path/converted_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-Table Database Processing in Volumes\n",
    "\n",
    "Extract all tables from Access databases directly into Volume storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Northwind database from Bronze to Silver\n",
    "bronze_db = f\"{bronze_path}/sample_datasets/access/Northwind_2007_VBNet.accdb\"\n",
    "silver_tables = f\"{silver_path}/northwind_tables/\"\n",
    "\n",
    "print(\"üóÑÔ∏è Multi-Table Database Processing:\")\n",
    "print(f\"   Source Database: {bronze_db}\")\n",
    "print(f\"   Target Directory: {silver_tables}\")\n",
    "\n",
    "# Extract all tables using PyForge Databricks\n",
    "result = forge.extract_database_tables(\n",
    "    source_volume_path=bronze_db,\n",
    "    target_volume_path=silver_tables,\n",
    "    output_format='delta',  # Use Delta format for versioning\n",
    "    extract_all=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Database extraction completed:\")\n",
    "print(f\"   Tables extracted: {result['table_count']}\")\n",
    "print(f\"   Total rows: {result['total_rows']:,}\")\n",
    "\n",
    "# List extracted tables\n",
    "%fs ls $silver_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Serverless Compute Optimization\n",
    "\n",
    "Demonstrate automatic optimization for serverless vs. classic compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process multiple files with automatic optimization\n",
    "bronze_excel = f\"{bronze_path}/sample_datasets/excel/financial-sample.xlsx\"\n",
    "silver_excel = f\"{silver_path}/financial_data/\"\n",
    "\n",
    "print(\"‚ö° Serverless Optimization Demo:\")\n",
    "print(f\"   Environment: {'Serverless' if env_info['is_serverless'] else 'Classic Compute'}\")\n",
    "\n",
    "# Convert with automatic optimization\n",
    "result = forge.convert_optimized(\n",
    "    source_volume_path=bronze_excel,\n",
    "    target_volume_path=silver_excel,\n",
    "    optimization_level='auto',  # Let PyForge choose optimal strategy\n",
    "    enable_caching=True if env_info['is_serverless'] else False\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Optimized conversion completed:\")\n",
    "print(f\"   Processing mode: {result['processing_mode']}\")\n",
    "print(f\"   Memory usage: {result['peak_memory_mb']}MB\")\n",
    "print(f\"   Cache hits: {result.get('cache_hits', 0)}\")\n",
    "\n",
    "# Show performance metrics\n",
    "if 'performance_metrics' in result:\n",
    "    metrics = result['performance_metrics']\n",
    "    print(f\"\\nüìä Performance Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"   {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Delta Lake Integration and Gold Layer\n",
    "\n",
    "Create curated, analytics-ready datasets in the Gold layer using Delta format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gold layer with aggregated data\n",
    "%fs mkdirs $gold_path/analytics/\n",
    "\n",
    "# Load Silver data and create Gold aggregations\n",
    "titanic_df = spark.read.parquet(silver_parquet)\n",
    "\n",
    "# Create survival analysis (Gold layer)\n",
    "survival_analysis = titanic_df.groupBy(\"Pclass\", \"Sex\").agg(\n",
    "    count(\"*\").alias(\"total_passengers\"),\n",
    "    spark_sum(\"Survived\").alias(\"survivors\"),\n",
    "    (spark_sum(\"Survived\") / count(\"*\") * 100).alias(\"survival_rate\")\n",
    ").withColumn(\"analysis_date\", current_timestamp())\n",
    "\n",
    "# Write to Gold layer as Delta table\n",
    "gold_survival_path = f\"{gold_path}/analytics/titanic_survival_analysis\"\n",
    "\n",
    "survival_analysis.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(gold_survival_path)\n",
    "\n",
    "print(\"üèÜ Gold Layer Analytics Created:\")\n",
    "print(f\"   Delta table: {gold_survival_path}\")\n",
    "print(f\"   Records: {survival_analysis.count()}\")\n",
    "\n",
    "# Display results\n",
    "display(survival_analysis.orderBy(\"Pclass\", \"Sex\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Batch Processing with Distributed Computing\n",
    "\n",
    "Process multiple files using Spark's distributed capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch process multiple DBF files using distributed computing\n",
    "dbf_bronze = f\"{bronze_path}/sample_datasets/dbf/\"\n",
    "dbf_silver = f\"{silver_path}/geographic_data/\"\n",
    "\n",
    "print(\"üîÑ Distributed Batch Processing:\")\n",
    "print(f\"   Source: {dbf_bronze}\")\n",
    "print(f\"   Target: {dbf_silver}\")\n",
    "\n",
    "# Process all DBF files in parallel\n",
    "batch_result = forge.batch_convert_distributed(\n",
    "    source_volume_path=dbf_bronze,\n",
    "    target_volume_path=dbf_silver,\n",
    "    output_format='delta',\n",
    "    parallelism=4,  # Number of parallel tasks\n",
    "    enable_spark_optimization=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Batch processing completed:\")\n",
    "print(f\"   Files processed: {batch_result['files_processed']}\")\n",
    "print(f\"   Total records: {batch_result['total_records']:,}\")\n",
    "print(f\"   Processing time: {batch_result['total_duration']:.2f}s\")\n",
    "print(f\"   Throughput: {batch_result['records_per_second']:,.0f} records/sec\")\n",
    "\n",
    "# List converted files\n",
    "%fs ls $dbf_silver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Monitoring and Performance Analysis\n",
    "\n",
    "Monitor conversion performance and resource usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get comprehensive performance metrics\n",
    "performance_report = forge.get_performance_metrics()\n",
    "\n",
    "print(\"üìä PyForge Databricks Performance Report:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Display key metrics\n",
    "metrics = [\n",
    "    (\"Total Conversions\", performance_report['total_conversions']),\n",
    "    (\"Success Rate\", f\"{performance_report['success_rate']:.1f}%\"),\n",
    "    (\"Avg Processing Time\", f\"{performance_report['avg_duration']:.2f}s\"),\n",
    "    (\"Total Data Processed\", f\"{performance_report['total_data_gb']:.2f}GB\"),\n",
    "    (\"Peak Memory Usage\", f\"{performance_report['peak_memory_mb']}MB\"),\n",
    "    (\"Cache Hit Rate\", f\"{performance_report['cache_hit_rate']:.1f}%\")\n",
    "]\n",
    "\n",
    "for metric, value in metrics:\n",
    "    print(f\"   {metric:20}: {value}\")\n",
    "\n",
    "# Show format-specific performance\n",
    "print(\"\\nüìã Performance by Format:\")\n",
    "for fmt, stats in performance_report['format_stats'].items():\n",
    "    print(f\"   {fmt:10}: {stats['count']:3} files, {stats['avg_duration']:.2f}s avg\")\n",
    "\n",
    "# Volume usage summary\n",
    "print(\"\\nüíæ Volume Storage Summary:\")\n",
    "for layer in ['bronze', 'silver', 'gold']:\n",
    "    path = eval(f\"{layer}_path\")\n",
    "    try:\n",
    "        size_info = dbutils.fs.ls(path)\n",
    "        file_count = len([f for f in size_info if not f.name.endswith('/')])\n",
    "        print(f\"   {layer.title():6} Layer: {file_count} files\")\n",
    "    except:\n",
    "        print(f\"   {layer.title():6} Layer: Not accessible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've successfully explored **PyForge Databricks** advanced capabilities:\n",
    "\n",
    "### ‚úÖ **Key Achievements:**\n",
    "\n",
    "üèóÔ∏è **Unity Catalog Integration**: Direct Volume operations with governed data access  \n",
    "‚ö° **Serverless Optimization**: Automatic detection and performance tuning  \n",
    "üîÑ **Volume-to-Volume Conversions**: Seamless data movement between layers  \n",
    "üóÑÔ∏è **Multi-Table Processing**: Intelligent database extraction to Volumes  \n",
    "üìä **Delta Lake Integration**: Versioned, ACID-compliant data storage  \n",
    "üöÄ **Distributed Processing**: Spark-powered batch operations  \n",
    "üìà **Performance Monitoring**: Comprehensive metrics and optimization  \n",
    "\n",
    "### üéØ **Production Ready Features:**\n",
    "\n",
    "- **Medallion Architecture**: Bronze ‚Üí Silver ‚Üí Gold data pipeline\n",
    "- **Automatic Optimization**: Adapts to serverless vs. classic compute\n",
    "- **Error Resilience**: Robust handling of distributed operations\n",
    "- **Performance Monitoring**: Real-time metrics and optimization insights\n",
    "- **Governance Ready**: Full Unity Catalog and Volume integration\n",
    "\n",
    "### üöÄ **Next Steps:**\n",
    "\n",
    "1. **Scale Up**: Process larger datasets with distributed computing\n",
    "2. **Automate**: Create scheduled workflows using Databricks Jobs\n",
    "3. **Monitor**: Set up alerts based on performance metrics\n",
    "4. **Govern**: Implement data lineage and quality checks\n",
    "5. **Optimize**: Fine-tune settings for your specific workloads\n",
    "\n",
    "**PyForge Databricks transforms enterprise data processing with intelligent automation and distributed computing!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}