{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyForge Databricks Integration - Volume Operations & Serverless Optimization\n",
    "## Complete Walkthrough with Databricks SDK Integration\n",
    "\n",
    "This notebook demonstrates how to use PyForge with Databricks Volumes, showcasing:\n",
    "- **Databricks SDK Integration**: Direct Volume file operations\n",
    "- **Serverless Environment Detection**: Automatic optimization for serverless compute\n",
    "- **Volume-to-Volume Processing**: Direct conversion without local downloads\n",
    "- **Format-Specific Routing**: Native Databricks processing for supported formats\n",
    "\n",
    "### Prerequisites:\n",
    "- Running in Databricks environment (workspace or serverless)\n",
    "- Unity Catalog enabled with Volume access\n",
    "- PyForge packages installed: `pyforge-core` and `pyforge-databricks`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyForge packages\n",
    "%pip install pyforge-core pyforge-databricks --quiet\n",
    "\n",
    "# Restart Python kernel to ensure clean imports\n",
    "# Note: In serverless, packages are installed at the environment level\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Databricks SDK imports\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service import catalog\n",
    "\n",
    "# PyForge imports\n",
    "from pyforge_core import PyForgeCore\n",
    "from pyforge_databricks import PyForgeDatabricks\n",
    "\n",
    "# Spark imports (pre-installed in Databricks)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, current_timestamp\n",
    "\n",
    "print(\"üöÄ PyForge Databricks Integration loaded successfully!\")\n",
    "print(f\"üìç Python version: {sys.version}\")\n",
    "print(f\"üì¶ Running in: {os.environ.get('DATABRICKS_RUNTIME_VERSION', 'Local environment')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Detection and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyForge with Databricks integration\n",
    "forge = PyForgeDatabricks()\n",
    "\n",
    "# Display detailed environment information\n",
    "env_info = forge.env.get_environment_info()\n",
    "\n",
    "print(\"üîç Environment Detection Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìä Databricks Environment: {env_info['is_databricks']}\")\n",
    "print(f\"‚ö° Serverless Compute: {env_info['is_serverless']}\")\n",
    "print(f\"üî¢ Environment Version: {env_info['environment_version']}\")\n",
    "print(f\"üêç Python Version: {env_info['python_version']}\")\n",
    "print(f\"‚ú® Spark Version: {env_info['spark_version']}\")\n",
    "print(f\"üè¢ Workspace URL: {env_info['workspace_url']}\")\n",
    "\n",
    "# Validate SDK connection\n",
    "print(\"\\nüîå Validating Databricks SDK connection...\")\n",
    "try:\n",
    "    current_user = forge.w.current_user.me()\n",
    "    print(f\"‚úÖ Connected as: {current_user.display_name}\")\n",
    "    print(f\"üìß Email: {current_user.user_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå SDK connection failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Volume Configuration and Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Volume paths\n",
    "# Update these to match your Unity Catalog setup\n",
    "CATALOG = \"main\"  # or your catalog name\n",
    "SCHEMA = \"default\"  # or your schema name\n",
    "BRONZE_VOLUME = \"bronze\"  # raw data volume\n",
    "SILVER_VOLUME = \"silver\"  # processed data volume\n",
    "\n",
    "# Construct Volume paths\n",
    "bronze_path = f\"/Volumes/{CATALOG}/{SCHEMA}/{BRONZE_VOLUME}\"\n",
    "silver_path = f\"/Volumes/{CATALOG}/{SCHEMA}/{SILVER_VOLUME}\"\n",
    "\n",
    "print(\"üìÅ Volume Configuration:\")\n",
    "print(f\"   Bronze (Raw): {bronze_path}\")\n",
    "print(f\"   Silver (Processed): {silver_path}\")\n",
    "\n",
    "# List available volumes\n",
    "print(\"\\nüìÇ Available Volumes in Catalog:\")\n",
    "try:\n",
    "    volumes = forge.w.volumes.list(catalog_name=CATALOG, schema_name=SCHEMA)\n",
    "    for vol in volumes:\n",
    "        print(f\"   - {vol.name} ({vol.volume_type}): {vol.full_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Could not list volumes: {str(e)}\")\n",
    "    print(\"   Please ensure you have access to Unity Catalog volumes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Volume File Operations with Databricks SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Volume file operations\n",
    "test_file_path = f\"{bronze_path}/test_data.csv\"\n",
    "\n",
    "# Create sample data\n",
    "sample_csv_content = \"\"\"id,name,value,date\n",
    "1,Alice,100.5,2024-01-01\n",
    "2,Bob,200.3,2024-01-02\n",
    "3,Charlie,150.7,2024-01-03\n",
    "4,Diana,300.1,2024-01-04\n",
    "5,Eve,250.9,2024-01-05\n",
    "\"\"\"\n",
    "\n",
    "# Write file to Volume\n",
    "print(\"üìù Writing sample file to Volume...\")\n",
    "try:\n",
    "    forge.volume_handler.write_file(test_file_path, sample_csv_content.encode('utf-8'))\n",
    "    print(f\"‚úÖ File written to: {test_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Write failed: {str(e)}\")\n",
    "\n",
    "# List files in bronze volume\n",
    "print(\"\\nüìã Files in Bronze Volume:\")\n",
    "try:\n",
    "    files = forge.volume_handler.list_files(bronze_path)\n",
    "    for file in files[:10]:  # Show first 10 files\n",
    "        print(f\"   - {file.path} ({file.file_size} bytes)\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Could not list files: {str(e)}\")\n",
    "\n",
    "# Read file from Volume\n",
    "print(\"\\nüìñ Reading file from Volume...\")\n",
    "try:\n",
    "    content = forge.volume_handler.read_file(test_file_path)\n",
    "    print(f\"‚úÖ File content (first 200 chars):\")\n",
    "    print(content.decode('utf-8')[:200])\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Read failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Format-Specific Processing with Serverless Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate format routing and processing strategy selection\n",
    "test_formats = [\n",
    "    (\"data.csv\", \"CSV - Native Spark processing\"),\n",
    "    (\"report.xlsx\", \"Excel - Hybrid processing\"),\n",
    "    (\"config.json\", \"JSON - Native Spark processing\"),\n",
    "    (\"catalog.xml\", \"XML - Native Spark processing\"),\n",
    "    (\"document.pdf\", \"PDF - PyForge converter\"),\n",
    "    (\"legacy.mdb\", \"Access DB - PyForge converter\"),\n",
    "    (\"archive.dbf\", \"dBase - PyForge converter\")\n",
    "]\n",
    "\n",
    "print(\"üéØ Processing Strategy Selection:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for filename, description in test_formats:\n",
    "    file_ext = Path(filename).suffix.lower()\n",
    "    strategy = forge.processor.get_processing_strategy(file_ext)\n",
    "    \n",
    "    print(f\"\\nüìÑ {filename}\")\n",
    "    print(f\"   Format: {description}\")\n",
    "    print(f\"   Strategy: {strategy}\")\n",
    "    \n",
    "    if forge.env.is_serverless and strategy in ['serverless_native', 'databricks_native']:\n",
    "        print(\"   ‚ö° Serverless Optimizations:\")\n",
    "        print(\"      - Automatic scaling\")\n",
    "        print(\"      - Photon acceleration\")\n",
    "        print(\"      - Adaptive query execution\")\n",
    "    elif strategy == 'hybrid_excel':\n",
    "        print(\"   üîÑ Hybrid Processing:\")\n",
    "        print(\"      - PyForge sheet analysis\")\n",
    "        print(\"      - Spark DataFrame output\")\n",
    "        print(\"      - Column signature matching\")\n",
    "    elif strategy == 'pyforge_converter':\n",
    "        print(\"   üîß Specialized Converter:\")\n",
    "        print(\"      - Format-specific parser\")\n",
    "        print(\"      - String normalization\")\n",
    "        print(\"      - Memory optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Volume-to-Volume Data Conversion"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "    # Perform conversion\n    result = forge.convert(\n        input_path=source_csv,\n        output_path=target_parquet,\n        format_options={\n            'compression': 'snappy',\n            'schema_inference': True\n        }\n    )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Excel to Delta Lake conversion with multi-sheet handling\n",
    "source_excel = f\"{bronze_path}/financial_report.xlsx\"\n",
    "target_delta = f\"{silver_path}/financial_report_delta\"\n",
    "\n",
    "# First, create a sample Excel file in Volume\n",
    "print(\"üìä Creating sample Excel file...\")\n",
    "sample_excel_data = {\n",
    "    'Q1_Sales': spark.createDataFrame([\n",
    "        ('Product A', 10000, 'Q1'),\n",
    "        ('Product B', 15000, 'Q1'),\n",
    "        ('Product C', 12000, 'Q1')\n",
    "    ], ['product', 'revenue', 'quarter']),\n",
    "    \n",
    "    'Q2_Sales': spark.createDataFrame([\n",
    "        ('Product A', 12000, 'Q2'),\n",
    "        ('Product B', 18000, 'Q2'),\n",
    "        ('Product C', 14000, 'Q2')\n",
    "    ], ['product', 'revenue', 'quarter'])\n",
    "}\n",
    "\n",
    "# Note: In real scenario, Excel file would already exist in Volume\n",
    "print(\"\\nüîÑ Converting Excel to Delta Lake...\")\n",
    "print(f\"   Source: {source_excel}\")\n",
    "print(f\"   Target: {target_delta}\")\n",
    "\n",
    "try:\n",
    "    # Perform conversion with multi-sheet options\n",
    "    result = forge.convert_from_volume(\n",
    "        input_path=source_excel,\n",
    "        output_path=target_delta,\n",
    "        output_format='delta',\n",
    "        format_options={\n",
    "            'combine_sheets': True,\n",
    "            'sheet_matching_strategy': 'column_signature',\n",
    "            'partition_by': ['quarter'],\n",
    "            'overwrite_mode': 'overwrite'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Excel to Delta conversion successful!\")\n",
    "    print(f\"   Sheets processed: {result['sheets_processed']}\")\n",
    "    print(f\"   Total rows: {result['row_count']}\")\n",
    "    print(f\"   Delta table location: {result['output_path']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Excel conversion simulated: {str(e)}\")\n",
    "    print(\"   In production, ensure Excel file exists in Volume\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Direct Spark DataFrame Operations with Volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read directly from Volume using Spark\n",
    "spark = SparkSession.getActiveSession()\n",
    "\n",
    "print(\"üìä Direct Spark operations on Volume data:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Read CSV from Volume\n",
    "csv_volume_path = f\"{bronze_path}/test_data.csv\"\n",
    "print(f\"\\nüìñ Reading CSV from Volume: {csv_volume_path}\")\n",
    "\n",
    "try:\n",
    "    df = spark.read.csv(\n",
    "        csv_volume_path,\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Data loaded successfully!\")\n",
    "    print(f\"   Rows: {df.count()}\")\n",
    "    print(f\"   Columns: {len(df.columns)}\")\n",
    "    \n",
    "    # Show schema\n",
    "    print(\"\\nüìã Schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    # Show sample data\n",
    "    print(\"\\nüìä Sample Data:\")\n",
    "    display(df)\n",
    "    \n",
    "    # Apply transformations\n",
    "    print(\"\\nüîß Applying transformations...\")\n",
    "    transformed_df = df \\\n",
    "        .withColumn(\"processed_timestamp\", current_timestamp()) \\\n",
    "        .withColumn(\"value_doubled\", col(\"value\") * 2) \\\n",
    "        .withColumn(\"processing_engine\", lit(\"Databricks Serverless\"))\n",
    "    \n",
    "    # Write back to Volume as Parquet\n",
    "    output_path = f\"{silver_path}/transformed_data.parquet\"\n",
    "    print(f\"\\nüíæ Writing transformed data to: {output_path}\")\n",
    "    \n",
    "    transformed_df.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(output_path)\n",
    "    \n",
    "    print(\"‚úÖ Data written successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Operation failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Batch Processing Multiple Files in Volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch process multiple files from Volume\n",
    "print(\"üì¶ Batch Processing Example:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create sample files for batch processing\n",
    "batch_files = [\n",
    "    (f\"{bronze_path}/sales_jan.csv\", \"id,product,amount\\n1,A,100\\n2,B,200\"),\n",
    "    (f\"{bronze_path}/sales_feb.csv\", \"id,product,amount\\n3,A,150\\n4,B,250\"),\n",
    "    (f\"{bronze_path}/sales_mar.csv\", \"id,product,amount\\n5,A,200\\n6,B,300\")\n",
    "]\n",
    "\n",
    "# Write sample files\n",
    "print(\"\\nüìù Creating sample files for batch processing...\")\n",
    "for file_path, content in batch_files:\n",
    "    try:\n",
    "        forge.volume_handler.write_file(file_path, content.encode('utf-8'))\n",
    "        print(f\"   ‚úÖ Created: {Path(file_path).name}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Batch convert all CSV files to Parquet\n",
    "print(\"\\nüîÑ Batch converting CSV files to Parquet...\")\n",
    "batch_results = []\n",
    "\n",
    "for file_path, _ in batch_files:\n",
    "    filename = Path(file_path).stem\n",
    "    output_path = f\"{silver_path}/{filename}.parquet\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\n   Processing: {filename}.csv\")\n",
    "        \n",
    "        result = forge.convert_from_volume(\n",
    "            input_path=file_path,\n",
    "            output_path=output_path,\n",
    "            format_options={'compression': 'snappy'}\n",
    "        )\n",
    "        \n",
    "        batch_results.append({\n",
    "            'file': filename,\n",
    "            'status': 'success',\n",
    "            'rows': result.get('row_count', 0),\n",
    "            'duration': result.get('duration', 0)\n",
    "        })\n",
    "        \n",
    "        print(f\"      ‚úÖ Converted in {result.get('duration', 0):.2f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        batch_results.append({\n",
    "            'file': filename,\n",
    "            'status': 'failed',\n",
    "            'error': str(e)\n",
    "        })\n",
    "        print(f\"      ‚ùå Failed: {str(e)}\")\n",
    "\n",
    "# Display batch results summary\n",
    "print(\"\\nüìä Batch Processing Summary:\")\n",
    "print(\"=\" * 50)\n",
    "success_count = sum(1 for r in batch_results if r['status'] == 'success')\n",
    "print(f\"‚úÖ Successful: {success_count}/{len(batch_results)}\")\n",
    "print(f\"‚ùå Failed: {len(batch_results) - success_count}/{len(batch_results)}\")\n",
    "\n",
    "if success_count > 0:\n",
    "    total_rows = sum(r.get('rows', 0) for r in batch_results if r['status'] == 'success')\n",
    "    total_time = sum(r.get('duration', 0) for r in batch_results if r['status'] == 'success')\n",
    "    print(f\"üìä Total rows processed: {total_rows}\")\n",
    "    print(f\"‚è±Ô∏è  Total processing time: {total_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Volume Operations with PyForge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced example: Process XML with hierarchical data\n",
    "xml_content = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<catalog>\n",
    "    <product id=\"1\">\n",
    "        <name>Widget A</name>\n",
    "        <category>Electronics</category>\n",
    "        <price currency=\"USD\">29.99</price>\n",
    "        <features>\n",
    "            <feature>Waterproof</feature>\n",
    "            <feature>Bluetooth</feature>\n",
    "        </features>\n",
    "    </product>\n",
    "    <product id=\"2\">\n",
    "        <name>Widget B</name>\n",
    "        <category>Home</category>\n",
    "        <price currency=\"USD\">45.50</price>\n",
    "        <features>\n",
    "            <feature>Energy Efficient</feature>\n",
    "        </features>\n",
    "    </product>\n",
    "</catalog>\n",
    "\"\"\"\n",
    "\n",
    "xml_path = f\"{bronze_path}/catalog.xml\"\n",
    "output_path = f\"{silver_path}/catalog_flattened.parquet\"\n",
    "\n",
    "print(\"üå≥ Processing hierarchical XML data:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Write XML to Volume\n",
    "try:\n",
    "    forge.volume_handler.write_file(xml_path, xml_content.encode('utf-8'))\n",
    "    print(f\"‚úÖ XML file written to: {xml_path}\")\n",
    "    \n",
    "    # Convert XML to Parquet with flattening\n",
    "    print(\"\\nüîÑ Converting XML to flattened Parquet...\")\n",
    "    \n",
    "    result = forge.convert_from_volume(\n",
    "        input_path=xml_path,\n",
    "        output_path=output_path,\n",
    "        format_options={\n",
    "            'flatten_nested': True,\n",
    "            'array_detection': True,\n",
    "            'preserve_attributes': True,\n",
    "            'root_tag': 'product'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ XML conversion successful!\")\n",
    "    print(f\"   Flattened structure created\")\n",
    "    print(f\"   Arrays detected and normalized\")\n",
    "    print(f\"   Output: {output_path}\")\n",
    "    \n",
    "    # Read and display the flattened data\n",
    "    print(\"\\nüìä Flattened XML data:\")\n",
    "    flattened_df = spark.read.parquet(output_path)\n",
    "    display(flattened_df)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå XML processing failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Performance Monitoring and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor conversion performance and resource usage\n",
    "print(\"üìä Performance Monitoring Dashboard:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get conversion statistics\n",
    "stats = forge.get_conversion_statistics()\n",
    "\n",
    "if stats:\n",
    "    print(\"\\nüìà Conversion Statistics:\")\n",
    "    print(f\"   Total conversions: {stats['total_conversions']}\")\n",
    "    print(f\"   Successful: {stats['successful']}\")\n",
    "    print(f\"   Failed: {stats['failed']}\")\n",
    "    print(f\"   Average duration: {stats['avg_duration']:.2f}s\")\n",
    "    print(f\"   Total data processed: {stats['total_bytes_processed'] / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Display serverless-specific optimizations\n",
    "if forge.env.is_serverless:\n",
    "    print(\"\\n‚ö° Serverless Optimizations Active:\")\n",
    "    print(\"   ‚úÖ Photon acceleration enabled\")\n",
    "    print(\"   ‚úÖ Adaptive query execution\")\n",
    "    print(\"   ‚úÖ Dynamic partition pruning\")\n",
    "    print(\"   ‚úÖ Automatic scaling based on workload\")\n",
    "    print(\"   ‚úÖ Columnar caching for repeated queries\")\n",
    "    \n",
    "    # Show Spark configuration\n",
    "    print(\"\\nüîß Spark Configuration (Serverless):\")\n",
    "    important_configs = [\n",
    "        \"spark.databricks.compute.type\",\n",
    "        \"spark.databricks.photon.enabled\",\n",
    "        \"spark.sql.adaptive.enabled\",\n",
    "        \"spark.sql.adaptive.coalescePartitions.enabled\"\n",
    "    ]\n",
    "    \n",
    "    for config in important_configs:\n",
    "        try:\n",
    "            value = spark.conf.get(config)\n",
    "            print(f\"   {config}: {value}\")\n",
    "        except:\n",
    "            print(f\"   {config}: Not set\")\n",
    "\n",
    "# Memory usage estimation\n",
    "print(\"\\nüíæ Memory Usage Guidelines:\")\n",
    "memory_guidelines = {\n",
    "    'CSV': '~2x file size in memory',\n",
    "    'JSON': '~3x file size (due to parsing)',\n",
    "    'XML': '~4x file size (hierarchical structure)',\n",
    "    'Excel': '~2.5x file size per sheet',\n",
    "    'Parquet': '~0.5x file size (columnar compression)'\n",
    "}\n",
    "\n",
    "for format_type, guideline in memory_guidelines.items():\n",
    "    print(f\"   {format_type}: {guideline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Best Practices and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìö PyForge Databricks Integration Best Practices:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_practices = [\n",
    "    {\n",
    "        \"category\": \"üóÇÔ∏è Volume Organization\",\n",
    "        \"practices\": [\n",
    "            \"Use bronze/silver/gold pattern for data layers\",\n",
    "            \"Partition large datasets by date or category\",\n",
    "            \"Use descriptive naming conventions\",\n",
    "            \"Clean up temporary files regularly\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"‚ö° Performance Optimization\",\n",
    "        \"practices\": [\n",
    "            \"Use native Databricks formats when possible (CSV, JSON, XML)\",\n",
    "            \"Enable compression for Parquet files (snappy/zstd)\",\n",
    "            \"Batch process files when dealing with many small files\",\n",
    "            \"Monitor memory usage for large Excel/PDF conversions\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"üîí Security & Governance\",\n",
    "        \"practices\": [\n",
    "            \"Use Unity Catalog for access control\",\n",
    "            \"Implement data retention policies\",\n",
    "            \"Audit file access and conversions\",\n",
    "            \"Encrypt sensitive data at rest\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"üõ†Ô∏è Development Workflow\",\n",
    "        \"practices\": [\n",
    "            \"Test conversions with small datasets first\",\n",
    "            \"Use try-except blocks for robust error handling\",\n",
    "            \"Log conversion metrics for monitoring\",\n",
    "            \"Version control your conversion pipelines\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "for section in best_practices:\n",
    "    print(f\"\\n{section['category']}\")\n",
    "    for practice in section['practices']:\n",
    "        print(f\"   ‚Ä¢ {practice}\")\n",
    "\n",
    "print(\"\\nüí° Pro Tips:\")\n",
    "print(\"   1. Use serverless compute for variable workloads\")\n",
    "print(\"   2. Cache frequently accessed converted files\")\n",
    "print(\"   3. Monitor Unity Catalog usage for cost optimization\")\n",
    "print(\"   4. Leverage Delta Lake for versioned data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ PyForge Databricks Integration Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n‚úÖ What we've demonstrated:\")\n",
    "print(\"   ‚Ä¢ Databricks SDK integration for Volume operations\")\n",
    "print(\"   ‚Ä¢ Serverless environment detection and optimization\")\n",
    "print(\"   ‚Ä¢ Direct Volume-to-Volume file conversion\")\n",
    "print(\"   ‚Ä¢ Format-specific processing strategies\")\n",
    "print(\"   ‚Ä¢ Batch processing capabilities\")\n",
    "print(\"   ‚Ä¢ Performance monitoring and optimization\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"   1. Install packages: pip install pyforge-core pyforge-databricks\")\n",
    "print(\"   2. Configure Unity Catalog access and create volumes\")\n",
    "print(\"   3. Start with simple CSV/JSON conversions\")\n",
    "print(\"   4. Scale to complex formats (Excel, XML, PDF)\")\n",
    "print(\"   5. Build automated conversion pipelines\")\n",
    "print(\"   6. Monitor performance and optimize as needed\")\n",
    "\n",
    "print(\"\\nüì¶ Package Information:\")\n",
    "print(f\"   pyforge-core version: {PyForgeCore.__version__}\")\n",
    "print(f\"   pyforge-databricks version: {PyForgeDatabricks.__version__}\")\n",
    "print(f\"   Databricks SDK version: {forge.w.__module__.split('.')[0]}\")\n",
    "\n",
    "print(\"\\nüéâ Happy converting with PyForge on Databricks!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PyForge Databricks Volume Integration",
   "notebookOrigID": 0,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}