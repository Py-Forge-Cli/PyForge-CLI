{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyForge CLI Databricks Extension - Serverless Testing\n",
    "\n",
    "This notebook specifically tests PyForge CLI Databricks extension in serverless compute environments.\n",
    "\n",
    "## Key Features Tested\n",
    "- **Serverless Detection**: Verify serverless environment detection\n",
    "- **PySpark Optimization**: Test PySpark-optimized converters\n",
    "- **Memory Efficiency**: Test memory-efficient processing\n",
    "- **Streaming Support**: Test streaming for large datasets\n",
    "- **Delta Lake**: Test Delta Lake output format\n",
    "\n",
    "## Serverless-Specific Optimizations\n",
    "- Automatic PySpark converter selection\n",
    "- Memory-aware processing modes\n",
    "- Streaming for large datasets\n",
    "- Unity Catalog Volume integration\n",
    "\n",
    "## Prerequisites\n",
    "1. Databricks workspace with serverless compute\n",
    "2. PyForge CLI with Databricks extension installed\n",
    "3. Unity Catalog Volume access\n",
    "4. PySpark and Delta Lake available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configuration for Serverless Testing\n",
    "# =============================================================================\n",
    "# SERVERLESS CONFIGURATION SECTION\n",
    "# =============================================================================\n",
    "\n",
    "# Create widgets for serverless-specific configuration\n",
    "dbutils.widgets.text(\"pyforge_version\", \"latest\", \"PyForge Version\")\n",
    "dbutils.widgets.text(\"volume_path\", \"/Volumes/main/default/pyforge\", \"Volume Path\")\n",
    "dbutils.widgets.dropdown(\"test_dataset_size\", \"medium\", [\"small\", \"medium\", \"large\"], \"Test Dataset Size\")\n",
    "dbutils.widgets.checkbox(\"test_streaming\", True, \"Test Streaming Mode\")\n",
    "dbutils.widgets.checkbox(\"test_delta_lake\", True, \"Test Delta Lake\")\n",
    "dbutils.widgets.checkbox(\"test_memory_optimization\", True, \"Test Memory Optimization\")\n",
    "dbutils.widgets.checkbox(\"verbose_logging\", True, \"Verbose Logging\")\n",
    "\n",
    "# Get configuration values\n",
    "PYFORGE_VERSION = dbutils.widgets.get(\"pyforge_version\")\n",
    "VOLUME_PATH = dbutils.widgets.get(\"volume_path\")\n",
    "TEST_DATASET_SIZE = dbutils.widgets.get(\"test_dataset_size\")\n",
    "TEST_STREAMING = dbutils.widgets.get(\"test_streaming\") == \"true\"\n",
    "TEST_DELTA_LAKE = dbutils.widgets.get(\"test_delta_lake\") == \"true\"\n",
    "TEST_MEMORY_OPTIMIZATION = dbutils.widgets.get(\"test_memory_optimization\") == \"true\"\n",
    "VERBOSE_LOGGING = dbutils.widgets.get(\"verbose_logging\") == \"true\"\n",
    "\n",
    "# Dataset size configuration\n",
    "DATASET_SIZES = {\n",
    "    \"small\": {\"rows\": 1000, \"columns\": 5},\n",
    "    \"medium\": {\"rows\": 10000, \"columns\": 10},\n",
    "    \"large\": {\"rows\": 100000, \"columns\": 15}\n",
    "}\n",
    "\n",
    "dataset_config = DATASET_SIZES[TEST_DATASET_SIZE]\n",
    "\n",
    "print(\"üîß Serverless Testing Configuration:\")\n",
    "print(f\"   PyForge Version: {PYFORGE_VERSION}\")\n",
    "print(f\"   Volume Path: {VOLUME_PATH}\")\n",
    "print(f\"   Dataset Size: {TEST_DATASET_SIZE} ({dataset_config['rows']:,} rows, {dataset_config['columns']} columns)\")\n",
    "print(f\"   Test Streaming: {TEST_STREAMING}\")\n",
    "print(f\"   Test Delta Lake: {TEST_DELTA_LAKE}\")\n",
    "print(f\"   Test Memory Optimization: {TEST_MEMORY_OPTIMIZATION}\")\n",
    "print(f\"   Verbose Logging: {VERBOSE_LOGGING}\")\n",
    "\n",
    "# Initialize test tracking\n",
    "serverless_test_results = {\n",
    "    'environment_verification': None,\n",
    "    'pyspark_optimization': None,\n",
    "    'memory_efficiency': None,\n",
    "    'streaming_support': None,\n",
    "    'delta_lake_support': None,\n",
    "    'performance_metrics': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Serverless Environment Verification\n",
    "# =============================================================================\n",
    "# ENVIRONMENT VERIFICATION SECTION\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üîç Verifying serverless environment...\")\n",
    "\n",
    "verification_start_time = time.time()\n",
    "env_verification = {}\n",
    "\n",
    "try:\n",
    "    # Check if we're in serverless compute\n",
    "    runtime_version = os.environ.get('DATABRICKS_RUNTIME_VERSION', 'unknown')\n",
    "    is_serverless = 'serverless' in runtime_version.lower()\n",
    "    \n",
    "    print(f\"‚úÖ Runtime Version: {runtime_version}\")\n",
    "    print(f\"‚úÖ Serverless Environment: {is_serverless}\")\n",
    "    \n",
    "    env_verification['runtime_version'] = runtime_version\n",
    "    env_verification['is_serverless'] = is_serverless\n",
    "    \n",
    "    # Verify PySpark availability (critical for serverless)\n",
    "    try:\n",
    "        import pyspark\n",
    "        pyspark_version = pyspark.__version__\n",
    "        \n",
    "        # Get Spark configuration\n",
    "        spark_conf = dict(spark.conf.getAll())\n",
    "        \n",
    "        print(f\"‚úÖ PySpark Version: {pyspark_version}\")\n",
    "        print(f\"‚úÖ Spark Version: {spark.version}\")\n",
    "        print(f\"‚úÖ Spark Master: {spark.sparkContext.master}\")\n",
    "        \n",
    "        env_verification['pyspark_version'] = pyspark_version\n",
    "        env_verification['spark_version'] = spark.version\n",
    "        env_verification['spark_master'] = spark.sparkContext.master\n",
    "        env_verification['pyspark_available'] = True\n",
    "        \n",
    "        # Check for serverless-specific Spark configurations\n",
    "        serverless_indicators = []\n",
    "        for key, value in spark_conf.items():\n",
    "            if 'serverless' in key.lower() or 'serverless' in str(value).lower():\n",
    "                serverless_indicators.append(f\"{key}: {value}\")\n",
    "        \n",
    "        if serverless_indicators:\n",
    "            print(f\"‚úÖ Serverless Spark configurations found: {len(serverless_indicators)}\")\n",
    "            env_verification['serverless_spark_config'] = serverless_indicators\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ùå PySpark not available - this will significantly impact serverless performance\")\n",
    "        env_verification['pyspark_available'] = False\n",
    "    \n",
    "    # Verify Delta Lake availability (important for serverless)\n",
    "    try:\n",
    "        import delta\n",
    "        delta_version = delta.__version__\n",
    "        print(f\"‚úÖ Delta Lake Version: {delta_version}\")\n",
    "        env_verification['delta_available'] = True\n",
    "        env_verification['delta_version'] = delta_version\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è Delta Lake not available\")\n",
    "        env_verification['delta_available'] = False\n",
    "    \n",
    "    # Check Unity Catalog Volume access\n",
    "    try:\n",
    "        volume_files = dbutils.fs.ls(VOLUME_PATH)\n",
    "        print(f\"‚úÖ Unity Catalog Volume accessible: {len(volume_files)} items\")\n",
    "        env_verification['volume_accessible'] = True\n",
    "        \n",
    "        # Create test directories\n",
    "        test_data_path = f\"{VOLUME_PATH}/serverless-test-data\"\n",
    "        test_output_path = f\"{VOLUME_PATH}/serverless-test-output\"\n",
    "        dbutils.fs.mkdirs(test_data_path)\n",
    "        dbutils.fs.mkdirs(test_output_path)\n",
    "        print(\"‚úÖ Test directories created\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Volume access failed: {e}\")\n",
    "        env_verification['volume_accessible'] = False\n",
    "    \n",
    "    env_verification['verification_success'] = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Environment verification failed: {e}\")\n",
    "    env_verification['verification_success'] = False\n",
    "    env_verification['error'] = str(e)\n",
    "\n",
    "verification_time = time.time() - verification_start_time\n",
    "env_verification['verification_time'] = verification_time\n",
    "serverless_test_results['environment_verification'] = env_verification\n",
    "\n",
    "print(f\"‚è±Ô∏è Environment verification time: {verification_time:.2f} seconds\")\n",
    "\n",
    "if not env_verification.get('verification_success', False):\n",
    "    print(\"‚ùå Cannot proceed without successful environment verification\")\n",
    "    dbutils.notebook.exit(\"Environment verification failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install and Test PyForge Databricks Extension\n",
    "# =============================================================================\n",
    "# EXTENSION INSTALLATION AND TESTING SECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üì¶ Installing and testing PyForge Databricks extension...\")\n",
    "\n",
    "install_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Install PyForge CLI with Databricks extension\n",
    "    if PYFORGE_VERSION == \"latest\":\n",
    "        %pip install --no-cache-dir pyforge-cli[databricks]\n",
    "    else:\n",
    "        %pip install --no-cache-dir pyforge-cli[databricks]=={PYFORGE_VERSION}\n",
    "    \n",
    "    print(\"‚úÖ PyForge CLI with Databricks extension installed\")\n",
    "    \n",
    "    # Test PyForgeDatabricks API initialization\n",
    "    from pyforge_cli.extensions.databricks.pyforge_databricks import PyForgeDatabricks\n",
    "    \n",
    "    # Initialize with Spark session\n",
    "    forge = PyForgeDatabricks(spark_session=spark, auto_init=True)\n",
    "    print(\"‚úÖ PyForgeDatabricks initialized with Spark session\")\n",
    "    \n",
    "    # Verify serverless environment detection\n",
    "    env_info = forge.get_environment_info()\n",
    "    print(f\"‚úÖ Environment detected: {env_info['compute_type']} ({env_info['runtime_version']})\")\n",
    "    \n",
    "    if env_info['compute_type'] != 'serverless':\n",
    "        print(\"‚ö†Ô∏è Not running in serverless environment - some optimizations may not apply\")\n",
    "    \n",
    "    serverless_test_results['extension_installation'] = {\n",
    "        'installation_success': True,\n",
    "        'api_initialization': True,\n",
    "        'detected_compute_type': env_info['compute_type']\n",
    "    }\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Extension installation/testing failed: {e}\")\n",
    "    serverless_test_results['extension_installation'] = {\n",
    "        'installation_success': False,\n",
    "        'error': str(e)\n",
    "    }\n",
    "    dbutils.notebook.exit(\"Extension installation failed\")\n",
    "\n",
    "install_time = time.time() - install_start_time\n",
    "serverless_test_results['performance_metrics']['install_time'] = install_time\n",
    "print(f\"‚è±Ô∏è Installation time: {install_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test PySpark-Optimized Converters\n",
    "# =============================================================================\n",
    "# PYSPARK OPTIMIZATION TESTING SECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"‚ö° Testing PySpark-optimized converters...\")\n",
    "\n",
    "pyspark_test_start = time.time()\n",
    "pyspark_results = {}\n",
    "\n",
    "try:\n",
    "    # Generate test dataset using Spark\n",
    "    print(f\"üìä Generating test dataset: {dataset_config['rows']:,} rows...\")\n",
    "    \n",
    "    # Create large test dataset with Spark\n",
    "    from pyspark.sql.functions import col, rand, when, concat, lit\n",
    "    \n",
    "    test_df = spark.range(dataset_config['rows']).toDF(\"id\")\n",
    "    \n",
    "    # Add various column types for comprehensive testing\n",
    "    for i in range(dataset_config['columns'] - 1):\n",
    "        if i % 4 == 0:\n",
    "            # String column\n",
    "            test_df = test_df.withColumn(f\"string_col_{i}\", concat(lit(\"test_\"), col(\"id\").cast(\"string\")))\n",
    "        elif i % 4 == 1:\n",
    "            # Numeric column\n",
    "            test_df = test_df.withColumn(f\"numeric_col_{i}\", rand() * 1000)\n",
    "        elif i % 4 == 2:\n",
    "            # Boolean column\n",
    "            test_df = test_df.withColumn(f\"boolean_col_{i}\", when(rand() > 0.5, True).otherwise(False))\n",
    "        else:\n",
    "            # Category column\n",
    "            test_df = test_df.withColumn(f\"category_col_{i}\", when(col(\"id\") % 3 == 0, \"A\").when(col(\"id\") % 3 == 1, \"B\").otherwise(\"C\"))\n",
    "    \n",
    "    # Cache for performance\n",
    "    test_df.cache()\n",
    "    actual_rows = test_df.count()\n",
    "    \n",
    "    print(f\"‚úÖ Test dataset created: {actual_rows:,} rows, {len(test_df.columns)} columns\")\n",
    "    \n",
    "    # Save as CSV for conversion testing\n",
    "    test_csv_path = f\"{VOLUME_PATH}/serverless-test-data/large_test_data.csv\"\n",
    "    test_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(test_csv_path)\n",
    "    \n",
    "    # Get the actual CSV file path (Spark creates a directory)\n",
    "    csv_files = dbutils.fs.ls(test_csv_path)\n",
    "    actual_csv_path = None\n",
    "    for file_info in csv_files:\n",
    "        if file_info.name.endswith('.csv'):\n",
    "            actual_csv_path = file_info.path\n",
    "            break\n",
    "    \n",
    "    if actual_csv_path:\n",
    "        print(f\"‚úÖ CSV test file created: {actual_csv_path}\")\n",
    "        \n",
    "        # Test PySpark CSV converter\n",
    "        print(\"Testing PySpark CSV converter...\")\n",
    "        \n",
    "        csv_convert_start = time.time()\n",
    "        \n",
    "        # Test conversion with PyForge\n",
    "        output_path = f\"{VOLUME_PATH}/serverless-test-output/converted_data.parquet\"\n",
    "        \n",
    "        conversion_result = forge.convert(\n",
    "            input_path=actual_csv_path,\n",
    "            output_path=output_path,\n",
    "            format=\"parquet\",\n",
    "            engine=\"spark\"  # Force Spark engine\n",
    "        )\n",
    "        \n",
    "        csv_convert_time = time.time() - csv_convert_start\n",
    "        \n",
    "        print(f\"‚úÖ PySpark CSV conversion completed in {csv_convert_time:.2f} seconds\")\n",
    "        print(f\"   Rows processed: {conversion_result['rows_processed']:,}\")\n",
    "        print(f\"   Engine used: {conversion_result['engine_used']}\")\n",
    "        print(f\"   Output size: {conversion_result.get('output_size_mb', 'unknown')} MB\")\n",
    "        \n",
    "        # Verify output\n",
    "        verify_df = spark.read.parquet(output_path)\n",
    "        verify_count = verify_df.count()\n",
    "        \n",
    "        data_integrity = verify_count == actual_rows\n",
    "        print(f\"‚úÖ Data integrity check: {data_integrity} ({verify_count:,} rows)\")\n",
    "        \n",
    "        pyspark_results['csv_conversion'] = {\n",
    "            'success': True,\n",
    "            'conversion_time': csv_convert_time,\n",
    "            'rows_processed': conversion_result['rows_processed'],\n",
    "            'engine_used': conversion_result['engine_used'],\n",
    "            'data_integrity': data_integrity,\n",
    "            'throughput_rows_per_sec': actual_rows / csv_convert_time\n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå CSV test file creation failed\")\n",
    "        pyspark_results['csv_conversion'] = {'success': False, 'error': 'CSV file creation failed'}\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå PySpark optimization testing failed: {e}\")\n",
    "    pyspark_results['error'] = str(e)\n",
    "\n",
    "pyspark_test_time = time.time() - pyspark_test_start\n",
    "pyspark_results['total_test_time'] = pyspark_test_time\n",
    "serverless_test_results['pyspark_optimization'] = pyspark_results\n",
    "serverless_test_results['performance_metrics']['pyspark_test_time'] = pyspark_test_time\n",
    "\n",
    "print(f\"‚è±Ô∏è PySpark optimization testing time: {pyspark_test_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test Streaming Support for Large Datasets\n",
    "# =============================================================================\n",
    "# STREAMING SUPPORT TESTING SECTION\n",
    "# =============================================================================\n",
    "\n",
    "if TEST_STREAMING:\n",
    "    print(\"üåä Testing streaming support for large datasets...\")\n",
    "    \n",
    "    streaming_test_start = time.time()\n",
    "    streaming_results = {}\n",
    "    \n",
    "    try:\n",
    "        # Test streaming mode conversion\n",
    "        print(\"Testing streaming mode conversion...\")\n",
    "        \n",
    "        if actual_csv_path:\n",
    "            # Test streaming conversion\n",
    "            streaming_output_path = f\"{VOLUME_PATH}/serverless-test-output/streaming_converted.parquet\"\n",
    "            \n",
    "            streaming_convert_start = time.time()\n",
    "            \n",
    "            streaming_result = forge.convert(\n",
    "                input_path=actual_csv_path,\n",
    "                output_path=streaming_output_path,\n",
    "                format=\"parquet\",\n",
    "                streaming=True,  # Enable streaming mode\n",
    "                chunk_size=1000  # Process in chunks\n",
    "            )\n",
    "            \n",
    "            streaming_convert_time = time.time() - streaming_convert_start\n",
    "            \n",
    "            print(f\"‚úÖ Streaming conversion completed in {streaming_convert_time:.2f} seconds\")\n",
    "            print(f\"   Rows processed: {streaming_result['rows_processed']:,}\")\n",
    "            print(f\"   Chunks processed: {streaming_result.get('chunks_processed', 'unknown')}\")\n",
    "            \n",
    "            # Compare streaming vs batch performance\n",
    "            batch_time = pyspark_results.get('csv_conversion', {}).get('conversion_time', 0)\n",
    "            if batch_time > 0:\n",
    "                streaming_efficiency = batch_time / streaming_convert_time\n",
    "                print(f\"‚úÖ Streaming efficiency: {streaming_efficiency:.2f}x {'faster' if streaming_efficiency > 1 else 'slower'} than batch\")\n",
    "            \n",
    "            streaming_results['streaming_conversion'] = {\n",
    "                'success': True,\n",
    "                'conversion_time': streaming_convert_time,\n",
    "                'rows_processed': streaming_result['rows_processed'],\n",
    "                'streaming_efficiency': streaming_efficiency if 'streaming_efficiency' in locals() else None\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No test file available for streaming test\")\n",
    "            streaming_results['streaming_conversion'] = {'skipped': 'no_test_file'}\n",
    "        \n",
    "        # Test memory efficiency during streaming\n",
    "        if TEST_MEMORY_OPTIMIZATION:\n",
    "            print(\"Testing memory efficiency...\")\n",
    "            \n",
    "            try:\n",
    "                import psutil\n",
    "                \n",
    "                # Get memory usage before and after streaming\n",
    "                process = psutil.Process()\n",
    "                memory_before = process.memory_info().rss / 1024 / 1024  # MB\n",
    "                \n",
    "                # Process a dataset in streaming mode\n",
    "                # (This would be implementation-specific)\n",
    "                \n",
    "                memory_after = process.memory_info().rss / 1024 / 1024  # MB\n",
    "                memory_usage = memory_after - memory_before\n",
    "                \n",
    "                print(f\"‚úÖ Memory usage during streaming: {memory_usage:.1f} MB\")\n",
    "                \n",
    "                streaming_results['memory_efficiency'] = {\n",
    "                    'memory_before_mb': memory_before,\n",
    "                    'memory_after_mb': memory_after,\n",
    "                    'memory_used_mb': memory_usage\n",
    "                }\n",
    "                \n",
    "            except ImportError:\n",
    "                print(\"‚ö†Ô∏è psutil not available - skipping memory efficiency test\")\n",
    "                streaming_results['memory_efficiency'] = {'skipped': 'psutil_unavailable'}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Streaming support testing failed: {e}\")\n",
    "        streaming_results['error'] = str(e)\n",
    "    \n",
    "    streaming_test_time = time.time() - streaming_test_start\n",
    "    streaming_results['total_test_time'] = streaming_test_time\n",
    "    serverless_test_results['streaming_support'] = streaming_results\n",
    "    serverless_test_results['performance_metrics']['streaming_test_time'] = streaming_test_time\n",
    "    \n",
    "    print(f\"‚è±Ô∏è Streaming support testing time: {streaming_test_time:.2f} seconds\")\n",
    "\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Streaming support testing skipped (disabled in configuration)\")\n",
    "    serverless_test_results['streaming_support'] = {'skipped': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test Delta Lake Support\n",
    "# =============================================================================\n",
    "# DELTA LAKE TESTING SECTION\n",
    "# =============================================================================\n",
    "\n",
    "if TEST_DELTA_LAKE and env_verification.get('delta_available', False):\n",
    "    print(\"üî∫ Testing Delta Lake support...\")\n",
    "    \n",
    "    delta_test_start = time.time()\n",
    "    delta_results = {}\n",
    "    \n",
    "    try:\n",
    "        # Test Delta Lake conversion\n",
    "        print(\"Testing Delta Lake format conversion...\")\n",
    "        \n",
    "        if actual_csv_path:\n",
    "            delta_output_path = f\"{VOLUME_PATH}/serverless-test-output/delta_table\"\n",
    "            \n",
    "            delta_convert_start = time.time()\n",
    "            \n",
    "            # Convert to Delta Lake format\n",
    "            delta_result = forge.convert(\n",
    "                input_path=actual_csv_path,\n",
    "                output_path=delta_output_path,\n",
    "                format=\"delta\",\n",
    "                partition_by=\"category_col_6\" if dataset_config['columns'] > 7 else None\n",
    "            )\n",
    "            \n",
    "            delta_convert_time = time.time() - delta_convert_start\n",
    "            \n",
    "            print(f\"‚úÖ Delta Lake conversion completed in {delta_convert_time:.2f} seconds\")\n",
    "            print(f\"   Rows processed: {delta_result['rows_processed']:,}\")\n",
    "            print(f\"   Engine used: {delta_result['engine_used']}\")\n",
    "            \n",
    "            # Verify Delta table\n",
    "            delta_df = spark.read.format(\"delta\").load(delta_output_path)\n",
    "            delta_count = delta_df.count()\n",
    "            \n",
    "            print(f\"‚úÖ Delta table verification: {delta_count:,} rows\")\n",
    "            \n",
    "            # Test Delta table operations\n",
    "            try:\n",
    "                # Test ACID properties - simple update\n",
    "                from delta.tables import DeltaTable\n",
    "                \n",
    "                delta_table = DeltaTable.forPath(spark, delta_output_path)\n",
    "                \n",
    "                # Get table history\n",
    "                history = delta_table.history().select(\"version\", \"timestamp\", \"operation\").collect()\n",
    "                print(f\"‚úÖ Delta table history: {len(history)} versions\")\n",
    "                \n",
    "                delta_results['delta_conversion'] = {\n",
    "                    'success': True,\n",
    "                    'conversion_time': delta_convert_time,\n",
    "                    'rows_processed': delta_result['rows_processed'],\n",
    "                    'delta_count': delta_count,\n",
    "                    'versions': len(history),\n",
    "                    'acid_support': True\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Delta table operations test failed: {e}\")\n",
    "                delta_results['delta_conversion'] = {\n",
    "                    'success': True,\n",
    "                    'conversion_time': delta_convert_time,\n",
    "                    'rows_processed': delta_result['rows_processed'],\n",
    "                    'acid_support': False,\n",
    "                    'acid_error': str(e)\n",
    "                }\n",
    "        \n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No test file available for Delta Lake test\")\n",
    "            delta_results['delta_conversion'] = {'skipped': 'no_test_file'}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Delta Lake testing failed: {e}\")\n",
    "        delta_results['error'] = str(e)\n",
    "    \n",
    "    delta_test_time = time.time() - delta_test_start\n",
    "    delta_results['total_test_time'] = delta_test_time\n",
    "    serverless_test_results['delta_lake_support'] = delta_results\n",
    "    serverless_test_results['performance_metrics']['delta_test_time'] = delta_test_time\n",
    "    \n",
    "    print(f\"‚è±Ô∏è Delta Lake testing time: {delta_test_time:.2f} seconds\")\n",
    "\n",
    "else:\n",
    "    skip_reason = \"disabled\" if not TEST_DELTA_LAKE else \"delta_unavailable\"\n",
    "    print(f\"‚è≠Ô∏è Delta Lake testing skipped ({skip_reason})\")\n",
    "    serverless_test_results['delta_lake_support'] = {'skipped': skip_reason}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Serverless Test Results Summary\n",
    "# =============================================================================\n",
    "# RESULTS SUMMARY SECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìä Generating serverless test results summary...\")\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Add metadata\n",
    "serverless_test_results['metadata'] = {\n",
    "    'test_type': 'serverless_functional',\n",
    "    'notebook_name': '03-databricks-extension-serverless-test',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'dataset_config': dataset_config,\n",
    "    'configuration': {\n",
    "        'pyforge_version': PYFORGE_VERSION,\n",
    "        'volume_path': VOLUME_PATH,\n",
    "        'test_dataset_size': TEST_DATASET_SIZE,\n",
    "        'test_streaming': TEST_STREAMING,\n",
    "        'test_delta_lake': TEST_DELTA_LAKE,\n",
    "        'test_memory_optimization': TEST_MEMORY_OPTIMIZATION\n",
    "    }\n",
    "}\n",
    "\n",
    "# Calculate overall success\n",
    "critical_tests = [\n",
    "    serverless_test_results['environment_verification'].get('verification_success', False),\n",
    "    serverless_test_results.get('extension_installation', {}).get('installation_success', False),\n",
    "    serverless_test_results.get('pyspark_optimization', {}).get('csv_conversion', {}).get('success', False)\n",
    "]\n",
    "\n",
    "overall_success = all(critical_tests)\n",
    "serverless_test_results['overall_success'] = overall_success\n",
    "\n",
    "# Calculate total test time\n",
    "total_test_time = sum(serverless_test_results['performance_metrics'].values())\n",
    "serverless_test_results['total_test_time'] = total_test_time\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ SERVERLESS DATABRICKS EXTENSION TEST SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "status_icon = \"‚úÖ\" if overall_success else \"‚ùå\"\n",
    "print(f\"{status_icon} Overall Test Status: {'PASSED' if overall_success else 'FAILED'}\")\n",
    "print(f\"üïê Total Test Duration: {total_test_time:.2f} seconds\")\n",
    "print(f\"üåê Environment: {serverless_test_results['environment_verification'].get('runtime_version', 'unknown')}\")\n",
    "print(f\"üì¶ Dataset Size: {TEST_DATASET_SIZE} ({dataset_config['rows']:,} rows)\")\n",
    "\n",
    "print(\"\\nüìã Test Results:\")\n",
    "print(f\"   Environment Verification: {'‚úÖ' if serverless_test_results['environment_verification'].get('verification_success') else '‚ùå'}\")\n",
    "print(f\"   Extension Installation: {'‚úÖ' if serverless_test_results.get('extension_installation', {}).get('installation_success') else '‚ùå'}\")\n",
    "print(f\"   PySpark Optimization: {'‚úÖ' if serverless_test_results.get('pyspark_optimization', {}).get('csv_conversion', {}).get('success') else '‚ùå'}\")\n",
    "\n",
    "if not serverless_test_results.get('streaming_support', {}).get('skipped'):\n",
    "    print(f\"   Streaming Support: {'‚úÖ' if serverless_test_results.get('streaming_support', {}).get('streaming_conversion', {}).get('success') else '‚ùå'}\")\n",
    "\n",
    "if not serverless_test_results.get('delta_lake_support', {}).get('skipped'):\n",
    "    print(f\"   Delta Lake Support: {'‚úÖ' if serverless_test_results.get('delta_lake_support', {}).get('delta_conversion', {}).get('success') else '‚ùå'}\")\n",
    "\n",
    "print(\"\\n‚ö° Performance Metrics:\")\n",
    "pyspark_csv = serverless_test_results.get('pyspark_optimization', {}).get('csv_conversion', {})\n",
    "if pyspark_csv.get('success'):\n",
    "    print(f\"   CSV Conversion Throughput: {pyspark_csv.get('throughput_rows_per_sec', 0):,.0f} rows/sec\")\n",
    "    print(f\"   Conversion Time: {pyspark_csv.get('conversion_time', 0):.2f} seconds\")\n",
    "    print(f\"   Engine Used: {pyspark_csv.get('engine_used', 'unknown')}\")\n",
    "\n",
    "streaming_conv = serverless_test_results.get('streaming_support', {}).get('streaming_conversion', {})\n",
    "if streaming_conv.get('success'):\n",
    "    efficiency = streaming_conv.get('streaming_efficiency')\n",
    "    if efficiency:\n",
    "        print(f\"   Streaming Efficiency: {efficiency:.2f}x {'faster' if efficiency > 1 else 'slower'} than batch\")\n",
    "\n",
    "print(\"\\n‚è±Ô∏è Time Breakdown:\")\n",
    "for metric, time_val in serverless_test_results['performance_metrics'].items():\n",
    "    print(f\"   {metric.replace('_', ' ').title()}: {time_val:.2f}s\")\n",
    "\n",
    "# Save results\n",
    "results_json = json.dumps(serverless_test_results, indent=2, default=str)\n",
    "print(\"\\nüíæ Serverless test results saved to serverless_test_results variable\")\n",
    "\n",
    "if VERBOSE_LOGGING:\n",
    "    print(\"\\nüìù Detailed Results:\")\n",
    "    print(results_json[:1500] + \"...\" if len(results_json) > 1500 else results_json)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üèÅ Serverless testing completed!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}