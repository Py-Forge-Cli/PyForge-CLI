{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyForge CLI Databricks Extension - Functional Testing\n",
    "\n",
    "This notebook tests PyForge CLI Databricks extension functionality following established patterns with:\n",
    "- Sample dataset downloads and conversions\n",
    "- Environment detection and optimization\n",
    "- PySpark vs pandas fallback testing\n",
    "- Unity Catalog Volume operations\n",
    "- Comprehensive API method testing\n",
    "\n",
    "## Databricks Widgets\n",
    "This notebook uses Databricks widgets for easy parameter configuration:\n",
    "\n",
    "- **sample_datasets_base_path**: Base path for sample datasets installation\n",
    "- **pyforge_version**: PyForge CLI version to test\n",
    "- **databricks_username**: Your Databricks username\n",
    "- **force_conversion**: Whether to force overwrite existing conversions\n",
    "- **test_smallest_files_only**: Test only the smallest file of each type\n",
    "- **test_databricks_extension**: Enable Databricks extension specific tests\n",
    "\n",
    "## Test Configuration\n",
    "- **Environment**: Auto-detected (Serverless/Classic)\n",
    "- **Installation Source**: Unity Catalog Volume (deployed wheel)\n",
    "- **Sample Data**: Real sample datasets with Databricks extension API\n",
    "- **Output Format**: Parquet/Delta optimized for Databricks\n",
    "\n",
    "## Prerequisites\n",
    "1. PyForge CLI wheel with Databricks extension deployed to volume\n",
    "2. Unity Catalog access permissions\n",
    "3. Databricks extension properly installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# DBTITLE 1,Initialize Databricks Extension Widgets\n",
    "# =============================================================================\n",
    "# DATABRICKS WIDGETS INITIALIZATION FOR EXTENSION TESTING\n",
    "# =============================================================================\n",
    "\n",
    "# Remove any existing widgets to ensure clean state\n",
    "dbutils.widgets.removeAll()\n",
    "\n",
    "# Create widgets for notebook parameters\n",
    "dbutils.widgets.text(\n",
    "    \"sample_datasets_base_path\", \n",
    "    \"/Volumes/main/default/pyforge/sample-datasets/\",\n",
    "    \"Sample Datasets Base Path\"\n",
    ")\n",
    "\n",
    "dbutils.widgets.text(\n",
    "    \"pyforge_version\",\n",
    "    \"latest\",\n",
    "    \"PyForge Version\"\n",
    ")\n",
    "\n",
    "dbutils.widgets.text(\n",
    "    \"databricks_username\",\n",
    "    \"your-username@company.com\",\n",
    "    \"Databricks Username\"\n",
    ")\n",
    "\n",
    "dbutils.widgets.dropdown(\n",
    "    \"force_conversion\",\n",
    "    \"True\",\n",
    "    [\"True\", \"False\"],\n",
    "    \"Force Conversion\"\n",
    ")\n",
    "\n",
    "dbutils.widgets.dropdown(\n",
    "    \"test_smallest_files_only\",\n",
    "    \"True\",\n",
    "    [\"True\", \"False\"],\n",
    "    \"Test Smallest Files Only\"\n",
    ")\n",
    "\n",
    "dbutils.widgets.dropdown(\n",
    "    \"test_databricks_extension\",\n",
    "    \"True\",\n",
    "    [\"True\", \"False\"],\n",
    "    \"Test Databricks Extension API\"\n",
    ")\n",
    "\n",
    "dbutils.widgets.dropdown(\n",
    "    \"test_large_files\",\n",
    "    \"False\",\n",
    "    [\"True\", \"False\"],\n",
    "    \"Test Large Files (>10MB)\"\n",
    ")\n",
    "\n",
    "# Display widget values\n",
    "print(\"üìã Databricks Extension Widget Parameters Initialized:\")\n",
    "print(f\"   Sample Datasets Base Path: {dbutils.widgets.get('sample_datasets_base_path')}\")\n",
    "print(f\"   PyForge Version: {dbutils.widgets.get('pyforge_version')}\")\n",
    "print(f\"   Databricks Username: {dbutils.widgets.get('databricks_username')}\")\n",
    "print(f\"   Force Conversion: {dbutils.widgets.get('force_conversion')}\")\n",
    "print(f\"   Test Smallest Files Only: {dbutils.widgets.get('test_smallest_files_only')}\")\n",
    "print(f\"   Test Databricks Extension: {dbutils.widgets.get('test_databricks_extension')}\")\n",
    "print(f\"   Test Large Files: {dbutils.widgets.get('test_large_files')}\")\n",
    "\n",
    "print(\"\\n‚úÖ Widgets created successfully! Modify parameters using widgets above.\")\n",
    "print(\"üîß This notebook will test PyForge CLI Databricks extension functionality.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# DBTITLE 1,Configuration from Widgets\n",
    "# =============================================================================\n",
    "# CONFIGURATION SECTION - Using Widget Values\n",
    "# =============================================================================\n",
    "\n",
    "# Get widget values\n",
    "SAMPLE_DATASETS_BASE_PATH = dbutils.widgets.get(\"sample_datasets_base_path\")\n",
    "PYFORGE_VERSION = dbutils.widgets.get(\"pyforge_version\")\n",
    "DATABRICKS_USERNAME = dbutils.widgets.get(\"databricks_username\")\n",
    "FORCE_CONVERSION = dbutils.widgets.get(\"force_conversion\").lower() == \"true\"\n",
    "TEST_SMALLEST_FILES_ONLY = dbutils.widgets.get(\"test_smallest_files_only\").lower() == \"true\"\n",
    "TEST_DATABRICKS_EXTENSION = dbutils.widgets.get(\"test_databricks_extension\").lower() == \"true\"\n",
    "TEST_LARGE_FILES = dbutils.widgets.get(\"test_large_files\").lower() == \"true\"\n",
    "\n",
    "# Derived paths for Databricks extension testing\n",
    "if PYFORGE_VERSION == \"latest\":\n",
    "    PYFORGE_WHEEL_PATH = f\"/Volumes/main/default/pyforge/wheels/pyforge_cli-latest-py3-none-any.whl\"\n",
    "else:\n",
    "    PYFORGE_WHEEL_PATH = f\"/Volumes/main/default/pyforge/wheels/pyforge_cli-{PYFORGE_VERSION}-py3-none-any.whl\"\n",
    "\n",
    "SAMPLE_DATASETS_PATH = SAMPLE_DATASETS_BASE_PATH.rstrip('/')  \n",
    "CONVERTED_OUTPUT_PATH = SAMPLE_DATASETS_PATH.replace('/sample-datasets', '/converted_output')\n",
    "DATABRICKS_TEST_OUTPUT = SAMPLE_DATASETS_PATH.replace('/sample-datasets', '/databricks_extension_output')\n",
    "\n",
    "print(f\"üîß Databricks Extension Configuration:\")\n",
    "print(f\"   PyForge Version: {PYFORGE_VERSION}\")\n",
    "print(f\"   PyForge Wheel Path: {PYFORGE_WHEEL_PATH}\")\n",
    "print(f\"   Sample Datasets Path: {SAMPLE_DATASETS_PATH}\")\n",
    "print(f\"   Standard Output Path: {CONVERTED_OUTPUT_PATH}\")\n",
    "print(f\"   Databricks Extension Output: {DATABRICKS_TEST_OUTPUT}\")\n",
    "print(f\"   Force Conversion: {FORCE_CONVERSION}\")\n",
    "print(f\"   Test Smallest Files Only: {TEST_SMALLEST_FILES_ONLY}\")\n",
    "print(f\"   Test Databricks Extension: {TEST_DATABRICKS_EXTENSION}\")\n",
    "print(f\"   Test Large Files: {TEST_LARGE_FILES}\")\n",
    "\n",
    "# Initialize test tracking\n",
    "databricks_test_results = {\n",
    "    'environment_detection': None,\n",
    "    'extension_loading': None,\n",
    "    'api_methods': {},\n",
    "    'volume_operations': None,\n",
    "    'sample_data_tests': {},\n",
    "    'large_file_tests': {},\n",
    "    'performance_metrics': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# DBTITLE 1,Environment Check and PyForge Installation\n",
    "# =============================================================================\n",
    "# ENVIRONMENT VERIFICATION AND INSTALLATION\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üîç Verifying Databricks environment for extension testing...\")\n",
    "\n",
    "# Detect environment type\n",
    "runtime_version = os.environ.get('DATABRICKS_RUNTIME_VERSION', 'unknown')\n",
    "is_serverless = 'serverless' in runtime_version.lower()\n",
    "environment_type = 'serverless' if is_serverless else 'classic'\n",
    "\n",
    "print(f\"‚úÖ Runtime Version: {runtime_version}\")\n",
    "print(f\"‚úÖ Environment Type: {environment_type}\")\n",
    "\n",
    "# Check for PySpark availability\n",
    "pyspark_available = False\n",
    "try:\n",
    "    import pyspark\n",
    "    pyspark_version = pyspark.__version__\n",
    "    pyspark_available = True\n",
    "    print(f\"‚úÖ PySpark available: {pyspark_version}\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è PySpark not available - will test fallback behavior\")\n",
    "\n",
    "databricks_test_results['environment_detection'] = {\n",
    "    'runtime_version': runtime_version,\n",
    "    'environment_type': environment_type,\n",
    "    'pyspark_available': pyspark_available,\n",
    "    'pyspark_version': pyspark_version if pyspark_available else None\n",
    "}\n",
    "\n",
    "# Install PyForge CLI with Databricks extension\n",
    "print(f\"\\nüì¶ Installing PyForge CLI with Databricks extension...\")\n",
    "%pip install pyforge-cli[databricks] --no-cache-dir --quiet --index-url https://pypi.org/simple/ --trusted-host pypi.org\n",
    "\n",
    "print(f\"‚úÖ PyForge CLI with Databricks extension installed!\")\n",
    "print(\"üîÑ Restarting Python environment...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Restart Python to ensure clean environment\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# DBTITLE 1,Re-initialize Configuration After Restart\n",
    "# =============================================================================\n",
    "# VARIABLE RE-INITIALIZATION AFTER PYTHON RESTART\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Re-initialize all configuration variables from widgets\n",
    "SAMPLE_DATASETS_BASE_PATH = dbutils.widgets.get(\"sample_datasets_base_path\")\n",
    "PYFORGE_VERSION = dbutils.widgets.get(\"pyforge_version\")\n",
    "DATABRICKS_USERNAME = dbutils.widgets.get(\"databricks_username\")\n",
    "FORCE_CONVERSION = dbutils.widgets.get(\"force_conversion\").lower() == \"true\"\n",
    "TEST_SMALLEST_FILES_ONLY = dbutils.widgets.get(\"test_smallest_files_only\").lower() == \"true\"\n",
    "TEST_DATABRICKS_EXTENSION = dbutils.widgets.get(\"test_databricks_extension\").lower() == \"true\"\n",
    "TEST_LARGE_FILES = dbutils.widgets.get(\"test_large_files\").lower() == \"true\"\n",
    "\n",
    "# Derived paths\n",
    "SAMPLE_DATASETS_PATH = SAMPLE_DATASETS_BASE_PATH.rstrip('/')  \n",
    "CONVERTED_OUTPUT_PATH = SAMPLE_DATASETS_PATH.replace('/sample-datasets', '/converted_output')\n",
    "DATABRICKS_TEST_OUTPUT = SAMPLE_DATASETS_PATH.replace('/sample-datasets', '/databricks_extension_output')\n",
    "\n",
    "# Re-initialize test tracking\n",
    "databricks_test_results = {\n",
    "    'environment_detection': None,\n",
    "    'extension_loading': None,\n",
    "    'api_methods': {},\n",
    "    'volume_operations': None,\n",
    "    'sample_data_tests': {},\n",
    "    'large_file_tests': {},\n",
    "    'performance_metrics': {}\n",
    "}\n",
    "\n",
    "print(f\"üîÑ Configuration restored after Python restart:\")\n",
    "print(f\"   Sample Datasets Path: {SAMPLE_DATASETS_PATH}\")\n",
    "print(f\"   Databricks Extension Output: {DATABRICKS_TEST_OUTPUT}\")\n",
    "print(f\"   Test Databricks Extension: {TEST_DATABRICKS_EXTENSION}\")\n",
    "print(f\"   Test Large Files: {TEST_LARGE_FILES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# DBTITLE 1,Verify Databricks Extension Installation\n",
    "# =============================================================================\n",
    "# DATABRICKS EXTENSION VERIFICATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üîç Verifying Databricks extension installation...\")\n",
    "\n",
    "# Test PyForge CLI basic functionality\n",
    "try:\n",
    "    import pyforge_cli\n",
    "    print(f\"‚úÖ PyForge CLI imported: {pyforge_cli.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import PyForge CLI: {e}\")\n",
    "    raise\n",
    "\n",
    "# Test Databricks extension import\n",
    "extension_loaded = False\n",
    "try:\n",
    "    from pyforge_cli.extensions.databricks.pyforge_databricks import PyForgeDatabricks\n",
    "    print(f\"‚úÖ Databricks extension imported successfully\")\n",
    "    extension_loaded = True\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Databricks extension not available: {e}\")\n",
    "    print(f\"   Will test core PyForge CLI functionality only\")\n",
    "\n",
    "# Test extension initialization if available\n",
    "if extension_loaded and TEST_DATABRICKS_EXTENSION:\n",
    "    try:\n",
    "        forge = PyForgeDatabricks(auto_init=True)\n",
    "        print(f\"‚úÖ PyForgeDatabricks initialized successfully\")\n",
    "        \n",
    "        # Get environment info\n",
    "        env_info = forge.get_environment_info()\n",
    "        print(f\"‚úÖ Environment detected: {env_info['compute_type']} ({env_info['runtime_version']})\")\n",
    "        \n",
    "        databricks_test_results['extension_loading'] = {\n",
    "            'success': True,\n",
    "            'api_available': True,\n",
    "            'environment_info': env_info\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Databricks extension initialization failed: {e}\")\n",
    "        databricks_test_results['extension_loading'] = {\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "        extension_loaded = False\n",
    "else:\n",
    "    databricks_test_results['extension_loading'] = {\n",
    "        'success': False,\n",
    "        'skipped': True,\n",
    "        'reason': 'Extension not available or testing disabled'\n",
    "    }\n",
    "\n",
    "print(f\"\\nüîß Extension Status: {'Available' if extension_loaded else 'Not Available'}\")\n",
    "print(f\"   Testing Mode: {'Databricks Extension' if (extension_loaded and TEST_DATABRICKS_EXTENSION) else 'Core CLI Only'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# DBTITLE 1,Setup Sample Datasets\n",
    "# =============================================================================\n",
    "# SAMPLE DATASETS SETUP FOLLOWING ESTABLISHED PATTERNS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"üì• Setting up sample datasets in volume: {SAMPLE_DATASETS_PATH}\")\n",
    "\n",
    "# Create volume directories\n",
    "volume_datasets_path = SAMPLE_DATASETS_PATH.replace('/Volumes/', 'dbfs:/Volumes/')\n",
    "volume_output_path = CONVERTED_OUTPUT_PATH.replace('/Volumes/', 'dbfs:/Volumes/')\n",
    "volume_databricks_output = DATABRICKS_TEST_OUTPUT.replace('/Volumes/', 'dbfs:/Volumes/')\n",
    "\n",
    "try:\n",
    "    # Create directories\n",
    "    for path in [volume_datasets_path, volume_output_path, volume_databricks_output]:\n",
    "        dbutils.fs.mkdirs(path)\n",
    "        print(f\"‚úÖ Created directory: {path.replace('dbfs:/Volumes/', '/Volumes/')}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Directory creation warning: {e}\")\n",
    "\n",
    "# Install sample datasets using PyForge CLI\n",
    "print(\"\\nüì¶ Installing sample datasets...\")\n",
    "try:\n",
    "    result = subprocess.run([\n",
    "        'pyforge', 'install', 'sample-datasets', SAMPLE_DATASETS_PATH, '--force'\n",
    "    ], capture_output=True, text=True, timeout=300)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Sample datasets installed successfully!\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Sample datasets installation had issues, creating test data...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Creating minimal test datasets: {e}\")\n",
    "\n",
    "# Create test datasets for Databricks extension testing\n",
    "test_datasets = {\n",
    "    'csv': {\n",
    "        'small_sales.csv': '''id,product,category,price,quantity,date,region\n",
    "1,Laptop,Electronics,999.99,2,2023-01-15,North\n",
    "2,Coffee Maker,Appliances,89.99,1,2023-01-16,South\n",
    "3,Desk Chair,Furniture,159.99,1,2023-01-17,East\n",
    "4,Monitor,Electronics,299.99,3,2023-01-18,West\n",
    "5,Printer,Electronics,149.99,1,2023-01-19,North''',\n",
    "        \n",
    "        'medium_inventory.csv': '''\\n'''.join([\n",
    "            'sku,name,category,cost,retail_price,stock,supplier,last_updated',\n",
    "            *[f'SKU{i:04d},Product {i},{[\"Electronics\",\"Furniture\",\"Clothing\",\"Books\"][i%4]},{50+i*2},{100+i*5},{100-i},Supplier{i%3+1},2023-01-{(i%28)+1:02d}' \n",
    "              for i in range(1, 501)]\n",
    "        ])\n",
    "    },\n",
    "    'xml': {\n",
    "        'catalog.xml': '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<catalog>\n",
    "    <products>\n",
    "        <product id=\"1\">\n",
    "            <name>Wireless Headphones</name>\n",
    "            <category>Electronics</category>\n",
    "            <price currency=\"USD\">79.99</price>\n",
    "            <stock>50</stock>\n",
    "            <specs>\n",
    "                <battery>20 hours</battery>\n",
    "                <color>Black</color>\n",
    "                <wireless>true</wireless>\n",
    "            </specs>\n",
    "        </product>\n",
    "        <product id=\"2\">\n",
    "            <name>Gaming Mouse</name>\n",
    "            <category>Electronics</category>\n",
    "            <price currency=\"USD\">49.99</price>\n",
    "            <stock>75</stock>\n",
    "            <specs>\n",
    "                <dpi>12000</dpi>\n",
    "                <color>RGB</color>\n",
    "                <wireless>false</wireless>\n",
    "            </specs>\n",
    "        </product>\n",
    "    </products>\n",
    "</catalog>'''\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create test files\n",
    "for format_type, files in test_datasets.items():\n",
    "    format_dir = f\"{SAMPLE_DATASETS_PATH}/{format_type}\"\n",
    "    dbutils.fs.mkdirs(format_dir.replace('/Volumes/', 'dbfs:/Volumes/'))\n",
    "    \n",
    "    for filename, content in files.items():\n",
    "        file_path = f\"{format_dir}/{filename}\"\n",
    "        dbutils.fs.put(file_path.replace('/Volumes/', 'dbfs:/Volumes/'), content, overwrite=True)\n",
    "        print(f\"‚úÖ Created {format_type.upper()} test file: {filename}\")\n",
    "\n",
    "print(\"\\n‚úÖ Sample datasets setup completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# DBTITLE 1,Discover Test Files\n",
    "# =============================================================================\n",
    "# FILE DISCOVERY FOR DATABRICKS EXTENSION TESTING\n",
    "# =============================================================================\n",
    "\n",
    "def discover_test_files():\n",
    "    \"\"\"Discover all test files for Databricks extension testing.\"\"\"\n",
    "    print(\"üîç Discovering test files for Databricks extension testing...\")\n",
    "    \n",
    "    all_files = []\n",
    "    supported_extensions = {'.csv': 'CSV', '.xml': 'XML', '.xlsx': 'Excel'}\n",
    "    \n",
    "    def list_files_recursive(path, prefix=\"\"):\n",
    "        items = []\n",
    "        try:\n",
    "            files = dbutils.fs.ls(path)\n",
    "            for file_info in files:\n",
    "                if file_info.isDir():\n",
    "                    subdir_items = list_files_recursive(file_info.path, prefix + file_info.name + \"/\")\n",
    "                    items.extend(subdir_items)\n",
    "                else:\n",
    "                    items.append({\n",
    "                        'path': file_info.path,\n",
    "                        'name': file_info.name,\n",
    "                        'size': file_info.size,\n",
    "                        'relative_path': prefix + file_info.name\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"   Warning: Could not list {path}: {e}\")\n",
    "        return items\n",
    "    \n",
    "    # Get all files\n",
    "    volume_path = SAMPLE_DATASETS_PATH.replace('/Volumes/', 'dbfs:/Volumes/')\n",
    "    all_files_raw = list_files_recursive(volume_path)\n",
    "    \n",
    "    # Process files\n",
    "    for file_info in all_files_raw:\n",
    "        file_name = file_info['name']\n",
    "        file_ext = '.' + file_name.split('.')[-1].lower() if '.' in file_name else ''\n",
    "        \n",
    "        if file_ext in supported_extensions:\n",
    "            file_path = file_info['path'].replace('dbfs:/Volumes/', '/Volumes/')\n",
    "            \n",
    "            # Categorize by size for testing\n",
    "            size_mb = file_info['size'] / (1024*1024)\n",
    "            if size_mb < 0.1:\n",
    "                size_category = 'tiny'\n",
    "            elif size_mb < 1:\n",
    "                size_category = 'small' \n",
    "            elif size_mb < 10:\n",
    "                size_category = 'medium'\n",
    "            else:\n",
    "                size_category = 'large'\n",
    "            \n",
    "            file_dict = {\n",
    "                'file_name': file_name,\n",
    "                'file_type': supported_extensions[file_ext],\n",
    "                'extension': file_ext,\n",
    "                'file_path': file_path,\n",
    "                'size_bytes': file_info['size'],\n",
    "                'size_mb': round(size_mb, 3),\n",
    "                'size_category': size_category,\n",
    "                'relative_path': file_info['relative_path']\n",
    "            }\n",
    "            all_files.append(file_dict)\n",
    "    \n",
    "    return all_files\n",
    "\n",
    "# Discover files\n",
    "test_files = discover_test_files()\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nüìä Test Files Summary:\")\n",
    "print(f\"   Total files found: {len(test_files)}\")\n",
    "\n",
    "if test_files:\n",
    "    df_files = pd.DataFrame(test_files)\n",
    "    \n",
    "    # Group by type and size\n",
    "    type_summary = df_files.groupby(['file_type', 'size_category']).size().unstack(fill_value=0)\n",
    "    print(f\"\\nüìã Files by Type and Size:\")\n",
    "    display(type_summary)\n",
    "    \n",
    "    # Show all files\n",
    "    print(f\"\\nüìÅ All Test Files:\")\n",
    "    display(df_files[['file_name', 'file_type', 'size_category', 'size_mb', 'relative_path']])\n",
    "    \n",
    "    # Select files for testing\n",
    "    if TEST_SMALLEST_FILES_ONLY:\n",
    "        # Get smallest file of each type\n",
    "        selected_files = []\n",
    "        for file_type in df_files['file_type'].unique():\n",
    "            type_files = df_files[df_files['file_type'] == file_type].sort_values('size_bytes')\n",
    "            if len(type_files) > 0:\n",
    "                selected_files.append(type_files.iloc[0].to_dict())\n",
    "        \n",
    "        print(f\"\\nüéØ Selected smallest files for testing: {len(selected_files)}\")\n",
    "    else:\n",
    "        selected_files = test_files\n",
    "        print(f\"\\nüìã Selected all files for testing: {len(selected_files)}\")\n",
    "    \n",
    "    # Filter large files if not testing them\n",
    "    if not TEST_LARGE_FILES:\n",
    "        selected_files = [f for f in selected_files if f['size_category'] != 'large']\n",
    "        print(f\"   Excluding large files: {len(selected_files)} files remaining\")\n",
    "    \n",
    "else:\n",
    "    selected_files = []\n",
    "    print(\"‚ö†Ô∏è No test files found\")\n",
    "\n",
    "print(f\"\\n‚úÖ File discovery completed. {len(selected_files)} files ready for testing.\")\n",
    "files_for_testing = selected_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# DBTITLE 1,Test Databricks Extension API Methods\n",
    "# =============================================================================\n",
    "# DATABRICKS EXTENSION API TESTING\n",
    "# =============================================================================\n",
    "\n",
    "if TEST_DATABRICKS_EXTENSION and extension_loaded:\n",
    "    print(\"üß™ Testing Databricks Extension API methods...\")\n",
    "    \n",
    "    api_test_results = {}\n",
    "    \n",
    "    try:\n",
    "        # Test get_environment_info()\n",
    "        print(\"\\nüîç Testing get_environment_info()...\")\n",
    "        env_info = forge.get_environment_info()\n",
    "        print(f\"   ‚úÖ Environment: {env_info['compute_type']} ({env_info['runtime_version']})\")\n",
    "        print(f\"   ‚úÖ Databricks: {env_info['is_databricks']}\")\n",
    "        api_test_results['get_environment_info'] = True\n",
    "        \n",
    "        # Test get_info() method with sample file\n",
    "        if files_for_testing:\n",
    "            print(\"\\nüìä Testing get_info() method...\")\n",
    "            test_file = files_for_testing[0]\n",
    "            file_info = forge.get_info(test_file['file_path'])\n",
    "            print(f\"   ‚úÖ File info for {test_file['file_name']}:\")\n",
    "            print(f\"      Format: {file_info['format']}\")\n",
    "            print(f\"      Size: {file_info.get('size', 'unknown')} bytes\")\n",
    "            api_test_results['get_info'] = True\n",
    "        \n",
    "            # Test validate() method\n",
    "            print(\"\\n‚úÖ Testing validate() method...\")\n",
    "            validation_result = forge.validate(test_file['file_path'])\n",
    "            print(f\"   ‚úÖ Validation result: {validation_result['valid']}\")\n",
    "            if not validation_result['valid']:\n",
    "                print(f\"      Issues: {validation_result.get('issues', [])}\")\n",
    "            api_test_results['validate'] = True\n",
    "        \n",
    "            # Test convert() method\n",
    "            print(\"\\nüîÑ Testing convert() method...\")\n",
    "            output_path = f\"{DATABRICKS_TEST_OUTPUT}/api_test_{test_file['file_name']}.parquet\"\n",
    "            \n",
    "            conversion_start = time.time()\n",
    "            conversion_result = forge.convert(\n",
    "                input_path=test_file['file_path'],\n",
    "                output_path=output_path,\n",
    "                format=\"parquet\"\n",
    "            )\n",
    "            conversion_time = time.time() - conversion_start\n",
    "            \n",
    "            print(f\"   ‚úÖ Conversion successful in {conversion_time:.2f}s\")\n",
    "            print(f\"      Rows processed: {conversion_result['rows_processed']}\")\n",
    "            print(f\"      Engine used: {conversion_result['engine_used']}\")\n",
    "            print(f\"      Output: {conversion_result['output_path']}\")\n",
    "            api_test_results['convert'] = True\n",
    "            \n",
    "            # Test batch_convert() with multiple small files\n",
    "            if len(files_for_testing) > 1:\n",
    "                print(\"\\nüì¶ Testing batch_convert() method...\")\n",
    "                batch_files = files_for_testing[:3]  # Test with first 3 files\n",
    "                \n",
    "                # Create input pattern or file list\n",
    "                file_paths = [f['file_path'] for f in batch_files]\n",
    "                batch_output_dir = f\"{DATABRICKS_TEST_OUTPUT}/batch_test\"\n",
    "                \n",
    "                batch_result = forge.batch_convert(\n",
    "                    input_files=file_paths,\n",
    "                    output_dir=batch_output_dir,\n",
    "                    format=\"parquet\"\n",
    "                )\n",
    "                \n",
    "                print(f\"   ‚úÖ Batch conversion successful\")\n",
    "                print(f\"      Files processed: {batch_result['files_processed']}\")\n",
    "                print(f\"      Successful: {batch_result['successful']}\")\n",
    "                print(f\"      Failed: {batch_result['failed']}\")\n",
    "                api_test_results['batch_convert'] = True\n",
    "        \n",
    "        # Test install_sample_datasets() if available\n",
    "        print(\"\\nüì• Testing install_sample_datasets() method...\")\n",
    "        try:\n",
    "            sample_install_path = f\"{DATABRICKS_TEST_OUTPUT}/api_sample_datasets\"\n",
    "            install_result = forge.install_sample_datasets(\n",
    "                target_path=sample_install_path,\n",
    "                formats=['csv'],\n",
    "                max_size_mb=1\n",
    "            )\n",
    "            print(f\"   ‚úÖ Sample datasets installed: {install_result['datasets_installed']} datasets\")\n",
    "            api_test_results['install_sample_datasets'] = True\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Sample datasets installation failed: {e}\")\n",
    "            api_test_results['install_sample_datasets'] = False\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå API testing failed: {e}\")\n",
    "        api_test_results['error'] = str(e)\n",
    "    \n",
    "    databricks_test_results['api_methods'] = api_test_results\n",
    "    \n",
    "    # Display API test summary\n",
    "    print(f\"\\nüìä API Methods Test Summary:\")\n",
    "    successful_methods = sum(1 for k, v in api_test_results.items() if v is True)\n",
    "    total_methods = len([k for k, v in api_test_results.items() if isinstance(v, bool)])\n",
    "    print(f\"   Successful API methods: {successful_methods}/{total_methods}\")\n",
    "    \n",
    "    for method, result in api_test_results.items():\n",
    "        if isinstance(result, bool):\n",
    "            status = \"‚úÖ\" if result else \"‚ùå\"\n",
    "            print(f\"   {status} {method}()\")\n",
    "\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Databricks Extension API testing skipped\")\n",
    "    databricks_test_results['api_methods'] = {'skipped': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# DBTITLE 1,Test Sample Data Conversions\n",
    "# =============================================================================\n",
    "# SAMPLE DATA CONVERSION TESTING\n",
    "# =============================================================================\n",
    "\n",
    "def test_file_conversion(file_info, use_extension_api=False):\n",
    "    \"\"\"Test conversion of a single file using either core CLI or extension API.\"\"\"\n",
    "    file_path = file_info['file_path']\n",
    "    file_name = file_info['file_name']\n",
    "    file_type = file_info['file_type']\n",
    "    \n",
    "    print(f\"\\nüîÑ Converting {file_name} ({file_type}, {file_info['size_category']})...\")\n",
    "    \n",
    "    if use_extension_api and extension_loaded:\n",
    "        # Use Databricks extension API\n",
    "        output_path = f\"{DATABRICKS_TEST_OUTPUT}/extension_{file_name}.parquet\"\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            result = forge.convert(file_path, output_path, format=\"parquet\")\n",
    "            duration = time.time() - start_time\n",
    "            \n",
    "            print(f\"   ‚úÖ Extension API conversion successful ({duration:.2f}s)\")\n",
    "            print(f\"      Engine: {result['engine_used']}\")\n",
    "            print(f\"      Rows: {result['rows_processed']}\")\n",
    "            \n",
    "            return {\n",
    "                'method': 'extension_api',\n",
    "                'status': 'SUCCESS',\n",
    "                'duration': duration,\n",
    "                'engine': result['engine_used'],\n",
    "                'rows': result['rows_processed']\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Extension API conversion failed: {e}\")\n",
    "            return {\n",
    "                'method': 'extension_api',\n",
    "                'status': 'FAILED',\n",
    "                'error': str(e)\n",
    "            }\n",
    "    else:\n",
    "        # Use core CLI\n",
    "        output_path = f\"{CONVERTED_OUTPUT_PATH}/core_{file_name}.parquet\"\n",
    "        \n",
    "        try:\n",
    "            force_flag = '--force' if FORCE_CONVERSION else ''\n",
    "            cmd = ['pyforge', 'convert', file_path, output_path, '--format', 'parquet', force_flag]\n",
    "            cmd = [arg for arg in cmd if arg]  # Remove empty strings\n",
    "            \n",
    "            start_time = time.time()\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)\n",
    "            duration = time.time() - start_time\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                print(f\"   ‚úÖ Core CLI conversion successful ({duration:.2f}s)\")\n",
    "                engine = 'PySpark' if 'pyspark' in result.stdout.lower() else 'pandas'\n",
    "                return {\n",
    "                    'method': 'core_cli',\n",
    "                    'status': 'SUCCESS',\n",
    "                    'duration': duration,\n",
    "                    'engine': engine\n",
    "                }\n",
    "            else:\n",
    "                print(f\"   ‚ùå Core CLI conversion failed: {result.stderr}\")\n",
    "                return {\n",
    "                    'method': 'core_cli',\n",
    "                    'status': 'FAILED',\n",
    "                    'error': result.stderr\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Core CLI conversion error: {e}\")\n",
    "            return {\n",
    "                'method': 'core_cli',\n",
    "                'status': 'FAILED',\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "# Run conversion tests\n",
    "print(\"üöÄ Starting sample data conversion tests...\")\n",
    "conversion_results = []\n",
    "\n",
    "for file_info in files_for_testing:\n",
    "    # Test with core CLI\n",
    "    core_result = test_file_conversion(file_info, use_extension_api=False)\n",
    "    core_result['file_name'] = file_info['file_name']\n",
    "    core_result['file_type'] = file_info['file_type']\n",
    "    conversion_results.append(core_result)\n",
    "    \n",
    "    # Test with extension API if available\n",
    "    if TEST_DATABRICKS_EXTENSION and extension_loaded:\n",
    "        ext_result = test_file_conversion(file_info, use_extension_api=True)\n",
    "        ext_result['file_name'] = file_info['file_name']\n",
    "        ext_result['file_type'] = file_info['file_type']\n",
    "        conversion_results.append(ext_result)\n",
    "\n",
    "# Store results\n",
    "databricks_test_results['sample_data_tests'] = {\n",
    "    'files_tested': len(files_for_testing),\n",
    "    'conversion_results': conversion_results\n",
    "}\n",
    "\n",
    "# Display results summary\n",
    "print(f\"\\nüìä Conversion Test Results:\")\n",
    "if conversion_results:\n",
    "    df_results = pd.DataFrame(conversion_results)\n",
    "    \n",
    "    # Summary by method and status\n",
    "    summary = df_results.groupby(['method', 'status']).size().unstack(fill_value=0)\n",
    "    display(summary)\n",
    "    \n",
    "    # Detailed results\n",
    "    print(f\"\\nüìã Detailed Results:\")\n",
    "    display(df_results[['file_name', 'file_type', 'method', 'status', 'engine', 'duration']].fillna(''))\n",
    "    \n",
    "    # Performance comparison\n",
    "    successful = df_results[df_results['status'] == 'SUCCESS']\n",
    "    if len(successful) > 0:\n",
    "        print(f\"\\n‚ö° Performance Summary:\")\n",
    "        perf_summary = successful.groupby('method')['duration'].agg(['count', 'mean', 'min', 'max']).round(3)\n",
    "        display(perf_summary)\n",
    "\n",
    "print(f\"\\n‚úÖ Sample data conversion testing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# DBTITLE 1,Large File Testing (Optional)\n",
    "# =============================================================================\n",
    "# LARGE FILE TESTING FOR DATABRICKS EXTENSION\n",
    "# =============================================================================\n",
    "\n",
    "if TEST_LARGE_FILES:\n",
    "    print(\"üìà Testing large file handling...\")\n",
    "    \n",
    "    # Create larger test datasets\n",
    "    large_file_results = []\n",
    "    \n",
    "    # Generate large CSV file\n",
    "    print(\"\\nüìä Creating large CSV file...\")\n",
    "    large_csv_rows = 50000  # 50K rows\n",
    "    large_csv_content = \"id,name,category,value,timestamp,region,status\\n\"\n",
    "    \n",
    "    import random\n",
    "    categories = ['Electronics', 'Furniture', 'Clothing', 'Books', 'Sports']\n",
    "    regions = ['North', 'South', 'East', 'West', 'Central']\n",
    "    statuses = ['Active', 'Inactive', 'Pending']\n",
    "    \n",
    "    for i in range(1, large_csv_rows + 1):\n",
    "        row = f\"{i},Product_{i},{random.choice(categories)},{random.uniform(10, 1000):.2f},2023-{random.randint(1,12):02d}-{random.randint(1,28):02d},{random.choice(regions)},{random.choice(statuses)}\\n\"\n",
    "        large_csv_content += row\n",
    "    \n",
    "    large_csv_path = f\"{SAMPLE_DATASETS_PATH}/csv/large_dataset.csv\"\n",
    "    dbutils.fs.put(large_csv_path.replace('/Volumes/', 'dbfs:/Volumes/'), large_csv_content, overwrite=True)\n",
    "    \n",
    "    large_csv_size = len(large_csv_content.encode('utf-8')) / (1024*1024)\n",
    "    print(f\"‚úÖ Created large CSV: {large_csv_rows:,} rows, {large_csv_size:.2f} MB\")\n",
    "    \n",
    "    # Test large CSV conversion\n",
    "    print(\"\\nüîÑ Testing large CSV conversion...\")\n",
    "    \n",
    "    if extension_loaded and TEST_DATABRICKS_EXTENSION:\n",
    "        try:\n",
    "            output_path = f\"{DATABRICKS_TEST_OUTPUT}/large_dataset_extension.parquet\"\n",
    "            \n",
    "            start_time = time.time()\n",
    "            result = forge.convert(\n",
    "                input_path=large_csv_path,\n",
    "                output_path=output_path,\n",
    "                format=\"parquet\",\n",
    "                streaming=True  # Enable streaming for large files\n",
    "            )\n",
    "            duration = time.time() - start_time\n",
    "            \n",
    "            throughput = large_csv_rows / duration\n",
    "            print(f\"   ‚úÖ Extension API large file conversion successful\")\n",
    "            print(f\"      Duration: {duration:.2f}s\")\n",
    "            print(f\"      Throughput: {throughput:,.0f} rows/sec\")\n",
    "            print(f\"      Engine: {result['engine_used']}\")\n",
    "            print(f\"      Rows: {result['rows_processed']:,}\")\n",
    "            \n",
    "            large_file_results.append({\n",
    "                'file_type': 'Large CSV',\n",
    "                'method': 'extension_api',\n",
    "                'rows': large_csv_rows,\n",
    "                'size_mb': large_csv_size,\n",
    "                'duration': duration,\n",
    "                'throughput': throughput,\n",
    "                'engine': result['engine_used'],\n",
    "                'status': 'SUCCESS'\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Extension API large file conversion failed: {e}\")\n",
    "            large_file_results.append({\n",
    "                'file_type': 'Large CSV',\n",
    "                'method': 'extension_api',\n",
    "                'status': 'FAILED',\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # Test with core CLI for comparison\n",
    "    try:\n",
    "        output_path = f\"{CONVERTED_OUTPUT_PATH}/large_dataset_core.parquet\"\n",
    "        cmd = ['pyforge', 'convert', large_csv_path, output_path, '--format', 'parquet']\n",
    "        if FORCE_CONVERSION:\n",
    "            cmd.append('--force')\n",
    "        \n",
    "        start_time = time.time()\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)  # 10 min timeout\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            throughput = large_csv_rows / duration\n",
    "            engine = 'PySpark' if 'pyspark' in result.stdout.lower() else 'pandas'\n",
    "            print(f\"   ‚úÖ Core CLI large file conversion successful\")\n",
    "            print(f\"      Duration: {duration:.2f}s\")\n",
    "            print(f\"      Throughput: {throughput:,.0f} rows/sec\")\n",
    "            print(f\"      Engine: {engine}\")\n",
    "            \n",
    "            large_file_results.append({\n",
    "                'file_type': 'Large CSV',\n",
    "                'method': 'core_cli',\n",
    "                'rows': large_csv_rows,\n",
    "                'size_mb': large_csv_size,\n",
    "                'duration': duration,\n",
    "                'throughput': throughput,\n",
    "                'engine': engine,\n",
    "                'status': 'SUCCESS'\n",
    "            })\n",
    "        else:\n",
    "            print(f\"   ‚ùå Core CLI large file conversion failed: {result.stderr}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Core CLI large file conversion error: {e}\")\n",
    "    \n",
    "    # Store and display large file results\n",
    "    databricks_test_results['large_file_tests'] = large_file_results\n",
    "    \n",
    "    if large_file_results:\n",
    "        print(f\"\\nüìä Large File Test Results:\")\n",
    "        df_large = pd.DataFrame(large_file_results)\n",
    "        display(df_large)\n",
    "        \n",
    "        # Performance comparison\n",
    "        successful_large = df_large[df_large['status'] == 'SUCCESS']\n",
    "        if len(successful_large) > 1:\n",
    "            print(f\"\\n‚ö° Large File Performance Comparison:\")\n",
    "            for _, row in successful_large.iterrows():\n",
    "                print(f\"   {row['method']}: {row['throughput']:,.0f} rows/sec ({row['engine']})\")\n",
    "\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Large file testing skipped (disabled in configuration)\")\n",
    "    databricks_test_results['large_file_tests'] = {'skipped': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# DBTITLE 1,Databricks Extension Test Summary\n",
    "# =============================================================================\n",
    "# COMPREHENSIVE TEST SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üéØ PYFORGE CLI DATABRICKS EXTENSION TEST SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Environment info\n",
    "env_detection = databricks_test_results['environment_detection']\n",
    "if env_detection:\n",
    "    print(f\"üìÖ Test Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"üè¢ Environment: {env_detection['environment_type']}\")\n",
    "    print(f\"üîß Runtime: {env_detection['runtime_version']}\")\n",
    "    print(f\"üöÄ PySpark: {'‚úÖ Available' if env_detection['pyspark_available'] else '‚ùå Not Available'}\")\n",
    "\n",
    "# Extension loading\n",
    "ext_loading = databricks_test_results['extension_loading']\n",
    "print(f\"\\nüîå DATABRICKS EXTENSION:\")\n",
    "if ext_loading and ext_loading.get('success'):\n",
    "    print(f\"   ‚úÖ Extension loaded and initialized successfully\")\n",
    "    print(f\"   ‚úÖ API available for testing\")\n",
    "    if 'environment_info' in ext_loading:\n",
    "        env_info = ext_loading['environment_info']\n",
    "        print(f\"   ‚úÖ Detected environment: {env_info['compute_type']}\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Extension not available or failed to load\")\n",
    "    if ext_loading and 'error' in ext_loading:\n",
    "        print(f\"   Error: {ext_loading['error']}\")\n",
    "\n",
    "# API methods testing\n",
    "api_methods = databricks_test_results['api_methods']\n",
    "print(f\"\\nüß™ API METHODS TESTING:\")\n",
    "if api_methods.get('skipped'):\n",
    "    print(f\"   ‚è≠Ô∏è API testing skipped\")\n",
    "else:\n",
    "    successful_api = sum(1 for k, v in api_methods.items() if v is True)\n",
    "    total_api = len([k for k, v in api_methods.items() if isinstance(v, bool)])\n",
    "    print(f\"   üìä API Methods Tested: {successful_api}/{total_api} successful\")\n",
    "    \n",
    "    for method, result in api_methods.items():\n",
    "        if isinstance(result, bool):\n",
    "            status = \"‚úÖ\" if result else \"‚ùå\"\n",
    "            print(f\"   {status} {method}()\")\n",
    "\n",
    "# Sample data conversion testing\n",
    "sample_tests = databricks_test_results['sample_data_tests']\n",
    "print(f\"\\nüìä SAMPLE DATA CONVERSION TESTING:\")\n",
    "if sample_tests:\n",
    "    files_tested = sample_tests.get('files_tested', 0)\n",
    "    conversion_results = sample_tests.get('conversion_results', [])\n",
    "    \n",
    "    print(f\"   üìÅ Files tested: {files_tested}\")\n",
    "    \n",
    "    if conversion_results:\n",
    "        successful_conversions = len([r for r in conversion_results if r['status'] == 'SUCCESS'])\n",
    "        total_conversions = len(conversion_results)\n",
    "        success_rate = (successful_conversions / total_conversions) * 100 if total_conversions > 0 else 0\n",
    "        \n",
    "        print(f\"   ‚úÖ Successful conversions: {successful_conversions}/{total_conversions} ({success_rate:.1f}%)\")\n",
    "        \n",
    "        # Performance summary\n",
    "        successful = [r for r in conversion_results if r['status'] == 'SUCCESS' and 'duration' in r]\n",
    "        if successful:\n",
    "            avg_duration = sum(r['duration'] for r in successful) / len(successful)\n",
    "            print(f\"   ‚ö° Average conversion time: {avg_duration:.2f}s\")\n",
    "            \n",
    "            # Group by method\n",
    "            by_method = {}\n",
    "            for result in successful:\n",
    "                method = result['method']\n",
    "                if method not in by_method:\n",
    "                    by_method[method] = []\n",
    "                by_method[method].append(result)\n",
    "            \n",
    "            for method, results in by_method.items():\n",
    "                avg_time = sum(r['duration'] for r in results) / len(results)\n",
    "                print(f\"      {method}: {len(results)} conversions, {avg_time:.2f}s avg\")\n",
    "\n",
    "# Large file testing\n",
    "large_tests = databricks_test_results['large_file_tests']\n",
    "print(f\"\\nüìà LARGE FILE TESTING:\")\n",
    "if large_tests.get('skipped'):\n",
    "    print(f\"   ‚è≠Ô∏è Large file testing skipped\")\n",
    "elif large_tests:\n",
    "    successful_large = [r for r in large_tests if r.get('status') == 'SUCCESS']\n",
    "    print(f\"   üìä Large file tests: {len(successful_large)}/{len(large_tests)} successful\")\n",
    "    \n",
    "    for result in successful_large:\n",
    "        print(f\"   ‚úÖ {result['method']}: {result['throughput']:,.0f} rows/sec ({result['engine']})\")\n",
    "\n",
    "# Overall assessment\n",
    "print(f\"\\nüéØ OVERALL ASSESSMENT:\")\n",
    "\n",
    "# Count successful tests\n",
    "scores = []\n",
    "\n",
    "# Environment detection\n",
    "if env_detection:\n",
    "    scores.append(1)\n",
    "\n",
    "# Extension loading\n",
    "if ext_loading and ext_loading.get('success'):\n",
    "    scores.append(1)\n",
    "else:\n",
    "    scores.append(0)\n",
    "\n",
    "# API methods (if tested)\n",
    "if not api_methods.get('skipped'):\n",
    "    api_score = successful_api / total_api if total_api > 0 else 0\n",
    "    scores.append(api_score)\n",
    "\n",
    "# Sample conversions\n",
    "if conversion_results:\n",
    "    conv_score = successful_conversions / total_conversions if total_conversions > 0 else 0\n",
    "    scores.append(conv_score)\n",
    "\n",
    "# Overall score\n",
    "overall_score = sum(scores) / len(scores) if scores else 0\n",
    "overall_percentage = overall_score * 100\n",
    "\n",
    "if overall_percentage >= 90:\n",
    "    assessment = \"üéâ EXCELLENT - Ready for production!\"\n",
    "elif overall_percentage >= 75:\n",
    "    assessment = \"‚úÖ GOOD - Minor issues to address\"\n",
    "elif overall_percentage >= 50:\n",
    "    assessment = \"‚ö†Ô∏è FAIR - Several issues need attention\"\n",
    "else:\n",
    "    assessment = \"‚ùå POOR - Major issues require fixing\"\n",
    "\n",
    "print(f\"   üìä Overall Score: {overall_percentage:.1f}% - {assessment}\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "if overall_percentage >= 90:\n",
    "    print(f\"   üöÄ PyForge CLI Databricks extension is working excellently!\")\n",
    "    print(f\"   ‚úÖ Ready for production deployment in Databricks environment\")\n",
    "    if env_detection and env_detection['pyspark_available']:\n",
    "        print(f\"   üî• PySpark integration provides excellent performance\")\n",
    "elif overall_percentage >= 75:\n",
    "    print(f\"   üîß Minor optimizations recommended before production\")\n",
    "    print(f\"   üìù Review failed tests and address specific issues\")\n",
    "else:\n",
    "    print(f\"   üõ†Ô∏è Significant work needed before production deployment\")\n",
    "    print(f\"   üîç Debug extension loading and API method failures\")\n",
    "\n",
    "print(f\"\\nüìÅ OUTPUT LOCATIONS:\")\n",
    "print(f\"   Sample Datasets: {SAMPLE_DATASETS_PATH}\")\n",
    "print(f\"   Core CLI Output: {CONVERTED_OUTPUT_PATH}\")\n",
    "print(f\"   Extension Output: {DATABRICKS_TEST_OUTPUT}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üèÅ DATABRICKS EXTENSION FUNCTIONAL TESTING COMPLETED!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n",
   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.8.0"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}