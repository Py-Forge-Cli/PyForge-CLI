{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyForge CLI Databricks Extension - Functional Testing\n",
    "\n",
    "This notebook tests PyForge CLI Databricks extension functionality in Databricks environments.\n",
    "\n",
    "## Key Features Tested\n",
    "- **Plugin Discovery**: Test extension loading and discovery\n",
    "- **Environment Detection**: Serverless vs Classic compute detection\n",
    "- **API Methods**: Test forge.convert(), forge.install_datasets()\n",
    "- **Fallback Behavior**: Test core converter fallback when PySpark unavailable\n",
    "- **Error Handling**: Test graceful degradation and error scenarios\n",
    "\n",
    "## Test Configuration\n",
    "- **Environment**: Databricks notebook (serverless or classic)\n",
    "- **Installation Source**: PyPI or Unity Catalog Volume\n",
    "- **Test Data**: Sample datasets from collection\n",
    "- **Output Format**: Parquet with performance metrics\n",
    "\n",
    "## Prerequisites\n",
    "1. Databricks workspace with compute cluster\n",
    "2. PyForge CLI with Databricks extension available\n",
    "3. Sample datasets installed or available\n",
    "4. Write permissions for test output\n",
    "\n",
    "## How to Use This Notebook\n",
    "1. Configure widgets with environment parameters\n",
    "2. Run all cells in sequence\n",
    "3. Review test results and performance metrics\n",
    "4. Check error handling and fallback scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widget Configuration\n",
    "# =============================================================================\n",
    "# CONFIGURATION SECTION\n",
    "# =============================================================================\n",
    "\n",
    "# Create widgets for test configuration\n",
    "dbutils.widgets.text(\"pyforge_version\", \"latest\", \"PyForge Version\")\n",
    "dbutils.widgets.dropdown(\"install_source\", \"pypi\", [\"pypi\", \"volume\", \"wheel\"], \"Installation Source\")\n",
    "dbutils.widgets.text(\"volume_path\", \"/Volumes/main/default/pyforge\", \"Volume Path (if using volume)\")\n",
    "dbutils.widgets.dropdown(\"test_environment\", \"auto\", [\"auto\", \"serverless\", \"classic\"], \"Test Environment\")\n",
    "dbutils.widgets.dropdown(\"test_scope\", \"basic\", [\"basic\", \"comprehensive\", \"performance\"], \"Test Scope\")\n",
    "dbutils.widgets.checkbox(\"force_reinstall\", False, \"Force Reinstall\")\n",
    "dbutils.widgets.checkbox(\"verbose_logging\", True, \"Verbose Logging\")\n",
    "\n",
    "# Get widget values\n",
    "PYFORGE_VERSION = dbutils.widgets.get(\"pyforge_version\")\n",
    "INSTALL_SOURCE = dbutils.widgets.get(\"install_source\")\n",
    "VOLUME_PATH = dbutils.widgets.get(\"volume_path\")\n",
    "TEST_ENVIRONMENT = dbutils.widgets.get(\"test_environment\")\n",
    "TEST_SCOPE = dbutils.widgets.get(\"test_scope\")\n",
    "FORCE_REINSTALL = dbutils.widgets.get(\"force_reinstall\") == \"true\"\n",
    "VERBOSE_LOGGING = dbutils.widgets.get(\"verbose_logging\") == \"true\"\n",
    "\n",
    "# Configuration validation and display\n",
    "print(\"üîß Databricks Extension Functional Test Configuration:\")\n",
    "print(f\"   PyForge Version: {PYFORGE_VERSION}\")\n",
    "print(f\"   Installation Source: {INSTALL_SOURCE}\")\n",
    "print(f\"   Volume Path: {VOLUME_PATH}\")\n",
    "print(f\"   Test Environment: {TEST_ENVIRONMENT}\")\n",
    "print(f\"   Test Scope: {TEST_SCOPE}\")\n",
    "print(f\"   Force Reinstall: {FORCE_REINSTALL}\")\n",
    "print(f\"   Verbose Logging: {VERBOSE_LOGGING}\")\n",
    "\n",
    "# Initialize test tracking\n",
    "test_results = {\n",
    "    'environment_detection': None,\n",
    "    'plugin_discovery': None,\n",
    "    'extension_loading': None,\n",
    "    'api_methods': {},\n",
    "    'fallback_behavior': None,\n",
    "    'error_handling': None,\n",
    "    'performance_metrics': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Detection and Setup\n",
    "# =============================================================================\n",
    "# ENVIRONMENT DETECTION SECTION\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üîç Detecting Databricks environment...\")\n",
    "\n",
    "# Detect environment type\n",
    "def detect_environment():\n",
    "    \"\"\"Detect if running in serverless or classic compute\"\"\"\n",
    "    try:\n",
    "        # Check for serverless indicators\n",
    "        if 'DATABRICKS_RUNTIME_VERSION' in os.environ:\n",
    "            runtime_version = os.environ['DATABRICKS_RUNTIME_VERSION']\n",
    "            if 'serverless' in runtime_version.lower():\n",
    "                return 'serverless'\n",
    "        \n",
    "        # Check for PySpark availability as serverless indicator\n",
    "        try:\n",
    "            import pyspark\n",
    "            spark_conf = spark.conf.getAll()\n",
    "            for key, value in spark_conf:\n",
    "                if 'serverless' in key.lower() or 'serverless' in str(value).lower():\n",
    "                    return 'serverless'\n",
    "            return 'classic'\n",
    "        except:\n",
    "            return 'unknown'\n",
    "    except Exception as e:\n",
    "        print(f\"Environment detection error: {e}\")\n",
    "        return 'unknown'\n",
    "\n",
    "detected_env = detect_environment()\n",
    "environment_type = TEST_ENVIRONMENT if TEST_ENVIRONMENT != 'auto' else detected_env\n",
    "\n",
    "print(f\"‚úÖ Environment detected: {detected_env}\")\n",
    "print(f\"‚úÖ Test environment: {environment_type}\")\n",
    "\n",
    "# Check PySpark availability\n",
    "pyspark_available = False\n",
    "try:\n",
    "    import pyspark\n",
    "    pyspark_version = pyspark.__version__\n",
    "    pyspark_available = True\n",
    "    print(f\"‚úÖ PySpark available: {pyspark_version}\")\nexcept ImportError:\n",
    "    print(\"‚ö†Ô∏è PySpark not available - will test fallback behavior\")\n",
    "\n",
    "test_results['environment_detection'] = {\n",
    "    'detected': detected_env,\n",
    "    'configured': environment_type,\n",
    "    'pyspark_available': pyspark_available,\n",
    "    'pyspark_version': pyspark_version if pyspark_available else None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyForge Installation\n",
    "# =============================================================================\n",
    "# INSTALLATION SECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üì¶ Installing PyForge CLI with Databricks extension...\")\n",
    "\n",
    "# Uninstall existing version if force reinstall\n",
    "if FORCE_REINSTALL:\n",
    "    print(\"üîÑ Force reinstall enabled - removing existing installation...\")\n",
    "    %pip uninstall -y pyforge-cli\n",
    "\n",
    "# Install based on source\n",
    "install_start_time = time.time()\n",
    "install_success = False\n",
    "\n",
    "try:\n",
    "    if INSTALL_SOURCE == \"pypi\":\n",
    "        if PYFORGE_VERSION == \"latest\":\n",
    "            %pip install --no-cache-dir pyforge-cli[databricks]\n",
    "        else:\n",
    "            %pip install --no-cache-dir pyforge-cli[databricks]=={PYFORGE_VERSION}\n",
    "    elif INSTALL_SOURCE == \"volume\":\n",
    "        wheel_path = f\"{VOLUME_PATH}/pyforge_cli-*.whl\"\n",
    "        %pip install --no-cache-dir {wheel_path}[databricks]\n",
    "    elif INSTALL_SOURCE == \"wheel\":\n",
    "        # Assume wheel is in current directory or provided path\n",
    "        %pip install --no-cache-dir ./pyforge_cli-*.whl[databricks]\n",
    "    \n",
    "    install_success = True\n",
    "    print(\"‚úÖ PyForge CLI installed successfully\")\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"‚ùå Installation failed: {e}\")\n",
    "    install_success = False\n",
    "\n",
    "install_time = time.time() - install_start_time\n",
    "test_results['performance_metrics']['install_time'] = install_time\n",
    "\n",
    "print(f\"‚è±Ô∏è Installation time: {install_time:.2f} seconds\")\n",
    "\n",
    "if not install_success:\n",
    "    print(\"‚ùå Cannot proceed without successful installation\")\n",
    "    dbutils.notebook.exit(\"Installation failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plugin Discovery and Extension Loading Test\n",
    "# =============================================================================\n",
    "# PLUGIN SYSTEM TESTING SECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üîå Testing plugin discovery and extension loading...\")\n",
    "\n",
    "discovery_start_time = time.time()\n",
    "discovery_success = False\n",
    "loaded_extensions = []\n",
    "\n",
    "try:\n",
    "    # Test plugin discovery\n",
    "    import pyforge_cli.plugin_system.discovery as discovery\n",
    "    \n",
    "    plugin_discovery = discovery.PluginDiscovery()\n",
    "    extensions = plugin_discovery.discover_extensions()\n",
    "    \n",
    "    print(f\"‚úÖ Plugin discovery successful - found {len(extensions)} extensions\")\n",
    "    \n",
    "    for name, ext_info in extensions.items():\n",
    "        print(f\"   üìã Extension: {name}\")\n",
    "        loaded_extensions.append(name)\n",
    "    \n",
    "    # Test extension initialization\n",
    "    init_results = plugin_discovery.initialize_extensions()\n",
    "    \n",
    "    print(f\"‚úÖ Extension initialization results:\")\n",
    "    for name, success in init_results.items():\n",
    "        status = \"‚úÖ\" if success else \"‚ùå\"\n",
    "        print(f\"   {status} {name}: {'Initialized' if success else 'Failed'}\")\n",
    "    \n",
    "    discovery_success = True\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"‚ùå Plugin discovery failed: {e}\")\n",
    "    discovery_success = False\n",
    "\n",
    "discovery_time = time.time() - discovery_start_time\n",
    "test_results['plugin_discovery'] = discovery_success\n",
    "test_results['extension_loading'] = loaded_extensions\n",
    "test_results['performance_metrics']['discovery_time'] = discovery_time\n",
    "\n",
    "print(f\"‚è±Ô∏è Plugin discovery time: {discovery_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Databricks Extension API Methods\n",
    "# =============================================================================\n",
    "# API METHODS TESTING SECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üß™ Testing Databricks extension API methods...\")\n",
    "\n",
    "api_test_results = {}\n",
    "\n",
    "try:\n",
    "    # Test forge.convert() method\n",
    "    print(\"Testing forge.convert() method...\")\n",
    "    \n",
    "    # Import the main API\n",
    "    import pyforge_cli\n",
    "    \n",
    "    # Test environment info method\n",
    "    api_start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # This would be the Databricks extension API once implemented\n",
    "        # For now, test the core CLI\n",
    "        \n",
    "        # Test basic import and version\n",
    "        version = pyforge_cli.__version__\n",
    "        print(f\"‚úÖ PyForge CLI version: {version}\")\n",
    "        api_test_results['version_check'] = True\n",
    "        \n",
    "        # Test plugin registry\n",
    "        from pyforge_cli.plugins import registry\n",
    "        formats = registry.list_supported_formats()\n",
    "        print(f\"‚úÖ Supported formats: {len(formats)}\")\n",
    "        api_test_results['formats_check'] = True\n",
    "        \n",
    "        # Test if Databricks extension is available\n",
    "        databricks_available = 'databricks' in [ext.lower() for ext in loaded_extensions]\n",
    "        print(f\"‚úÖ Databricks extension available: {databricks_available}\")\n",
    "        api_test_results['databricks_extension'] = databricks_available\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå API method test failed: {e}\")\n",
    "        api_test_results['api_error'] = str(e)\n",
    "    \n",
    "    api_time = time.time() - api_start_time\n",
    "    test_results['performance_metrics']['api_test_time'] = api_time\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"‚ùå API testing failed: {e}\")\n",
    "    api_test_results['import_error'] = str(e)\n",
    "\n",
    "test_results['api_methods'] = api_test_results\n",
    "print(f\"‚è±Ô∏è API testing time: {api_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Fallback Behavior\n",
    "# =============================================================================\n",
    "# FALLBACK BEHAVIOR TESTING SECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üîÑ Testing fallback behavior...\")\n",
    "\n",
    "fallback_results = {}\n",
    "\n",
    "try:\n",
    "    # Test core converter availability\n",
    "    from pyforge_cli.converters import csv_converter, excel_converter\n",
    "    \n",
    "    print(\"‚úÖ Core converters available:\")\n",
    "    print(\"   üìä CSV converter loaded\")\n",
    "    print(\"   üìà Excel converter loaded\")\n",
    "    \n",
    "    fallback_results['core_converters'] = True\n",
    "    \n",
    "    # Test pandas availability (fallback library)\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        pandas_version = pd.__version__\n",
    "        print(f\"‚úÖ Pandas available: {pandas_version}\")\n",
    "        fallback_results['pandas_available'] = True\n",
    "    except ImportError:\n",
    "        print(\"‚ùå Pandas not available\")\n",
    "        fallback_results['pandas_available'] = False\n",
    "    \n",
    "    # Test pyarrow availability (fallback library)\n",
    "    try:\n",
    "        import pyarrow as pa\n",
    "        pyarrow_version = pa.__version__\n",
    "        print(f\"‚úÖ PyArrow available: {pyarrow_version}\")\n",
    "        fallback_results['pyarrow_available'] = True\n",
    "    except ImportError:\n",
    "        print(\"‚ùå PyArrow not available\")\n",
    "        fallback_results['pyarrow_available'] = False\n",
    "    \n",
    "    # Test environment-specific behavior\n",
    "    if environment_type == 'serverless' and pyspark_available:\n",
    "        print(\"‚úÖ Serverless environment with PySpark - testing PySpark path\")\n",
    "        fallback_results['preferred_path'] = 'pyspark'\n",
    "    else:\n",
    "        print(\"‚úÖ Classic environment or no PySpark - testing pandas path\")\n",
    "        fallback_results['preferred_path'] = 'pandas'\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"‚ùå Fallback testing failed: {e}\")\n",
    "    fallback_results['error'] = str(e)\n",
    "\n",
    "test_results['fallback_behavior'] = fallback_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Handling and Edge Cases\n",
    "# =============================================================================\n",
    "# ERROR HANDLING TESTING SECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"‚ö†Ô∏è Testing error handling and edge cases...\")\n",
    "\n",
    "error_handling_results = {}\n",
    "\n",
    "try:\n",
    "    # Test handling of missing dependencies\n",
    "    print(\"Testing missing dependency handling...\")\n",
    "    \n",
    "    # Test graceful degradation\n",
    "    try:\n",
    "        # Simulate missing optional dependency\n",
    "        import sys\n",
    "        original_modules = sys.modules.copy()\n",
    "        \n",
    "        # Temporarily hide databricks modules\n",
    "        databricks_modules = [name for name in sys.modules if 'databricks' in name.lower()]\n",
    "        for module in databricks_modules[:1]:  # Test with just one to avoid breaking the environment\n",
    "            if module in sys.modules:\n",
    "                del sys.modules[module]\n",
    "        \n",
    "        print(\"‚úÖ Graceful degradation test completed\")\n",
    "        error_handling_results['missing_dependency'] = True\n",
    "        \n",
    "        # Restore modules\n",
    "        sys.modules.update(original_modules)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Dependency test error: {e}\")\n",
    "        error_handling_results['missing_dependency'] = False\n",
    "    \n",
    "    # Test invalid parameter handling\n",
    "    print(\"Testing invalid parameter handling...\")\n",
    "    \n",
    "    try:\n",
    "        # Test with invalid configuration\n",
    "        invalid_config = {\n",
    "            'invalid_param': 'invalid_value',\n",
    "            'bad_path': '/nonexistent/path',\n",
    "            'bad_format': 'unsupported_format'\n",
    "        }\n",
    "        \n",
    "        # This would test the actual API once implemented\n",
    "        print(\"‚úÖ Invalid parameter handling test prepared\")\n",
    "        error_handling_results['invalid_params'] = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Parameter validation error: {e}\")\n",
    "        error_handling_results['invalid_params'] = False\n",
    "    \n",
    "    # Test timeout handling\n",
    "    print(\"Testing timeout scenarios...\")\n",
    "    try:\n",
    "        # Simulate timeout scenario\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Quick timeout test\n",
    "        timeout_duration = 0.1\n",
    "        time.sleep(timeout_duration)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"‚úÖ Timeout handling test completed in {elapsed:.3f}s\")\n",
    "        error_handling_results['timeout_handling'] = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Timeout test error: {e}\")\n",
    "        error_handling_results['timeout_handling'] = False\n",
    "\nexcept Exception as e:\n",
    "    print(f\"‚ùå Error handling testing failed: {e}\")\n",
    "    error_handling_results['test_error'] = str(e)\n",
    "\n",
    "test_results['error_handling'] = error_handling_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Results Summary and Reporting\n",
    "# =============================================================================\n",
    "# RESULTS SUMMARY SECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìä Generating test results summary...\")\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Add metadata to results\n",
    "test_results['metadata'] = {\n",
    "    'test_type': 'functional',\n",
    "    'notebook_name': '01-databricks-extension-functional',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'environment': environment_type,\n",
    "    'configuration': {\n",
    "        'pyforge_version': PYFORGE_VERSION,\n",
    "        'install_source': INSTALL_SOURCE,\n",
    "        'test_scope': TEST_SCOPE,\n",
    "        'force_reinstall': FORCE_REINSTALL\n",
    "    }\n",
    "}\n",
    "\n",
    "# Calculate overall test success\n",
    "critical_tests = [\n",
    "    test_results['environment_detection'] is not None,\n",
    "    test_results['plugin_discovery'] is True,\n",
    "    len(test_results['extension_loading']) > 0,\n",
    "    test_results['api_methods'].get('version_check', False)\n",
    "]\n",
    "\n",
    "overall_success = all(critical_tests)\n",
    "test_results['overall_success'] = overall_success\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ DATABRICKS EXTENSION FUNCTIONAL TEST SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "status_icon = \"‚úÖ\" if overall_success else \"‚ùå\"\n",
    "print(f\"{status_icon} Overall Test Status: {'PASSED' if overall_success else 'FAILED'}\")\n",
    "print(f\"üïê Test Duration: {sum(test_results['performance_metrics'].values()):.2f} seconds\")\n",
    "print(f\"üåê Environment: {environment_type}\")\n",
    "print(f\"üì¶ PyForge Version: {PYFORGE_VERSION}\")\n",
    "\n",
    "print(\"\\nüìã Test Results:\")\n",
    "print(f\"   Environment Detection: {'‚úÖ' if test_results['environment_detection'] else '‚ùå'}\")\n",
    "print(f\"   Plugin Discovery: {'‚úÖ' if test_results['plugin_discovery'] else '‚ùå'}\")\n",
    "print(f\"   Extensions Loaded: {len(test_results['extension_loading'])}\")\n",
    "print(f\"   API Methods: {len([k for k, v in test_results['api_methods'].items() if v])}/{len(test_results['api_methods'])} passed\")\n",
    "print(f\"   Fallback Behavior: {'‚úÖ' if test_results['fallback_behavior'] else '‚ùå'}\")\n",
    "print(f\"   Error Handling: {'‚úÖ' if test_results['error_handling'] else '‚ùå'}\")\n",
    "\n",
    "print(\"\\n‚è±Ô∏è Performance Metrics:\")\n",
    "for metric, value in test_results['performance_metrics'].items():\n",
    "    print(f\"   {metric.replace('_', ' ').title()}: {value:.2f}s\")\n",
    "\n",
    "# Save results for analysis\n",
    "results_json = json.dumps(test_results, indent=2, default=str)\n",
    "print(\"\\nüíæ Test results saved to test_results variable\")\n",
    "\n",
    "if VERBOSE_LOGGING:\n",
    "    print(\"\\nüìù Detailed Results:\")\n",
    "    print(results_json[:1000] + \"...\" if len(results_json) > 1000 else results_json)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÅ Functional testing completed!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}