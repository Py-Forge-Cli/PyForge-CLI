{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# PyForge CLI End-to-End Testing - Databricks Serverless\n\nThis notebook tests PyForge CLI functionality in Databricks Serverless environment using the deployed wheel from Unity Catalog volumes.\n\n## Databricks Widgets\nThis notebook uses Databricks widgets for easy parameter configuration. The widgets will appear at the top of the notebook after running the first cell:\n\n- **sample_datasets_base_path**: Base path for sample datasets installation\n  - Default: `/Volumes/cortex_dev_catalog/0000_santosh/volume_sandbox/sample-datasets/`\n  - Type: Text input widget\n  \n- **pyforge_version**: PyForge CLI version to test\n  - Default: `1.0.8.dev2`\n  - Type: Text input widget\n  \n- **databricks_username**: Your Databricks username\n  - Default: `usa-sdandey@deloitte.com`\n  - Type: Text input widget\n  \n- **force_conversion**: Whether to force overwrite existing conversions\n  - Default: `True`\n  - Type: Dropdown (True/False)\n  \n- **use_pyspark_for_csv**: Enable PySpark converter for CSV files\n  - Default: `True`\n  - Type: Dropdown (True/False)\n  \n- **test_smallest_files_only**: Test only the smallest file of each type\n  - Default: `True`\n  - Type: Dropdown (True/False)\n\n## Test Configuration\n- **Environment**: Databricks Serverless Compute\n- **Installation Source**: Unity Catalog Volume (deployed wheel)\n- **Sample Data**: Real sample datasets from v1.0.5 release\n- **Output Format**: Parquet (optimized for Databricks)\n\n## Prerequisites\n1. PyForge CLI wheel deployed to volume via `scripts/deploy_pyforge_to_databricks.py`\n2. Unity Catalog access permissions to the specified volume path\n3. Workspace access to CoreDataEngineers folder\n\n## ‚ö†Ô∏è Important: PyPI Index URL Configuration\n**All `%pip install` commands in this notebook include the proper PyPI index URL for dependency resolution in corporate environments:**\n\n```python\n%pip install package --no-cache-dir --quiet --index-url https://pypi.org/simple/ --trusted-host pypi.org\n```\n\n**Required flags:**\n- `--no-cache-dir`: Ensures fresh installation without cached packages\n- `--quiet`: Reduces installation output verbosity  \n- `--index-url https://pypi.org/simple/`: Specifies PyPI index for dependency resolution\n- `--trusted-host pypi.org`: Trusts PyPI host for secure downloads\n\nThis configuration is memorized in `CLAUDE.md` for all future Databricks Serverless notebooks.\n\n## How to Use This Notebook\n1. Run the first cell to initialize the widgets\n2. Modify widget values as needed (they appear at the top of the notebook)\n3. Run all remaining cells in sequence\n4. Review the test results and summary report\n\n## Widget Benefits\n- **No Code Changes**: Modify parameters without editing code cells\n- **Persistence**: Widget values persist across cell executions\n- **Job Parameters**: Widgets can be passed as parameters when running as Databricks Jobs\n- **User-Friendly**: Interactive UI elements for configuration\n\n## Key Features of This Notebook\n1. **Improved File Discovery**: Displays all downloaded files with sizes using `dbutils.fs.ls`\n2. **Smart File Selection**: Option to test only smallest files or all files\n3. **Detailed Observations**: Logs detailed test observations for each conversion\n4. **No --verbose Flag**: Fixed the command to remove unsupported --verbose flag\n5. **Better Error Handling**: Enhanced error messages and timeout management",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# DBTITLE 1,Configuration Parameters from Widgets\n# =============================================================================\n# CONFIGURATION SECTION - Using Widget Values\n# =============================================================================\n\n# Get widget values\nSAMPLE_DATASETS_BASE_PATH = dbutils.widgets.get(\"sample_datasets_base_path\")\nPYFORGE_VERSION = dbutils.widgets.get(\"pyforge_version\")\nDATABRICKS_USERNAME = dbutils.widgets.get(\"databricks_username\")\nFORCE_CONVERSION = dbutils.widgets.get(\"force_conversion\").lower() == \"true\"\nUSE_PYSPARK_FOR_CSV = dbutils.widgets.get(\"use_pyspark_for_csv\").lower() == \"true\"\nTEST_SMALLEST_FILES_ONLY = dbutils.widgets.get(\"test_smallest_files_only\").lower() == \"true\"\n\n# Derived paths\nPYFORGE_WHEEL_PATH = f\"/Volumes/cortex_dev_catalog/sandbox_testing/pkgs/{DATABRICKS_USERNAME}/pyforge_cli-{PYFORGE_VERSION}-py3-none-any.whl\"\nSAMPLE_DATASETS_PATH = SAMPLE_DATASETS_BASE_PATH.rstrip('/')  # Remove trailing slash for consistency\nCONVERTED_OUTPUT_PATH = SAMPLE_DATASETS_PATH.replace('/sample-datasets', '/converted_output')\n\nprint(f\"üîß Configuration (from widgets):\")\nprint(f\"   PyForge Version: {PYFORGE_VERSION}\")\nprint(f\"   Databricks Username: {DATABRICKS_USERNAME}\")\nprint(f\"   PyForge Wheel Path: {PYFORGE_WHEEL_PATH}\")\nprint(f\"   Sample Datasets Base Path: {SAMPLE_DATASETS_BASE_PATH}\")\nprint(f\"   Sample Datasets Path: {SAMPLE_DATASETS_PATH}\")\nprint(f\"   Output Path: {CONVERTED_OUTPUT_PATH}\")\nprint(f\"   Force Conversion: {FORCE_CONVERSION}\")\nprint(f\"   Use PySpark for CSV: {USE_PYSPARK_FOR_CSV}\")\nprint(f\"   Test Smallest Files Only: {TEST_SMALLEST_FILES_ONLY}\")\n\n# Validate paths\nif not SAMPLE_DATASETS_BASE_PATH.startswith(\"/Volumes/\"):\n    print(\"‚ö†Ô∏è  Warning: Sample datasets path should start with /Volumes/ for Unity Catalog volumes\")\n\nprint(\"\\nüìù Tip: You can change these values using the widgets at the top of the notebook!\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# DBTITLE 1,Initialize Notebook Widgets\n# =============================================================================\n# DATABRICKS WIDGETS INITIALIZATION\n# =============================================================================\n\n# Remove any existing widgets to ensure clean state\ndbutils.widgets.removeAll()\n\n# Create widgets for notebook parameters\ndbutils.widgets.text(\n    \"sample_datasets_base_path\", \n    \"/Volumes/cortex_dev_catalog/0000_santosh/volume_sandbox/sample-datasets/\",\n    \"Sample Datasets Base Path\"\n)\n\ndbutils.widgets.text(\n    \"pyforge_version\",\n    \"1.0.8.dev2\",\n    \"PyForge Version\"\n)\n\ndbutils.widgets.text(\n    \"databricks_username\",\n    \"usa-sdandey@deloitte.com\",\n    \"Databricks Username\"\n)\n\ndbutils.widgets.dropdown(\n    \"force_conversion\",\n    \"True\",\n    [\"True\", \"False\"],\n    \"Force Conversion\"\n)\n\ndbutils.widgets.dropdown(\n    \"use_pyspark_for_csv\",\n    \"True\", \n    [\"True\", \"False\"],\n    \"Use PySpark for CSV\"\n)\n\ndbutils.widgets.dropdown(\n    \"test_smallest_files_only\",\n    \"True\",\n    [\"True\", \"False\"],\n    \"Test Smallest Files Only\"\n)\n\n# Display widget values\nprint(\"üìã Widget Parameters Initialized:\")\nprint(f\"   Sample Datasets Base Path: {dbutils.widgets.get('sample_datasets_base_path')}\")\nprint(f\"   PyForge Version: {dbutils.widgets.get('pyforge_version')}\")\nprint(f\"   Databricks Username: {dbutils.widgets.get('databricks_username')}\")\nprint(f\"   Force Conversion: {dbutils.widgets.get('force_conversion')}\")\nprint(f\"   Use PySpark for CSV: {dbutils.widgets.get('use_pyspark_for_csv')}\")\nprint(f\"   Test Smallest Files Only: {dbutils.widgets.get('test_smallest_files_only')}\")\n\nprint(\"\\n‚úÖ Widgets created successfully! You can modify the parameters using the widgets above.\")\nprint(\"üìù Note: Widget values will persist across cell executions until changed.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# MAGIC %md\n# MAGIC ### Using this Notebook in Databricks Jobs\n# MAGIC \n# MAGIC When running this notebook as a Databricks Job, you can pass widget values as job parameters:\n# MAGIC \n# MAGIC ```json\n# MAGIC {\n# MAGIC   \"notebook_task\": {\n# MAGIC     \"notebook_path\": \"/path/to/02-test-cli-end-to-end-serverless\",\n# MAGIC     \"base_parameters\": {\n# MAGIC       \"sample_datasets_base_path\": \"/Volumes/your_catalog/your_schema/sample-datasets/\",\n# MAGIC       \"pyforge_version\": \"1.0.8\",\n# MAGIC       \"databricks_username\": \"your-username@company.com\",\n# MAGIC       \"force_conversion\": \"True\",\n# MAGIC       \"use_pyspark_for_csv\": \"True\",\n# MAGIC       \"test_smallest_files_only\": \"True\"\n# MAGIC     }\n# MAGIC   }\n# MAGIC }\n# MAGIC ```\n# MAGIC \n# MAGIC The widgets will automatically use the job parameter values instead of the defaults.",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# DBTITLE 1,Validate Widget Parameters\n# =============================================================================\n# WIDGET PARAMETER VALIDATION\n# =============================================================================\n\n# Validate widget parameters before proceeding\nvalidation_errors = []\n\n# Check sample datasets path\nif not SAMPLE_DATASETS_BASE_PATH:\n    validation_errors.append(\"‚ùå Sample datasets base path cannot be empty\")\nelif not SAMPLE_DATASETS_BASE_PATH.startswith(\"/Volumes/\"):\n    validation_errors.append(\"‚ö†Ô∏è  Sample datasets path should start with /Volumes/ for Unity Catalog volumes\")\n\n# Check PyForge version format\nif not PYFORGE_VERSION:\n    validation_errors.append(\"‚ùå PyForge version cannot be empty\")\nelif not any(char.isdigit() for char in PYFORGE_VERSION):\n    validation_errors.append(\"‚ùå PyForge version should contain version numbers\")\n\n# Check username\nif not DATABRICKS_USERNAME:\n    validation_errors.append(\"‚ùå Databricks username cannot be empty\")\nelif \"@\" not in DATABRICKS_USERNAME and \"-\" not in DATABRICKS_USERNAME:\n    validation_errors.append(\"‚ö†Ô∏è  Username format may be incorrect (expected email or ID format)\")\n\n# Display validation results\nif validation_errors:\n    print(\"‚ö†Ô∏è  PARAMETER VALIDATION WARNINGS:\")\n    for error in validation_errors:\n        print(f\"   {error}\")\n    print(\"\\nüìù Please review the widget parameters above and update if needed.\")\n    \n    # For critical errors, stop execution\n    critical_errors = [e for e in validation_errors if e.startswith(\"‚ùå\")]\n    if critical_errors:\n        raise ValueError(f\"Critical validation errors found: {critical_errors}\")\nelse:\n    print(\"‚úÖ All widget parameters validated successfully!\")\n    \n# Additional checks for wheel path existence will be done in the next cell\nprint(f\"\\nüì¶ Expected wheel path: {PYFORGE_WHEEL_PATH}\")\nprint(\"   (Will verify existence in the next cell)\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBTITLE 1,Environment Check\n",
    "# =============================================================================\n",
    "# ENVIRONMENT VERIFICATION\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üîç Verifying Databricks Serverless environment...\")\n",
    "\n",
    "# Check if we're in Databricks environment\n",
    "try:\n",
    "    dbutils\n",
    "    print(\"‚úÖ Running in Databricks environment\")\n",
    "    \n",
    "    # Get current user info\n",
    "    current_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "    print(f\"   Current user: {current_user}\")\n",
    "    \n",
    "    # Check if wheel file exists\n",
    "    try:\n",
    "        dbutils.fs.ls(PYFORGE_WHEEL_PATH.replace('/Volumes/', 'dbfs:/Volumes/'))\n",
    "        print(f\"‚úÖ PyForge wheel found: {PYFORGE_WHEEL_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå PyForge wheel not found: {PYFORGE_WHEEL_PATH}\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        print(\"   Please run deployment script first: scripts/deploy_pyforge_to_databricks.py\")\n",
    "        raise\n",
    "        \n",
    "except NameError:\n",
    "    print(\"‚ùå Not running in Databricks environment\")\n",
    "    print(\"   This notebook is designed for Databricks Serverless only\")\n",
    "    raise RuntimeError(\"This notebook requires Databricks environment\")\n",
    "\n",
    "print(f\"\\nüïê Test started at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# DBTITLE 1,Install PyForge CLI from Unity Catalog Volume\n# =============================================================================\n# INSTALLATION FROM DEPLOYED WHEEL WITH PYPI INDEX URL\n# =============================================================================\n\nprint(f\"üì¶ Installing PyForge CLI from deployed wheel...\")\nprint(f\"   Installing from: {PYFORGE_WHEEL_PATH}\")\nprint(f\"   Using --no-cache-dir to ensure fresh installation\")\nprint(f\"   Using corporate PyPI index URL for dependency resolution\")\n\n# Install PyForge CLI from volume wheel with no cache and proper index URL\n%pip install {PYFORGE_WHEEL_PATH} --no-cache-dir --quiet --index-url https://pypi.org/simple/ --trusted-host pypi.org\n\nprint(f\"‚úÖ PyForge CLI installed successfully from volume!\")\nprint(\"üîÑ Restarting Python environment to ensure clean import...\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Restart Python to ensure clean environment\ndbutils.library.restartPython()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# DBTITLE 1,Re-initialize Configuration After Restart\n# =============================================================================\n# VARIABLE RE-INITIALIZATION AFTER PYTHON RESTART\n# =============================================================================\n\n# Re-initialize all configuration variables from widgets since Python was restarted\n# Widgets persist across Python restarts, so we can get the values again\n\n# Get widget values\nSAMPLE_DATASETS_BASE_PATH = dbutils.widgets.get(\"sample_datasets_base_path\")\nPYFORGE_VERSION = dbutils.widgets.get(\"pyforge_version\")\nDATABRICKS_USERNAME = dbutils.widgets.get(\"databricks_username\")\nFORCE_CONVERSION = dbutils.widgets.get(\"force_conversion\").lower() == \"true\"\nUSE_PYSPARK_FOR_CSV = dbutils.widgets.get(\"use_pyspark_for_csv\").lower() == \"true\"\nTEST_SMALLEST_FILES_ONLY = dbutils.widgets.get(\"test_smallest_files_only\").lower() == \"true\"\n\n# Derived paths\nPYFORGE_WHEEL_PATH = f\"/Volumes/cortex_dev_catalog/sandbox_testing/pkgs/{DATABRICKS_USERNAME}/pyforge_cli-{PYFORGE_VERSION}-py3-none-any.whl\"\nSAMPLE_DATASETS_PATH = SAMPLE_DATASETS_BASE_PATH.rstrip('/')  # Remove trailing slash for consistency\nCONVERTED_OUTPUT_PATH = SAMPLE_DATASETS_PATH.replace('/sample-datasets', '/converted_output')\n\nprint(f\"üîÑ Re-initialized configuration variables from widgets after Python restart:\")\nprint(f\"   PyForge Version: {PYFORGE_VERSION}\")\nprint(f\"   Databricks Username: {DATABRICKS_USERNAME}\")\nprint(f\"   PyForge Wheel Path: {PYFORGE_WHEEL_PATH}\")\nprint(f\"   Sample Datasets Base Path: {SAMPLE_DATASETS_BASE_PATH}\")\nprint(f\"   Sample Datasets Path: {SAMPLE_DATASETS_PATH}\")\nprint(f\"   Output Path: {CONVERTED_OUTPUT_PATH}\")\nprint(f\"   Force Conversion: {FORCE_CONVERSION}\")\nprint(f\"   Use PySpark for CSV: {USE_PYSPARK_FOR_CSV}\")\nprint(f\"   Test Smallest Files Only: {TEST_SMALLEST_FILES_ONLY}\")\n\nprint(\"\\n‚úÖ Configuration restored from widgets successfully!\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# DBTITLE 1,Re-initialize Configuration After Restart\n# =============================================================================\n# VARIABLE RE-INITIALIZATION AFTER PYTHON RESTART\n# =============================================================================\n\n# Re-initialize all configuration variables since Python was restarted\n# NOTEBOOK PARAMETERS - These can be set when running the notebook\nSAMPLE_DATASETS_BASE_PATH = \"/Volumes/cortex_dev_catalog/0000_santosh/volume_sandbox/sample-datasets/\"\n\nPYFORGE_VERSION = \"1.0.8.dev2\"\nDATABRICKS_USERNAME = \"usa-sdandey@deloitte.com\"  # Update with your username\nPYFORGE_WHEEL_PATH = f\"/Volumes/cortex_dev_catalog/sandbox_testing/pkgs/{DATABRICKS_USERNAME}/pyforge_cli-{PYFORGE_VERSION}-py3-none-any.whl\"\n\n# Paths configuration using the base path parameter\nSAMPLE_DATASETS_PATH = SAMPLE_DATASETS_BASE_PATH.rstrip('/')  # Remove trailing slash for consistency\nCONVERTED_OUTPUT_PATH = SAMPLE_DATASETS_PATH.replace('/sample-datasets', '/converted_output')\n\nFORCE_CONVERSION = True\nUSE_PYSPARK_FOR_CSV = True\n\nprint(f\"üîÑ Re-initialized configuration variables after Python restart:\")\nprint(f\"   PyForge Version: {PYFORGE_VERSION}\")\nprint(f\"   Databricks Username: {DATABRICKS_USERNAME}\")\nprint(f\"   PyForge Wheel Path: {PYFORGE_WHEEL_PATH}\")\nprint(f\"   Sample Datasets Base Path: {SAMPLE_DATASETS_BASE_PATH}\")\nprint(f\"   Sample Datasets Path: {SAMPLE_DATASETS_PATH}\")\nprint(f\"   Output Path: {CONVERTED_OUTPUT_PATH}\")\nprint(f\"   Force Conversion: {FORCE_CONVERSION}\")\nprint(f\"   Use PySpark for CSV: {USE_PYSPARK_FOR_CSV}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "echo \"üìã PyForge CLI Help Information:\"\n",
    "pyforge --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "echo \"üìä PyForge CLI Version Information:\"\n",
    "pyforge --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBTITLE 1,Check PySpark Availability in Serverless\n",
    "# =============================================================================\n",
    "# PYSPARK AVAILABILITY CHECK FOR SERVERLESS\n",
    "# =============================================================================\n",
    "\n",
    "def check_pyspark_availability():\n",
    "    \"\"\"Check if PySpark is available in the Databricks Serverless environment.\"\"\"\n",
    "    try:\n",
    "        import pyspark\n",
    "        from pyspark.sql import SparkSession\n",
    "        print(\"‚úÖ PySpark is available in this Databricks Serverless environment\")\n",
    "        print(f\"   PySpark Version: {pyspark.__version__}\")\n",
    "        \n",
    "        # Try to get or create a Spark session\n",
    "        try:\n",
    "            spark = SparkSession.builder.getOrCreate()\n",
    "            print(f\"   Spark Session: Active\")\n",
    "            print(f\"   Spark Version: {spark.version}\")\n",
    "            \n",
    "            # Check if it's Spark Connect (serverless)\n",
    "            try:\n",
    "                master = spark.sparkContext.master\n",
    "                print(f\"   Spark Master: {master}\")\n",
    "            except Exception:\n",
    "                print(f\"   Spark Mode: Serverless (Spark Connect)\")\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Could not create Spark session: {e}\")\n",
    "            return False\n",
    "    except ImportError:\n",
    "        print(\"‚ùå PySpark is NOT available in this environment\")\n",
    "        print(\"   CSV files will be converted using pandas\")\n",
    "        return False\n",
    "\n",
    "# Check PySpark availability\n",
    "pyspark_available = check_pyspark_availability()\n",
    "\n",
    "# Update USE_PYSPARK_FOR_CSV based on availability\n",
    "if not pyspark_available and USE_PYSPARK_FOR_CSV:\n",
    "    print(\"\\n‚ö†Ô∏è  Note: PySpark not available, CSV conversion will fall back to pandas\")\n",
    "    USE_PYSPARK_FOR_CSV = False\n",
    "elif pyspark_available:\n",
    "    print(\"\\nüöÄ PySpark is available! PyForge CLI will auto-detect and use PySpark for CSV conversions\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# DBTITLE 1,Comprehensive Conversion Testing\n# =============================================================================\n# BULK CONVERSION TESTING IN DATABRICKS SERVERLESS\n# =============================================================================\n\ndef run_serverless_conversion_test(file_info):\n    \"\"\"Run conversion test for a single file in Databricks Serverless environment.\"\"\"\n    file_path = file_info['file_path']\n    file_type = file_info['file_type']\n    file_name = file_info['file_name']\n    file_ext = file_info['extension']\n    \n    # Create output path in volume\n    output_name = file_name.split('.')[0]\n    output_path = f\"{CONVERTED_OUTPUT_PATH}/{file_info['category']}/{output_name}.parquet\"\n    \n    # Build conversion command (removed --verbose flag as it's not supported)\n    force_flag = '--force' if FORCE_CONVERSION else ''\n    pyspark_flag = '--force-pyspark' if USE_PYSPARK_FOR_CSV and file_ext == '.csv' else ''\n    excel_flag = '--separate' if file_ext in ['.xlsx', '.xls'] else ''\n    \n    cmd = [\n        'pyforge', 'convert', file_path, output_path, \n        '--format', 'parquet', force_flag, pyspark_flag, excel_flag\n    ]\n    cmd = [arg for arg in cmd if arg]  # Remove empty strings\n    \n    print(f\"\\nüîÑ Converting {file_name} ({file_type})...\")\n    print(f\"   File size: {file_info.get('size_readable', 'Unknown')}\")\n    print(f\"   Command: {' '.join(cmd)}\")\n    \n    # Log test observation\n    observation = {\n        'file': file_name,\n        'type': file_type,\n        'size': file_info.get('size_readable', 'Unknown'),\n        'start_time': datetime.now().strftime('%H:%M:%S')\n    }\n    \n    try:\n        start_time = time.time()\n        \n        # Set timeout based on file size\n        file_size_mb = file_info.get('size_mb', 0)\n        if file_size_mb > 100:\n            timeout = 600  # 10 minutes for large files\n        elif file_size_mb > 10:\n            timeout = 300  # 5 minutes for medium files\n        else:\n            timeout = 120  # 2 minutes for small files\n        \n        print(f\"   Timeout: {timeout}s\")\n        \n        # Run conversion\n        result = subprocess.run(\n            cmd, \n            capture_output=True, \n            text=True, \n            timeout=timeout\n        )\n        \n        end_time = time.time()\n        duration = round(end_time - start_time, 2)\n        \n        if result.returncode == 0:\n            status = 'SUCCESS'\n            error_message = None\n            # Check if PySpark was used for CSV files\n            converter_used = 'PySpark' if (file_ext == '.csv' and 'Using PySpark' in result.stdout) else 'Standard'\n            print(f\"   ‚úÖ Success ({duration}s) - {converter_used} converter\")\n            \n            # Log observation\n            observation['status'] = 'SUCCESS'\n            observation['duration'] = f\"{duration}s\"\n            observation['converter'] = converter_used\n            \n            # Verify output file exists in volume\n            try:\n                dbutils.fs.ls(output_path.replace('/Volumes/', 'dbfs:/Volumes/'))\n                print(f\"   ‚úÖ Output file verified in volume\")\n                observation['output_verified'] = True\n            except Exception:\n                print(f\"   ‚ö†Ô∏è  Output file not found in volume\")\n                observation['output_verified'] = False\n                \n        else:\n            status = 'FAILED'\n            error_message = result.stderr.strip() if result.stderr else result.stdout.strip()\n            converter_used = 'Unknown'\n            print(f\"   ‚ùå Failed ({duration}s)\")\n            print(f\"   Error: {error_message[:200]}...\")\n            \n            # Log observation\n            observation['status'] = 'FAILED'\n            observation['duration'] = f\"{duration}s\"\n            observation['error'] = error_message[:200]\n        \n        # Print detailed observation\n        print(f\"\\nüìù Test Observation:\")\n        for key, value in observation.items():\n            print(f\"   {key}: {value}\")\n        \n        return {\n            'file_name': file_name,\n            'file_type': file_type,\n            'status': status,\n            'duration_seconds': duration,\n            'error_message': error_message,\n            'output_path': output_path if status == 'SUCCESS' else None,\n            'size_mb': file_size_mb,\n            'command': ' '.join(cmd),\n            'converter_used': converter_used,\n            'observation': observation\n        }\n        \n    except subprocess.TimeoutExpired:\n        observation['status'] = 'TIMEOUT'\n        observation['duration'] = f\"{timeout}s\"\n        print(f\"   ‚è∞ Timeout after {timeout}s\")\n        \n        return {\n            'file_name': file_name,\n            'file_type': file_type,\n            'status': 'TIMEOUT',\n            'duration_seconds': timeout,\n            'error_message': f'Conversion timed out after {timeout} seconds',\n            'output_path': None,\n            'size_mb': file_size_mb,\n            'command': ' '.join(cmd),\n            'converter_used': 'Unknown',\n            'observation': observation\n        }\n    except Exception as e:\n        observation['status'] = 'ERROR'\n        observation['error'] = str(e)\n        print(f\"   üö´ Error: {str(e)}\")\n        \n        return {\n            'file_name': file_name,\n            'file_type': file_type,\n            'status': 'ERROR',\n            'duration_seconds': 0,\n            'error_message': str(e),\n            'output_path': None,\n            'size_mb': file_size_mb,\n            'command': ' '.join(cmd),\n            'converter_used': 'Unknown',\n            'observation': observation\n        }\n\ndef run_bulk_serverless_tests():\n    \"\"\"Run conversion tests for selected files in Databricks Serverless.\"\"\"\n    print(f\"\\nüöÄ Starting conversion tests in Databricks Serverless...\")\n    print(f\"üìÅ Output directory: {CONVERTED_OUTPUT_PATH}\")\n    print(f\"üìä Test mode: {'Smallest files only' if TEST_SMALLEST_FILES_ONLY else 'All files'}\")\n    print(f\"üîß Force conversion: {FORCE_CONVERSION}\")\n    print(f\"üöÄ Use PySpark for CSV: {USE_PYSPARK_FOR_CSV}\")\n    \n    test_results = []\n    test_observations = []\n    total_start_time = time.time()\n    \n    for i, file_info in enumerate(files_catalog, 1):\n        print(f\"\\n{'='*60}\")\n        print(f\"üìù Test {i}/{len(files_catalog)}\")\n        result = run_serverless_conversion_test(file_info)\n        test_results.append(result)\n        test_observations.append(result['observation'])\n    \n    total_end_time = time.time()\n    total_duration = round(total_end_time - total_start_time, 2)\n    \n    # Print test observations summary\n    print(f\"\\n{'='*60}\")\n    print(\"üìä TEST OBSERVATIONS SUMMARY:\")\n    print(f\"{'='*60}\")\n    for obs in test_observations:\n        print(f\"\\n{obs['file']} ({obs['type']}, {obs['size']}):\")\n        print(f\"   Status: {obs['status']}\")\n        print(f\"   Duration: {obs.get('duration', 'N/A')}\")\n        if 'converter' in obs:\n            print(f\"   Converter: {obs['converter']}\")\n        if 'error' in obs:\n            print(f\"   Error: {obs['error'][:100]}...\")\n    \n    return test_results, total_duration\n\n# Run the bulk conversion tests\nprint(\"üéØ Executing conversion tests...\")\ntest_results, total_test_duration = run_bulk_serverless_tests()\n\nprint(f\"\\nüèÅ Conversion testing completed in {total_test_duration} seconds!\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# MAGIC %md\n# MAGIC ### Conversion Testing Complete\n# MAGIC The conversion tests have been executed above. Continue to the next cell for the summary report.",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBTITLE 1,Setup Sample Datasets in Volume\n",
    "# =============================================================================\n",
    "# SAMPLE DATASETS SETUP IN UNITY CATALOG VOLUME\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"üì• Setting up sample datasets in volume: {SAMPLE_DATASETS_PATH}\")\n",
    "\n",
    "# Create volume directories using dbutils\n",
    "volume_datasets_path = SAMPLE_DATASETS_PATH.replace('/Volumes/', 'dbfs:/Volumes/')\n",
    "volume_output_path = CONVERTED_OUTPUT_PATH.replace('/Volumes/', 'dbfs:/Volumes/')\n",
    "\n",
    "try:\n",
    "    # Create sample datasets directory\n",
    "    dbutils.fs.mkdirs(volume_datasets_path)\n",
    "    print(f\"‚úÖ Created sample datasets directory: {SAMPLE_DATASETS_PATH}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    dbutils.fs.mkdirs(volume_output_path)\n",
    "    print(f\"‚úÖ Created output directory: {CONVERTED_OUTPUT_PATH}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Directory creation warning: {e}\")\n",
    "    print(\"   Directories may already exist\")\n",
    "\n",
    "# Install sample datasets using PyForge CLI\n",
    "print(\"\\nüì¶ Installing sample datasets using PyForge CLI...\")\n",
    "try:\n",
    "    # Use shell command to install sample datasets to volume path\n",
    "    result = subprocess.run([\n",
    "        'pyforge', 'install', 'sample-datasets', SAMPLE_DATASETS_PATH, '--force'\n",
    "    ], capture_output=True, text=True, timeout=300)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Sample datasets installed successfully!\")\n",
    "        print(f\"   Output: {result.stdout}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Sample datasets installation had issues: {result.stderr}\")\n",
    "        print(\"   Proceeding with available data...\")\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"‚ö†Ô∏è  Sample datasets installation timed out, creating minimal test datasets...\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Sample datasets installation failed: {e}\")\n",
    "    print(\"   Creating minimal test datasets in volume...\")\n",
    "\n",
    "# Create minimal test datasets directly in volume if needed\n",
    "try:\n",
    "    # Create test CSV file in volume\n",
    "    test_csv_data = \"\"\"id,name,category,value,date\n",
    "1,Sample Item 1,Category A,100.50,2023-01-01\n",
    "2,Sample Item 2,Category B,250.75,2023-01-02\n",
    "3,Sample Item 3,Category A,175.25,2023-01-03\n",
    "4,Sample Item 4,Category C,90.00,2023-01-04\n",
    "5,Sample Item 5,Category B,320.80,2023-01-05\"\"\"\n",
    "    \n",
    "    csv_path = f\"{SAMPLE_DATASETS_PATH}/csv/test_data.csv\"\n",
    "    dbutils.fs.mkdirs(f\"{volume_datasets_path}/csv\")\n",
    "    dbutils.fs.put(csv_path.replace('/Volumes/', 'dbfs:/Volumes/'), test_csv_data, overwrite=True)\n",
    "    print(f\"‚úÖ Created test CSV file: {csv_path}\")\n",
    "    \n",
    "    # Create test XML file in volume\n",
    "    test_xml_data = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<data>\n",
    "    <items>\n",
    "        <item id=\"1\">\n",
    "            <name>Sample Item 1</name>\n",
    "            <category>Category A</category>\n",
    "            <value>100.50</value>\n",
    "            <date>2023-01-01</date>\n",
    "        </item>\n",
    "        <item id=\"2\">\n",
    "            <name>Sample Item 2</name>\n",
    "            <category>Category B</category>\n",
    "            <value>250.75</value>\n",
    "            <date>2023-01-02</date>\n",
    "        </item>\n",
    "    </items>\n",
    "</data>\"\"\"\n",
    "    \n",
    "    xml_path = f\"{SAMPLE_DATASETS_PATH}/xml/test_data.xml\"\n",
    "    dbutils.fs.mkdirs(f\"{volume_datasets_path}/xml\")\n",
    "    dbutils.fs.put(xml_path.replace('/Volumes/', 'dbfs:/Volumes/'), test_xml_data, overwrite=True)\n",
    "    print(f\"‚úÖ Created test XML file: {xml_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error creating test files: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Sample datasets setup completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# DBTITLE 1,Comprehensive Conversion Testing\n# =============================================================================\n# BULK CONVERSION TESTING IN DATABRICKS SERVERLESS\n# =============================================================================\n\ndef run_serverless_conversion_test(file_info, verbose=None):\n    \"\"\"Run conversion test for a single file in Databricks Serverless environment.\"\"\"\n    # Use widget value if verbose not explicitly set\n    if verbose is None:\n        verbose = VERBOSE_OUTPUT\n        \n    file_path = file_info['file_path']\n    file_type = file_info['file_type']\n    file_name = file_info['file_name']\n    file_ext = file_info['extension']\n    \n    # Create output path in volume\n    output_name = file_name.split('.')[0]\n    output_path = f\"{CONVERTED_OUTPUT_PATH}/{file_info['category']}/{output_name}.parquet\"\n    \n    # Build conversion command\n    force_flag = '--force' if FORCE_CONVERSION else ''\n    pyspark_flag = '--force-pyspark' if USE_PYSPARK_FOR_CSV and file_ext == '.csv' else ''\n    excel_flag = '--separate' if file_ext in ['.xlsx', '.xls'] else ''\n    verbose_flag = '--verbose' if verbose else ''\n    \n    cmd = [\n        'pyforge', 'convert', file_path, output_path, \n        '--format', 'parquet', force_flag, pyspark_flag, excel_flag, verbose_flag\n    ]\n    cmd = [arg for arg in cmd if arg]  # Remove empty strings\n    \n    print(f\"üîÑ Converting {file_name} ({file_type})...\")\n    if verbose:\n        print(f\"   Command: {' '.join(cmd)}\")\n    \n    try:\n        start_time = time.time()\n        \n        # Set timeout based on file size\n        file_size_mb = file_info.get('size_mb', 0)\n        if file_size_mb > 100:\n            timeout = 600  # 10 minutes for large files\n        elif file_size_mb > 10:\n            timeout = 300  # 5 minutes for medium files\n        else:\n            timeout = 120  # 2 minutes for small files\n        \n        print(f\"   Timeout: {timeout}s (file size: {file_size_mb:.3f} MB)\")\n        \n        # Run conversion\n        result = subprocess.run(\n            cmd, \n            capture_output=True, \n            text=True, \n            timeout=timeout\n        )\n        \n        end_time = time.time()\n        duration = round(end_time - start_time, 2)\n        \n        if result.returncode == 0:\n            status = 'SUCCESS'\n            error_message = None\n            # Check if PySpark was used for CSV files\n            converter_used = 'PySpark' if (file_ext == '.csv' and 'PySpark' in result.stdout) else 'Standard'\n            print(f\"  ‚úÖ Success ({duration}s) - {converter_used} converter\")\n            \n            # Verify output file exists in volume\n            try:\n                dbutils.fs.ls(output_path.replace('/Volumes/', 'dbfs:/Volumes/'))\n                print(f\"  ‚úÖ Output file verified in volume: {output_path}\")\n            except Exception:\n                print(f\"  ‚ö†Ô∏è  Output file not found in volume: {output_path}\")\n                \n        else:\n            status = 'FAILED'\n            error_message = result.stderr.strip() if result.stderr else result.stdout.strip()\n            converter_used = 'Unknown'\n            print(f\"  ‚ùå Failed ({duration}s)\")\n            print(f\"     Error: {error_message[:200]}...\")\n            if len(error_message) > 200:\n                print(f\"     (Full error saved in results)\")\n        \n        return {\n            'file_name': file_name,\n            'file_type': file_type,\n            'status': status,\n            'duration_seconds': duration,\n            'error_message': error_message,\n            'output_path': output_path if status == 'SUCCESS' else None,\n            'size_mb': file_size_mb,\n            'command': ' '.join(cmd),\n            'converter_used': converter_used\n        }\n        \n    except subprocess.TimeoutExpired:\n        return {\n            'file_name': file_name,\n            'file_type': file_type,\n            'status': 'TIMEOUT',\n            'duration_seconds': timeout,\n            'error_message': f'Conversion timed out after {timeout} seconds',\n            'output_path': None,\n            'size_mb': file_size_mb,\n            'command': ' '.join(cmd),\n            'converter_used': 'Unknown'\n        }\n    except Exception as e:\n        return {\n            'file_name': file_name,\n            'file_type': file_type,\n            'status': 'ERROR',\n            'duration_seconds': 0,\n            'error_message': str(e),\n            'output_path': None,\n            'size_mb': file_size_mb,\n            'command': ' '.join(cmd),\n            'converter_used': 'Unknown'\n        }\n\ndef run_bulk_serverless_tests():\n    \"\"\"Run conversion tests for all discovered files in Databricks Serverless.\"\"\"\n    print(f\"üöÄ Starting bulk conversion tests in Databricks Serverless...\")\n    print(f\"üìÅ Output directory: {CONVERTED_OUTPUT_PATH}\")\n    print(f\"üìä Verbose mode: {'ON' if VERBOSE_OUTPUT else 'OFF'}\")\n    \n    test_results = []\n    total_start_time = time.time()\n    \n    for i, file_info in enumerate(files_catalog, 1):\n        print(f\"\\nüìù Test {i}/{len(files_catalog)}: {file_info['file_name']}\")\n        result = run_serverless_conversion_test(file_info)\n        test_results.append(result)\n    \n    total_end_time = time.time()\n    total_duration = round(total_end_time - total_start_time, 2)\n    \n    return test_results, total_duration\n\n# Run the bulk conversion tests\nprint(\"üéØ Executing bulk conversion tests in Databricks Serverless...\")\ntest_results, total_test_duration = run_bulk_serverless_tests()\n\nprint(f\"\\nüèÅ Bulk conversion testing completed in {total_test_duration} seconds!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBTITLE 1,Comprehensive Conversion Testing\n",
    "# =============================================================================\n",
    "# BULK CONVERSION TESTING IN DATABRICKS SERVERLESS\n",
    "# =============================================================================\n",
    "\n",
    "def run_serverless_conversion_test(file_info, verbose=True):\n",
    "    \"\"\"Run conversion test for a single file in Databricks Serverless environment.\"\"\"\n",
    "    file_path = file_info['file_path']\n",
    "    file_type = file_info['file_type']\n",
    "    file_name = file_info['file_name']\n",
    "    file_ext = file_info['extension']\n",
    "    \n",
    "    # Create output path in volume\n",
    "    output_name = file_name.split('.')[0]\n",
    "    output_path = f\"{CONVERTED_OUTPUT_PATH}/{file_info['category']}/{output_name}.parquet\"\n",
    "    \n",
    "    # Build conversion command\n",
    "    force_flag = '--force' if FORCE_CONVERSION else ''\n",
    "    pyspark_flag = '--force-pyspark' if USE_PYSPARK_FOR_CSV and file_ext == '.csv' else ''\n",
    "    excel_flag = '--separate' if file_ext in ['.xlsx', '.xls'] else ''\n",
    "    verbose_flag = '--verbose' if verbose else ''\n",
    "    \n",
    "    cmd = [\n",
    "        'pyforge', 'convert', file_path, output_path, \n",
    "        '--format', 'parquet', force_flag, pyspark_flag, excel_flag, verbose_flag\n",
    "    ]\n",
    "    cmd = [arg for arg in cmd if arg]  # Remove empty strings\n",
    "    \n",
    "    print(f\"üîÑ Converting {file_name} ({file_type})...\")\n",
    "    if verbose:\n",
    "        print(f\"   Command: {' '.join(cmd)}\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Set timeout based on file size\n",
    "        file_size_mb = file_info.get('size_mb', 0)\n",
    "        if file_size_mb > 100:\n",
    "            timeout = 600  # 10 minutes for large files\n",
    "        elif file_size_mb > 10:\n",
    "            timeout = 300  # 5 minutes for medium files\n",
    "        else:\n",
    "            timeout = 120  # 2 minutes for small files\n",
    "        \n",
    "        print(f\"   Timeout: {timeout}s (file size: {file_size_mb:.3f} MB)\")\n",
    "        \n",
    "        # Run conversion\n",
    "        result = subprocess.run(\n",
    "            cmd, \n",
    "            capture_output=True, \n",
    "            text=True, \n",
    "            timeout=timeout\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        duration = round(end_time - start_time, 2)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            status = 'SUCCESS'\n",
    "            error_message = None\n",
    "            # Check if PySpark was used for CSV files\n",
    "            converter_used = 'PySpark' if (file_ext == '.csv' and 'PySpark' in result.stdout) else 'Standard'\n",
    "            print(f\"  ‚úÖ Success ({duration}s) - {converter_used} converter\")\n",
    "            \n",
    "            # Verify output file exists in volume\n",
    "            try:\n",
    "                dbutils.fs.ls(output_path.replace('/Volumes/', 'dbfs:/Volumes/'))\n",
    "                print(f\"  ‚úÖ Output file verified in volume: {output_path}\")\n",
    "            except Exception:\n",
    "                print(f\"  ‚ö†Ô∏è  Output file not found in volume: {output_path}\")\n",
    "                \n",
    "        else:\n",
    "            status = 'FAILED'\n",
    "            error_message = result.stderr.strip() if result.stderr else result.stdout.strip()\n",
    "            converter_used = 'Unknown'\n",
    "            print(f\"  ‚ùå Failed ({duration}s)\")\n",
    "            print(f\"     Error: {error_message[:200]}...\")\n",
    "            if len(error_message) > 200:\n",
    "                print(f\"     (Full error saved in results)\")\n",
    "        \n",
    "        return {\n",
    "            'file_name': file_name,\n",
    "            'file_type': file_type,\n",
    "            'status': status,\n",
    "            'duration_seconds': duration,\n",
    "            'error_message': error_message,\n",
    "            'output_path': output_path if status == 'SUCCESS' else None,\n",
    "            'size_mb': file_size_mb,\n",
    "            'command': ' '.join(cmd),\n",
    "            'converter_used': converter_used\n",
    "        }\n",
    "        \n",
    "    except subprocess.TimeoutExpired:\n",
    "        return {\n",
    "            'file_name': file_name,\n",
    "            'file_type': file_type,\n",
    "            'status': 'TIMEOUT',\n",
    "            'duration_seconds': timeout,\n",
    "            'error_message': f'Conversion timed out after {timeout} seconds',\n",
    "            'output_path': None,\n",
    "            'size_mb': file_size_mb,\n",
    "            'command': ' '.join(cmd),\n",
    "            'converter_used': 'Unknown'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'file_name': file_name,\n",
    "            'file_type': file_type,\n",
    "            'status': 'ERROR',\n",
    "            'duration_seconds': 0,\n",
    "            'error_message': str(e),\n",
    "            'output_path': None,\n",
    "            'size_mb': file_size_mb,\n",
    "            'command': ' '.join(cmd),\n",
    "            'converter_used': 'Unknown'\n",
    "        }\n",
    "\n",
    "def run_bulk_serverless_tests():\n",
    "    \"\"\"Run conversion tests for all discovered files in Databricks Serverless.\"\"\"\n",
    "    print(f\"üöÄ Starting bulk conversion tests in Databricks Serverless...\")\n",
    "    print(f\"üìÅ Output directory: {CONVERTED_OUTPUT_PATH}\")\n",
    "    \n",
    "    test_results = []\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for i, file_info in enumerate(files_catalog, 1):\n",
    "        print(f\"\\nüìù Test {i}/{len(files_catalog)}: {file_info['file_name']}\")\n",
    "        result = run_serverless_conversion_test(file_info, verbose=True)\n",
    "        test_results.append(result)\n",
    "    \n",
    "    total_end_time = time.time()\n",
    "    total_duration = round(total_end_time - total_start_time, 2)\n",
    "    \n",
    "    return test_results, total_duration\n",
    "\n",
    "# Run the bulk conversion tests\n",
    "print(\"üéØ Executing bulk conversion tests in Databricks Serverless...\")\n",
    "test_results, total_test_duration = run_bulk_serverless_tests()\n",
    "\n",
    "print(f\"\\nüèÅ Bulk conversion testing completed in {total_test_duration} seconds!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBTITLE 1,Validate Converted Files with Spark\n",
    "# =============================================================================\n",
    "# CONVERTED FILE VALIDATION USING SPARK\n",
    "# =============================================================================\n",
    "\n",
    "def validate_converted_files_with_spark():\n",
    "    \"\"\"Validate converted Parquet files using Spark in Databricks Serverless.\"\"\"\n",
    "    print(\"üîç Validating converted Parquet files with Spark...\")\n",
    "    \n",
    "    successful_conversions = df_detailed_results[df_detailed_results['status'] == 'SUCCESS']\n",
    "    validation_results = []\n",
    "    \n",
    "    if len(successful_conversions) == 0:\n",
    "        print(\"‚ö†Ô∏è  No successful conversions to validate.\")\n",
    "        return\n",
    "    \n",
    "    for _, result in successful_conversions.iterrows():\n",
    "        output_path = result['output_path']\n",
    "        file_name = result['file_name']\n",
    "        \n",
    "        try:\n",
    "            # Try to read the parquet file with Spark\n",
    "            df_spark = spark.read.parquet(output_path)\n",
    "            row_count = df_spark.count()\n",
    "            col_count = len(df_spark.columns)\n",
    "            \n",
    "            # Get schema info\n",
    "            schema_info = [(field.name, str(field.dataType)) for field in df_spark.schema.fields]\n",
    "            \n",
    "            validation_results.append({\n",
    "                'file_name': file_name,\n",
    "                'status': 'VALID',\n",
    "                'rows': row_count,\n",
    "                'columns': col_count,\n",
    "                'schema_sample': str(schema_info[:3]) if schema_info else 'No schema',\n",
    "                'error': None\n",
    "            })\n",
    "            \n",
    "            print(f\"  ‚úÖ {file_name}: {row_count} rows, {col_count} columns\")\n",
    "            \n",
    "            # Show a sample of data for small files\n",
    "            if row_count <= 10:\n",
    "                print(f\"     Sample data:\")\n",
    "                df_spark.show(5, truncate=False)\n",
    "            \n",
    "        except Exception as e:\n",
    "            validation_results.append({\n",
    "                'file_name': file_name,\n",
    "                'status': 'INVALID',\n",
    "                'rows': 0,\n",
    "                'columns': 0,\n",
    "                'schema_sample': None,\n",
    "                'error': str(e)\n",
    "            })\n",
    "            print(f\"  ‚ùå {file_name}: Validation failed - {str(e)[:100]}...\")\n",
    "    \n",
    "    if validation_results:\n",
    "        print(f\"\\nüìä Spark Validation Summary:\")\n",
    "        df_validation = pd.DataFrame(validation_results)\n",
    "        display(df_validation)\n",
    "        \n",
    "        valid_count = len(df_validation[df_validation['status'] == 'VALID'])\n",
    "        total_count = len(df_validation)\n",
    "        print(f\"\\n‚úÖ Validation Results: {valid_count}/{total_count} files are valid Parquet files\")\n",
    "        \n",
    "        if valid_count == total_count:\n",
    "            print(\"üéâ ALL CONVERTED FILES ARE VALID PARQUET FILES!\")\n",
    "            print(\"‚úÖ PyForge CLI is working perfectly in Databricks Serverless environment\")\n",
    "\n",
    "# Run Spark validation\n",
    "validate_converted_files_with_spark()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}