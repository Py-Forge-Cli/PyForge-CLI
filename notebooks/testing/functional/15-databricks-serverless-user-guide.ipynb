{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyForge CLI - User Guide for Databricks Serverless\n",
    "\n",
    "This notebook demonstrates how to use PyForge CLI to convert various file formats in Databricks Serverless environments.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Installation and Setup](#installation)\n",
    "2. [Install Sample Datasets](#sample-datasets)\n",
    "3. [List Available Datasets](#list-datasets)\n",
    "4. [DBF File Conversion](#dbf-conversion)\n",
    "5. [Excel File Conversion](#excel-conversion)\n",
    "6. [CSV File Conversion](#csv-conversion)\n",
    "7. [XML File Conversion](#xml-conversion)\n",
    "8. [Working with Converted Data](#working-with-data)\n",
    "9. [Command Reference](#command-reference)\n",
    "10. [Database File Conversion (.mdb/.accdb) - Special Instructions](#database-conversion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup <a id='installation'></a>\n",
    "\n",
    "**Important:** In Databricks Serverless, always include the PyPI index URL for proper dependency resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyForge CLI with explicit PyPI index URL (required for Databricks Serverless)\n",
    "%pip install pyforge-cli==1.0.9 --no-cache-dir --quiet --index-url https://pypi.org/simple/ --trusted-host pypi.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart Python kernel to ensure clean imports\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Check PyForge version\n",
    "result = subprocess.run(['pyforge', '--version'], capture_output=True, text=True)\n",
    "print(f\"‚úÖ PyForge CLI Version: {result.stdout.strip()}\")\n",
    "\n",
    "# Check Python version\n",
    "print(f\"‚úÖ Python Version: {sys.version}\")\n",
    "\n",
    "# Check PyForge location\n",
    "which_result = subprocess.run(['which', 'pyforge'], capture_output=True, text=True)\n",
    "print(f\"‚úÖ PyForge Location: {which_result.stdout.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Sample Datasets <a id='sample-datasets'></a>\n",
    "\n",
    "PyForge CLI includes a curated collection of sample datasets for testing all supported formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Unity Catalog volume path for sample datasets\n",
    "volume_path = \"dbfs:/Volumes/cortex_dev_catalog/sandbox_testing/pyforge_sample_datasets\"\n",
    "local_volume_path = volume_path.replace('dbfs:', '/dbfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Install sample datasets using shell command\n",
    "pyforge install sample-datasets --output /dbfs/Volumes/cortex_dev_catalog/sandbox_testing/pyforge_sample_datasets --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. List Available Sample Datasets <a id='list-datasets'></a>\n",
    "\n",
    "Let's explore what sample datasets are available for conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the sample datasets directory\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    files = dbutils.fs.ls(volume_path)\n",
    "    \n",
    "    # Organize files by type\n",
    "    file_data = []\n",
    "    for file_info in files:\n",
    "        file_name = file_info.name\n",
    "        file_path = file_info.path\n",
    "        file_size = file_info.size / (1024 * 1024)  # Convert to MB\n",
    "        \n",
    "        # Determine file type\n",
    "        if file_name.endswith('.mdb'):\n",
    "            file_type = 'Access Database (Legacy)'\n",
    "        elif file_name.endswith('.accdb'):\n",
    "            file_type = 'Access Database (Modern)'\n",
    "        elif file_name.endswith('.dbf'):\n",
    "            file_type = 'dBASE Database'\n",
    "        elif file_name.endswith('.xlsx'):\n",
    "            file_type = 'Excel Spreadsheet'\n",
    "        elif file_name.endswith('.csv'):\n",
    "            file_type = 'CSV File'\n",
    "        elif file_name.endswith('.xml'):\n",
    "            file_type = 'XML Document'\n",
    "        elif file_name.endswith('.pdf'):\n",
    "            file_type = 'PDF Document'\n",
    "        else:\n",
    "            file_type = 'Other'\n",
    "        \n",
    "        if not file_info.isDir():\n",
    "            file_data.append({\n",
    "                'File Name': file_name,\n",
    "                'Type': file_type,\n",
    "                'Size (MB)': round(file_size, 2),\n",
    "                'Path': file_path\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame and display\n",
    "    df_files = pd.DataFrame(file_data)\n",
    "    df_files = df_files.sort_values(['Type', 'File Name'])\n",
    "    \n",
    "    print(\"üìÅ Available Sample Datasets:\\n\")\n",
    "    display(df_files)\n",
    "    \n",
    "    # Summary by type\n",
    "    print(\"\\nüìä Summary by File Type:\")\n",
    "    summary = df_files.groupby('Type').agg({\n",
    "        'File Name': 'count',\n",
    "        'Size (MB)': 'sum'\n",
    "    }).rename(columns={'File Name': 'Count', 'Size (MB)': 'Total Size (MB)'})\n",
    "    display(summary)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error listing files: {e}\")\n",
    "    print(\"Please ensure the sample datasets were installed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DBF File Conversion <a id='dbf-conversion'></a>\n",
    "\n",
    "Convert dBASE database files with automatic encoding detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Convert DBF file using shell command\n",
    "echo \"üîÑ Converting customer_data.dbf...\"\n",
    "pyforge convert /dbfs/Volumes/cortex_dev_catalog/sandbox_testing/pyforge_sample_datasets/customer_data.dbf \\\n",
    "    /dbfs/Volumes/cortex_dev_catalog/sandbox_testing/pyforge_sample_datasets/converted/customer_data.parquet \\\n",
    "    --encoding cp1252 \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and display the converted DBF data\n",
    "output_file = f\"{volume_path}/converted/customer_data.parquet\"\n",
    "df_dbf = pd.read_parquet(output_file.replace('dbfs:/', '/dbfs/'))\n",
    "print(f\"üìä DBF Data: {len(df_dbf)} records, {len(df_dbf.columns)} columns\")\n",
    "print(f\"Columns: {', '.join(df_dbf.columns)}\")\n",
    "display(df_dbf.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Excel File Conversion <a id='excel-conversion'></a>\n",
    "\n",
    "Convert Excel files with support for multiple sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Convert Excel file with sheet combination\n",
    "echo \"üîÑ Converting financial_report.xlsx...\"\n",
    "pyforge convert /dbfs/Volumes/cortex_dev_catalog/sandbox_testing/pyforge_sample_datasets/financial_report.xlsx \\\n",
    "    /dbfs/Volumes/cortex_dev_catalog/sandbox_testing/pyforge_sample_datasets/converted/financial/ \\\n",
    "    --combine \\\n",
    "    --compression gzip \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List and analyze converted Excel files\n",
    "output_dir = f\"{volume_path}/converted/financial/\"\n",
    "parquet_files = [f for f in dbutils.fs.ls(output_dir) if f.name.endswith('.parquet')]\n",
    "\n",
    "for file in parquet_files:\n",
    "    df = pd.read_parquet(file.path.replace('dbfs:/', '/dbfs/'))\n",
    "    print(f\"\\nüìä {file.name}:\")\n",
    "    print(f\"   - Rows: {len(df):,}\")\n",
    "    print(f\"   - Columns: {len(df.columns)}\")\n",
    "    print(f\"   - Column Names: {', '.join(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}\")\n",
    "    display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CSV File Conversion <a id='csv-conversion'></a>\n",
    "\n",
    "Convert CSV files with automatic delimiter and encoding detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Convert CSV file\n",
    "echo \"üîÑ Converting sales_data.csv...\"\n",
    "pyforge convert /dbfs/Volumes/cortex_dev_catalog/sandbox_testing/pyforge_sample_datasets/sales_data.csv \\\n",
    "    /dbfs/Volumes/cortex_dev_catalog/sandbox_testing/pyforge_sample_datasets/converted/sales_data.parquet \\\n",
    "    --compression snappy \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and analyze the converted CSV data\n",
    "output_file = f\"{volume_path}/converted/sales_data.parquet\"\n",
    "df_csv = pd.read_parquet(output_file.replace('dbfs:/', '/dbfs/'))\n",
    "print(f\"üìä Sales Data Analysis:\")\n",
    "print(f\"   - Total Records: {len(df_csv):,}\")\n",
    "print(f\"   - Columns: {', '.join(df_csv.columns)}\")\n",
    "print(f\"   - Data Types:\\n{df_csv.dtypes}\")\n",
    "display(df_csv.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. XML File Conversion <a id='xml-conversion'></a>\n",
    "\n",
    "Convert XML files with intelligent structure flattening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Convert XML file with moderate flattening\n",
    "echo \"üîÑ Converting books_catalog.xml...\"\n",
    "pyforge convert /dbfs/Volumes/cortex_dev_catalog/sandbox_testing/pyforge_sample_datasets/books_catalog.xml \\\n",
    "    /dbfs/Volumes/cortex_dev_catalog/sandbox_testing/pyforge_sample_datasets/converted/books_catalog.parquet \\\n",
    "    --flatten-strategy moderate \\\n",
    "    --array-handling expand \\\n",
    "    --compression gzip \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and display the flattened XML data\n",
    "output_file = f\"{volume_path}/converted/books_catalog.parquet\"\n",
    "df_xml = pd.read_parquet(output_file.replace('dbfs:/', '/dbfs/'))\n",
    "print(f\"üìö Books Catalog:\")\n",
    "print(f\"   - Total Books: {len(df_xml)}\")\n",
    "print(f\"   - Columns: {', '.join(df_xml.columns)}\")\n",
    "display(df_xml.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Working with Converted Data <a id='working-with-data'></a>\n",
    "\n",
    "Load and analyze the converted Parquet files using both Pandas and Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pandas for Small to Medium Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Analyze converted CSV data\n",
    "sales_path = f\"/dbfs/{volume_path.replace('dbfs:/', '')}/converted/sales_data.parquet\"\n",
    "sales_pd = pd.read_parquet(sales_path)\n",
    "\n",
    "print(\"üìä Pandas Analysis - Sales Data\")\n",
    "print(f\"Total records: {len(sales_pd)}\")\n",
    "print(f\"Columns: {', '.join(sales_pd.columns)}\")\n",
    "print(\"\\nData summary:\")\n",
    "display(sales_pd.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Spark for Large-Scale Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load converted files as Spark DataFrames\n",
    "sales_spark = spark.read.parquet(f\"{volume_path}/converted/sales_data.parquet\")\n",
    "\n",
    "# Register as temporary view for SQL queries\n",
    "sales_spark.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "print(\"‚úÖ Spark DataFrame created and registered as view\")\n",
    "print(f\"   - sales: {sales_spark.count()} rows\")\n",
    "\n",
    "# Run SQL analytics\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_records,\n",
    "        COUNT(DISTINCT customer_id) as unique_customers\n",
    "    FROM sales\n",
    "\"\"\")\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. PyForge CLI Command Reference <a id='command-reference'></a>\n",
    "\n",
    "### üìã Basic Commands:\n",
    "\n",
    "```bash\n",
    "# Check version\n",
    "pyforge --version\n",
    "\n",
    "# Get help\n",
    "pyforge --help\n",
    "pyforge convert --help\n",
    "\n",
    "# Install sample datasets\n",
    "pyforge install sample-datasets --output /path/to/output\n",
    "```\n",
    "\n",
    "### üîÑ Conversion Commands:\n",
    "\n",
    "```bash\n",
    "# Basic conversion\n",
    "pyforge convert input.csv output.parquet\n",
    "\n",
    "# With compression\n",
    "pyforge convert input.csv output.parquet --compression gzip\n",
    "\n",
    "# Excel with sheet combination\n",
    "pyforge convert data.xlsx output/ --combine\n",
    "\n",
    "# XML with flattening options\n",
    "pyforge convert data.xml output.parquet --flatten-strategy moderate --array-handling expand\n",
    "\n",
    "# DBF with encoding\n",
    "pyforge convert data.dbf output.parquet --encoding cp1252\n",
    "```\n",
    "\n",
    "### ‚öôÔ∏è Common Options:\n",
    "\n",
    "- `--compression`: gzip, snappy, brotli, lz4 (default: snappy)\n",
    "- `--verbose`: Show detailed conversion progress\n",
    "- `--encoding`: Specify character encoding (auto-detected by default)\n",
    "- `--combine`: Combine multiple sheets into one file (Excel only)\n",
    "- `--flatten-strategy`: conservative, moderate, aggressive (XML only)\n",
    "- `--array-handling`: expand, preserve (XML only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Database File Conversion (.mdb/.accdb) - Special Instructions <a id='database-conversion'></a>\n",
    "\n",
    "### ‚ö†Ô∏è Important: MDB/ACCDB Files Require Subprocess\n",
    "\n",
    "Due to Java SDK dependencies, MDB/ACCDB files **MUST** use subprocess commands instead of `%sh` magic commands.\n",
    "\n",
    "- ‚úÖ **MDB/ACCDB:** `subprocess.run(['pyforge', 'convert', 'file.mdb'])`\n",
    "- ‚ùå **Do NOT use:** `%sh pyforge convert file.mdb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert MDB File (Access 2000-2003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Northwind.mdb database\n",
    "mdb_file = f\"{volume_path}/Northwind.mdb\"\n",
    "output_dir = f\"{volume_path}/converted/northwind/\"\n",
    "\n",
    "print(\"üîÑ Converting Northwind.mdb database...\")\n",
    "print(f\"Input: {mdb_file}\")\n",
    "print(f\"Output: {output_dir}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Use subprocess for MDB files (REQUIRED due to Java SDK dependencies)\n",
    "result = subprocess.run([\n",
    "    'pyforge', 'convert',\n",
    "    mdb_file,\n",
    "    output_dir,\n",
    "    '--compression', 'gzip',\n",
    "    '--verbose'\n",
    "], capture_output=True, text=True)\n",
    "\n",
    "print(\"Conversion Output:\")\n",
    "print(result.stdout)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n‚úÖ Conversion successful!\")\n",
    "    \n",
    "    # List converted tables\n",
    "    converted_files = dbutils.fs.ls(output_dir)\n",
    "    tables = [f.name for f in converted_files if f.name.endswith('.parquet')]\n",
    "    \n",
    "    print(f\"\\nüìä Converted {len(tables)} tables:\")\n",
    "    for table in sorted(tables):\n",
    "        print(f\"  - {table}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Conversion failed!\")\n",
    "    print(f\"Error: {result.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert ACCDB File (Access 2007+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a modern Access database with specific tables\n",
    "accdb_file = f\"{volume_path}/AdventureWorks.accdb\"\n",
    "output_dir = f\"{volume_path}/converted/adventureworks/\"\n",
    "\n",
    "print(\"üîÑ Converting AdventureWorks.accdb database...\")\n",
    "print(\"Converting only specific tables: Customers, Orders, Products\")\n",
    "print(f\"Input: {accdb_file}\")\n",
    "print(f\"Output: {output_dir}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Use subprocess for ACCDB files (REQUIRED due to Java SDK dependencies)\n",
    "result = subprocess.run([\n",
    "    'pyforge', 'convert',\n",
    "    accdb_file,\n",
    "    output_dir,\n",
    "    '--tables', 'Customers,Orders,Products',  # Convert only these tables\n",
    "    '--compression', 'snappy',\n",
    "    '--verbose'\n",
    "], capture_output=True, text=True)\n",
    "\n",
    "print(\"Conversion Output:\")\n",
    "print(result.stdout)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n‚úÖ Conversion successful!\")\n",
    "    \n",
    "    # Read and display sample data from Customers table\n",
    "    customers_path = f\"{output_dir}Customers.parquet\"\n",
    "    df_customers = pd.read_parquet(customers_path.replace('dbfs:/', '/dbfs/'))\n",
    "    print(f\"\\nüìä Customers Table: {len(df_customers)} records\")\n",
    "    display(df_customers.head(5))\n",
    "else:\n",
    "    print(f\"\\n‚ùå Conversion failed: {result.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDB/ACCDB Command Reference\n",
    "\n",
    "```python\n",
    "# Basic MDB conversion\n",
    "subprocess.run(['pyforge', 'convert', 'database.mdb', 'output_dir/'])\n",
    "\n",
    "# Convert specific tables\n",
    "subprocess.run(['pyforge', 'convert', 'database.accdb', 'output/', '--tables', 'Table1,Table2'])\n",
    "\n",
    "# With compression\n",
    "subprocess.run(['pyforge', 'convert', 'database.mdb', 'output/', '--compression', 'gzip'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up example (optional)\n",
    "# Uncomment to remove converted files\n",
    "# dbutils.fs.rm(f\"{volume_path}/converted/\", recurse=True)\n",
    "print(\"üéâ Tutorial complete! Your converted files are available in the volume.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Notes for Databricks Serverless\n",
    "\n",
    "1. **Use %sh commands** for CSV, XML, Excel, DBF file conversions\n",
    "2. **Use subprocess.run()** ONLY for MDB/ACCDB files due to Java SDK dependencies\n",
    "3. **Include PyPI index URL** when installing: `--index-url https://pypi.org/simple/`\n",
    "4. **Use dbfs:// prefix** for Unity Catalog volume paths in Python code\n",
    "5. **Use /dbfs/ prefix** for shell commands\n",
    "6. **Restart Python kernel** after installation with `dbutils.library.restartPython()`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}