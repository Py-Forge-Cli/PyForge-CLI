{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyForge CLI Databricks Extension - Serverless Testing\n",
    "\n",
    "This notebook specifically tests PyForge CLI Databricks extension in serverless compute environments.\n",
    "\n",
    "## Key Serverless Features Tested\n",
    "- **PySpark Detection**: Automatic serverless environment detection\n",
    "- **PySpark Integration**: CSV, Excel, XML conversion using PySpark\n",
    "- **Performance Optimization**: Serverless-specific optimizations\n",
    "- **Unity Catalog Integration**: Volume operations and catalog access\n",
    "- **Fallback Behavior**: Graceful degradation when PySpark unavailable\n",
    "\n",
    "## Serverless-Specific Configuration\n",
    "- **Compute**: Databricks Serverless SQL/Python\n",
    "- **Storage**: Unity Catalog Volumes\n",
    "- **Processing**: PySpark-optimized conversions\n",
    "- **Memory**: Distributed processing support\n",
    "\n",
    "## Prerequisites\n",
    "1. Databricks serverless compute environment\n",
    "2. Unity Catalog access with volume permissions\n",
    "3. PyForge CLI with Databricks extension\n",
    "4. Sample datasets in Unity Catalog Volume\n",
    "\n",
    "## Serverless Optimizations Tested\n",
    "1. **Large File Processing**: Multi-GB file handling\n",
    "2. **Distributed Conversion**: Parallel processing\n",
    "3. **Memory Management**: Efficient memory usage\n",
    "4. **Catalog Integration**: Seamless volume operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serverless Configuration Widgets\n",
    "# =============================================================================\n",
    "# SERVERLESS CONFIGURATION SECTION\n",
    "# =============================================================================\n",
    "\n",
    "# Create serverless-specific widgets\n",
    "dbutils.widgets.text(\"catalog_name\", \"main\", \"Unity Catalog Name\")\n",
    "dbutils.widgets.text(\"schema_name\", \"default\", \"Schema Name\")\n",
    "dbutils.widgets.text(\"volume_name\", \"pyforge\", \"Volume Name\")\n",
    "dbutils.widgets.text(\"pyforge_version\", \"latest\", \"PyForge Version\")\n",
    "dbutils.widgets.dropdown(\"test_size\", \"medium\", [\"small\", \"medium\", \"large\"], \"Test Data Size\")\n",
    "dbutils.widgets.dropdown(\"pyspark_mode\", \"auto\", [\"auto\", \"force\", \"disable\"], \"PySpark Mode\")\n",
    "dbutils.widgets.checkbox(\"test_volumes\", True, \"Test Unity Catalog Volumes\")\n",
    "dbutils.widgets.checkbox(\"performance_tests\", True, \"Run Performance Tests\")\n",
    "dbutils.widgets.checkbox(\"verbose_logging\", True, \"Verbose Logging\")\n",
    "\n",
    "# Get widget values\n",
    "CATALOG_NAME = dbutils.widgets.get(\"catalog_name\")\n",
    "SCHEMA_NAME = dbutils.widgets.get(\"schema_name\")\n",
    "VOLUME_NAME = dbutils.widgets.get(\"volume_name\")\n",
    "PYFORGE_VERSION = dbutils.widgets.get(\"pyforge_version\")\n",
    "TEST_SIZE = dbutils.widgets.get(\"test_size\")\n",
    "PYSPARK_MODE = dbutils.widgets.get(\"pyspark_mode\")\n",
    "TEST_VOLUMES = dbutils.widgets.get(\"test_volumes\") == \"true\"\n",
    "PERFORMANCE_TESTS = dbutils.widgets.get(\"performance_tests\") == \"true\"\n",
    "VERBOSE_LOGGING = dbutils.widgets.get(\"verbose_logging\") == \"true\"\n",
    "\n",
    "# Construct volume path\n",
    "VOLUME_PATH = f\"/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/{VOLUME_NAME}\"\n",
    "SAMPLE_DATA_PATH = f\"{VOLUME_PATH}/sample-datasets\"\n",
    "TEST_OUTPUT_PATH = f\"{VOLUME_PATH}/test-output\"\n",
    "\n",
    "print(\"üöÄ Databricks Serverless Extension Test Configuration:\")\n",
    "print(f\"   Catalog: {CATALOG_NAME}\")\n",
    "print(f\"   Schema: {SCHEMA_NAME}\")\n",
    "print(f\"   Volume: {VOLUME_NAME}\")\n",
    "print(f\"   Volume Path: {VOLUME_PATH}\")\n",
    "print(f\"   PyForge Version: {PYFORGE_VERSION}\")\n",
    "print(f\"   Test Size: {TEST_SIZE}\")\n",
    "print(f\"   PySpark Mode: {PYSPARK_MODE}\")\n",
    "print(f\"   Test Volumes: {TEST_VOLUMES}\")\n",
    "print(f\"   Performance Tests: {PERFORMANCE_TESTS}\")\n",
    "print(f\"   Verbose Logging: {VERBOSE_LOGGING}\")\n",
    "\n",
    "# Initialize serverless test tracking\n",
    "serverless_results = {\n",
    "    'environment_validation': None,\n",
    "    'pyspark_integration': None,\n",
    "    'volume_operations': None,\n",
    "    'conversion_tests': {},\n",
    "    'performance_metrics': {},\n",
    "    'catalog_integration': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serverless Environment Validation\n",
    "# =============================================================================\n",
    "# SERVERLESS VALIDATION SECTION\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üîç Validating serverless environment...\")\n",
    "\n",
    "validation_start_time = time.time()\n",
    "validation_results = {}\n",
    "\n",
    "# Check serverless runtime\n",
    "try:\n",
    "    runtime_version = os.environ.get('DATABRICKS_RUNTIME_VERSION', 'unknown')\n",
    "    print(f\"üìã Runtime Version: {runtime_version}\")\n",
    "    \n",
    "    is_serverless = 'serverless' in runtime_version.lower()\n",
    "    validation_results['is_serverless'] = is_serverless\n",
    "    \n",
    "    if is_serverless:\n",
    "        print(\"‚úÖ Serverless runtime detected\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Non-serverless runtime - some tests may not apply\")\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"‚ùå Runtime detection failed: {e}\")\n",
    "    validation_results['runtime_error'] = str(e)\n",
    "\n",
    "# Validate PySpark availability and configuration\n",
    "try:\n",
    "    print(\"üîß Checking PySpark configuration...\")\n",
    "    \n",
    "    import pyspark\n",
    "    from pyspark.sql import SparkSession\n",
    "    \n",
    "    pyspark_version = pyspark.__version__\n",
    "    print(f\"‚úÖ PySpark version: {pyspark_version}\")\n",
    "    \n",
    "    # Get Spark session info\n",
    "    spark_info = {\n",
    "        'app_name': spark.sparkContext.appName,\n",
    "        'spark_version': spark.version,\n",
    "        'master': spark.sparkContext.master,\n",
    "        'executor_memory': spark.conf.get('spark.executor.memory', 'default'),\n",
    "        'driver_memory': spark.conf.get('spark.driver.memory', 'default')\n",
    "    }\n",
    "    \n",
    "    print(\"‚úÖ Spark Session Configuration:\")\n",
    "    for key, value in spark_info.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    validation_results['pyspark_available'] = True\n",
    "    validation_results['spark_info'] = spark_info\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"‚ùå PySpark validation failed: {e}\")\n",
    "    validation_results['pyspark_available'] = False\n",
    "    validation_results['pyspark_error'] = str(e)\n",
    "\n",
    "# Test Unity Catalog access\n",
    "if TEST_VOLUMES:\n",
    "    try:\n",
    "        print(f\"üìÅ Testing Unity Catalog volume access: {VOLUME_PATH}\")\n",
    "        \n",
    "        # Test volume listing\n",
    "        volume_files = dbutils.fs.ls(VOLUME_PATH)\n",
    "        print(f\"‚úÖ Volume accessible - found {len(volume_files)} items\")\n",
    "        \n",
    "        # Test write permissions\n",
    "        test_file_path = f\"{VOLUME_PATH}/test_write_permission.txt\"\n",
    "        dbutils.fs.put(test_file_path, \"test content\")\n",
    "        dbutils.fs.rm(test_file_path)\n",
    "        print(\"‚úÖ Volume write permissions confirmed\")\n",
    "        \n",
    "        validation_results['volume_access'] = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Volume access failed: {e}\")\n",
    "        validation_results['volume_access'] = False\n",
    "        validation_results['volume_error'] = str(e)\n",
    "\n",
    "validation_time = time.time() - validation_start_time\n",
    "serverless_results['environment_validation'] = validation_results\n",
    "serverless_results['performance_metrics']['validation_time'] = validation_time\n",
    "\n",
    "print(f\"‚è±Ô∏è Environment validation time: {validation_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyForge with Databricks Extension\n",
    "# =============================================================================\n",
    "# INSTALLATION SECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üì¶ Installing PyForge CLI with Databricks extension for serverless...\")\n",
    "\n",
    "install_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Install with no cache to ensure fresh installation\n",
    "    if PYFORGE_VERSION == \"latest\":\n",
    "        %pip install --no-cache-dir pyforge-cli[databricks]\n",
    "    else:\n",
    "        %pip install --no-cache-dir pyforge-cli[databricks]=={PYFORGE_VERSION}\n",
    "    \n",
    "    print(\"‚úÖ PyForge CLI with Databricks extension installed\")\n",
    "    \n",
    "    # Verify installation\n",
    "    import pyforge_cli\n",
    "    installed_version = pyforge_cli.__version__\n",
    "    print(f\"‚úÖ Installed version: {installed_version}\")\n",
    "    \n",
    "    serverless_results['installation'] = {\n",
    "        'success': True,\n",
    "        'version': installed_version\n",
    "    }\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"‚ùå Installation failed: {e}\")\n",
    "    serverless_results['installation'] = {\n",
    "        'success': False,\n",
    "        'error': str(e)\n",
    "    }\n",
    "    dbutils.notebook.exit(\"Installation failed\")\n",
    "\n",
    "install_time = time.time() - install_start_time\n",
    "serverless_results['performance_metrics']['install_time'] = install_time\n",
    "print(f\"‚è±Ô∏è Installation time: {install_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test PySpark Integration\n",
    "# =============================================================================\n",
    "# PYSPARK INTEGRATION TESTING SECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"‚ö° Testing PySpark integration for serverless optimization...\")\n",
    "\n",
    "pyspark_test_start = time.time()\n",
    "pyspark_results = {}\n",
    "\n",
    "try:\n",
    "    # Test Databricks extension discovery\n",
    "    from pyforge_cli.plugin_system import discovery\n",
    "    \n",
    "    plugin_discovery = discovery.PluginDiscovery()\n",
    "    extensions = plugin_discovery.discover_extensions()\n",
    "    \n",
    "    databricks_ext = None\n",
    "    for name, ext in extensions.items():\n",
    "        if 'databricks' in name.lower():\n",
    "            databricks_ext = ext\n",
    "            break\n",
    "    \n",
    "    if databricks_ext:\n",
    "        print(\"‚úÖ Databricks extension discovered\")\n",
    "        \n",
    "        # Test extension initialization\n",
    "        init_success = plugin_discovery.initialize_extensions()\n",
    "        databricks_init = init_success.get('databricks', False)\n",
    "        \n",
    "        if databricks_init:\n",
    "            print(\"‚úÖ Databricks extension initialized successfully\")\n",
    "            pyspark_results['extension_loaded'] = True\n",
    "            \n",
    "            # Test serverless detection within extension\n",
    "            try:\n",
    "                # This would test the actual extension API\n",
    "                print(\"üîç Testing serverless environment detection in extension...\")\n",
    "                \n",
    "                # Test if PySpark is preferred for serverless\n",
    "                if validation_results.get('is_serverless') and validation_results.get('pyspark_available'):\n",
    "                    print(\"‚úÖ Serverless + PySpark environment confirmed\")\n",
    "                    pyspark_results['optimal_environment'] = True\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è Non-optimal environment for PySpark\")\n",
    "                    pyspark_results['optimal_environment'] = False\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Serverless detection test failed: {e}\")\n",
    "                pyspark_results['detection_error'] = str(e)\n",
    "        else:\n",
    "            print(\"‚ùå Databricks extension initialization failed\")\n",
    "            pyspark_results['extension_loaded'] = False\n",
    "    else:\n",
    "        print(\"‚ùå Databricks extension not found\")\n",
    "        pyspark_results['extension_found'] = False\n",
    "    \n",
    "    # Test PySpark DataFrame operations\n",
    "    if validation_results.get('pyspark_available'):\n",
    "        print(\"üß™ Testing PySpark DataFrame operations...\")\n",
    "        \n",
    "        # Create test DataFrame\n",
    "        from pyspark.sql import Row\n",
    "        \n",
    "        test_data = [\n",
    "            Row(id=1, name=\"test1\", value=10.5),\n",
    "            Row(id=2, name=\"test2\", value=20.3),\n",
    "            Row(id=3, name=\"test3\", value=30.7)\n",
    "        ]\n",
    "        \n",
    "        test_df = spark.createDataFrame(test_data)\n",
    "        \n",
    "        # Test basic operations\n",
    "        row_count = test_df.count()\n",
    "        column_count = len(test_df.columns)\n",
    "        \n",
    "        print(f\"‚úÖ Test DataFrame created: {row_count} rows, {column_count} columns\")\n",
    "        \n",
    "        # Test write to volume (if accessible)\n",
    "        if validation_results.get('volume_access'):\n",
    "            test_output_path = f\"{TEST_OUTPUT_PATH}/pyspark_test.parquet\"\n",
    "            test_df.write.mode(\"overwrite\").parquet(test_output_path)\n",
    "            print(f\"‚úÖ Test DataFrame written to volume: {test_output_path}\")\n",
    "            \n",
    "            # Clean up\n",
    "            dbutils.fs.rm(test_output_path, recurse=True)\n",
    "        \n",
    "        pyspark_results['dataframe_operations'] = True\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"‚ùå PySpark integration test failed: {e}\")\n",
    "    pyspark_results['integration_error'] = str(e)\n",
    "\n",
    "pyspark_test_time = time.time() - pyspark_test_start\n",
    "serverless_results['pyspark_integration'] = pyspark_results\n",
    "serverless_results['performance_metrics']['pyspark_test_time'] = pyspark_test_time\n",
    "\n",
    "print(f\"‚è±Ô∏è PySpark integration test time: {pyspark_test_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Volume Operations\n",
    "# =============================================================================\n",
    "# VOLUME OPERATIONS TESTING SECTION\n",
    "# =============================================================================\n",
    "\n",
    "if TEST_VOLUMES:\n",
    "    print(\"üìÅ Testing Unity Catalog Volume operations...\")\n",
    "    \n",
    "    volume_test_start = time.time()\n",
    "    volume_results = {}\n",
    "    \n",
    "    try:\n",
    "        # Test sample datasets installation to volume\n",
    "        print(\"üì• Testing sample datasets installation to volume...\")\n",
    "        \n",
    "        # Check if sample datasets exist\n",
    "        try:\n",
    "            sample_files = dbutils.fs.ls(SAMPLE_DATA_PATH)\n",
    "            print(f\"‚úÖ Found {len(sample_files)} items in sample data path\")\n",
    "            volume_results['sample_data_available'] = True\n",
    "        except:\n",
    "            print(\"‚ö†Ô∏è Sample data not found - would need installation\")\n",
    "            volume_results['sample_data_available'] = False\n",
    "        \n",
    "        # Test creating output directory\n",
    "        try:\n",
    "            dbutils.fs.mkdirs(TEST_OUTPUT_PATH)\n",
    "            print(f\"‚úÖ Test output directory created: {TEST_OUTPUT_PATH}\")\n",
    "            volume_results['output_dir_created'] = True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to create output directory: {e}\")\n",
    "            volume_results['output_dir_created'] = False\n",
    "        \n",
    "        # Test file operations\n",
    "        try:\n",
    "            # Create test file\n",
    "            test_content = \"id,name,value\\n1,test1,10.5\\n2,test2,20.3\"\n",
    "            test_csv_path = f\"{TEST_OUTPUT_PATH}/test_data.csv\"\n",
    "            \n",
    "            dbutils.fs.put(test_csv_path, test_content)\n",
    "            print(f\"‚úÖ Test CSV file created: {test_csv_path}\")\n",
    "            \n",
    "            # Verify file exists and has content\n",
    "            file_info = dbutils.fs.ls(test_csv_path)\n",
    "            if file_info:\n",
    "                file_size = file_info[0].size\n",
    "                print(f\"‚úÖ Test file verified: {file_size} bytes\")\n",
    "                volume_results['file_operations'] = True\n",
    "            \n",
    "            # Test conversion using volume paths\n",
    "            if pyspark_results.get('extension_loaded'):\n",
    "                print(\"üîÑ Testing conversion with volume paths...\")\n",
    "                \n",
    "                # This would test the actual Databricks extension API\n",
    "                # For now, test PySpark DataFrame read\n",
    "                if validation_results.get('pyspark_available'):\n",
    "                    try:\n",
    "                        # Read CSV with PySpark\n",
    "                        df = spark.read.option(\"header\", \"true\").csv(test_csv_path)\n",
    "                        row_count = df.count()\n",
    "                        print(f\"‚úÖ Volume CSV read with PySpark: {row_count} rows\")\n",
    "                        \n",
    "                        # Write as Parquet\n",
    "                        parquet_path = f\"{TEST_OUTPUT_PATH}/test_data.parquet\"\n",
    "                        df.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "                        print(f\"‚úÖ Volume Parquet write successful: {parquet_path}\")\n",
    "                        \n",
    "                        volume_results['pyspark_conversion'] = True\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è PySpark conversion failed: {e}\")\n",
    "                        volume_results['pyspark_conversion'] = False\n",
    "            \n",
    "            # Clean up test files\n",
    "            dbutils.fs.rm(f\"{TEST_OUTPUT_PATH}/test_data.csv\")\n",
    "            if dbutils.fs.ls(f\"{TEST_OUTPUT_PATH}/test_data.parquet\"):\n",
    "                dbutils.fs.rm(f\"{TEST_OUTPUT_PATH}/test_data.parquet\", recurse=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå File operations failed: {e}\")\n",
    "            volume_results['file_operations'] = False\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Volume operations test failed: {e}\")\n",
    "        volume_results['test_error'] = str(e)\n",
    "    \n",
    "    volume_test_time = time.time() - volume_test_start\n",
    "    serverless_results['volume_operations'] = volume_results\n",
    "    serverless_results['performance_metrics']['volume_test_time'] = volume_test_time\n",
    "    \n",
    "    print(f\"‚è±Ô∏è Volume operations test time: {volume_test_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Volume testing skipped (disabled in configuration)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Testing for Serverless\n",
    "# =============================================================================\n",
    "# PERFORMANCE TESTING SECTION\n",
    "# =============================================================================\n",
    "\n",
    "if PERFORMANCE_TESTS:\n",
    "    print(\"üöÄ Running serverless performance tests...\")\n",
    "    \n",
    "    perf_test_start = time.time()\n",
    "    performance_results = {}\n",
    "    \n",
    "    try:\n",
    "        # Test data generation performance\n",
    "        print(\"üìä Testing data generation performance...\")\n",
    "        \n",
    "        if validation_results.get('pyspark_available'):\n",
    "            # Generate test data of different sizes\n",
    "            test_sizes = {\n",
    "                'small': 1000,\n",
    "                'medium': 10000,\n",
    "                'large': 100000\n",
    "            }\n",
    "            \n",
    "            current_test_size = test_sizes.get(TEST_SIZE, test_sizes['medium'])\n",
    "            \n",
    "            print(f\"üß™ Generating {TEST_SIZE} dataset ({current_test_size:,} rows)...\")\n",
    "            \n",
    "            # Generate test DataFrame\n",
    "            from pyspark.sql.functions import col, rand, round as spark_round\n",
    "            from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "            \n",
    "            # Create schema\n",
    "            schema = StructType([\n",
    "                StructField(\"id\", IntegerType(), True),\n",
    "                StructField(\"name\", StringType(), True),\n",
    "                StructField(\"value\", DoubleType(), True),\n",
    "                StructField(\"category\", StringType(), True)\n",
    "            ])\n",
    "            \n",
    "            # Generate data\n",
    "            data_gen_start = time.time()\n",
    "            \n",
    "            # Create range DataFrame and add columns\n",
    "            df = spark.range(current_test_size).toDF(\"id\")\n",
    "            df = df.withColumn(\"name\", concat(lit(\"test_\"), col(\"id\").cast(\"string\")))\n",
    "            df = df.withColumn(\"value\", spark_round(rand() * 100, 2))\n",
    "            df = df.withColumn(\"category\", \n",
    "                when(col(\"id\") % 4 == 0, \"A\")\n",
    "                .when(col(\"id\") % 4 == 1, \"B\")\n",
    "                .when(col(\"id\") % 4 == 2, \"C\")\n",
    "                .otherwise(\"D\")\n",
    "            )\n",
    "            \n",
    "            # Trigger computation\n",
    "            row_count = df.count()\n",
    "            data_gen_time = time.time() - data_gen_start\n",
    "            \n",
    "            print(f\"‚úÖ Generated {row_count:,} rows in {data_gen_time:.2f} seconds\")\n",
    "            print(f\"üìà Generation rate: {row_count/data_gen_time:,.0f} rows/second\")\n",
    "            \n",
    "            performance_results['data_generation'] = {\n",
    "                'rows': row_count,\n",
    "                'time': data_gen_time,\n",
    "                'rate': row_count / data_gen_time\n",
    "            }\n",
    "            \n",
    "            # Test conversion performance\n",
    "            if TEST_VOLUMES and volume_results.get('output_dir_created'):\n",
    "                print(\"‚ö° Testing conversion performance...\")\n",
    "                \n",
    "                # Test Parquet write performance\n",
    "                parquet_write_start = time.time()\n",
    "                test_parquet_path = f\"{TEST_OUTPUT_PATH}/performance_test_{TEST_SIZE}.parquet\"\n",
    "                \n",
    "                df.write.mode(\"overwrite\").parquet(test_parquet_path)\n",
    "                parquet_write_time = time.time() - parquet_write_start\n",
    "                \n",
    "                # Get file size\n",
    "                parquet_files = dbutils.fs.ls(test_parquet_path)\n",
    "                total_size = sum(f.size for f in parquet_files if f.name.endswith('.parquet'))\n",
    "                \n",
    "                print(f\"‚úÖ Parquet write: {total_size:,} bytes in {parquet_write_time:.2f} seconds\")\n",
    "                print(f\"üìä Write rate: {total_size/(parquet_write_time*1024*1024):.1f} MB/second\")\n",
    "                \n",
    "                # Test read performance\n",
    "                parquet_read_start = time.time()\n",
    "                read_df = spark.read.parquet(test_parquet_path)\n",
    "                read_count = read_df.count()\n",
    "                parquet_read_time = time.time() - parquet_read_start\n",
    "                \n",
    "                print(f\"‚úÖ Parquet read: {read_count:,} rows in {parquet_read_time:.2f} seconds\")\n",
    "                print(f\"üìà Read rate: {read_count/parquet_read_time:,.0f} rows/second\")\n",
    "                \n",
    "                performance_results['parquet_operations'] = {\n",
    "                    'write_time': parquet_write_time,\n",
    "                    'read_time': parquet_read_time,\n",
    "                    'file_size': total_size,\n",
    "                    'write_rate_mb_s': total_size / (parquet_write_time * 1024 * 1024),\n",
    "                    'read_rate_rows_s': read_count / parquet_read_time\n",
    "                }\n",
    "                \n",
    "                # Clean up\n",
    "                dbutils.fs.rm(test_parquet_path, recurse=True)\n",
    "        \n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è PySpark not available - skipping performance tests\")\n",
    "            performance_results['pyspark_unavailable'] = True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Performance testing failed: {e}\")\n",
    "        performance_results['test_error'] = str(e)\n",
    "    \n",
    "    perf_test_time = time.time() - perf_test_start\n",
    "    serverless_results['performance_metrics']['performance_test_time'] = perf_test_time\n",
    "    serverless_results['conversion_tests'] = performance_results\n",
    "    \n",
    "    print(f\"‚è±Ô∏è Performance testing time: {perf_test_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Performance testing skipped (disabled in configuration)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serverless Test Results Summary\n",
    "# =============================================================================\n",
    "# SERVERLESS RESULTS SUMMARY SECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìä Generating serverless test results summary...\")\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Add metadata\n",
    "serverless_results['metadata'] = {\n",
    "    'test_type': 'serverless_functional',\n",
    "    'notebook_name': '02-databricks-extension-serverless',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'configuration': {\n",
    "        'catalog_name': CATALOG_NAME,\n",
    "        'schema_name': SCHEMA_NAME,\n",
    "        'volume_name': VOLUME_NAME,\n",
    "        'pyforge_version': PYFORGE_VERSION,\n",
    "        'test_size': TEST_SIZE,\n",
    "        'pyspark_mode': PYSPARK_MODE,\n",
    "        'test_volumes': TEST_VOLUMES,\n",
    "        'performance_tests': PERFORMANCE_TESTS\n",
    "    }\n",
    "}\n",
    "\n",
    "# Calculate overall success\n",
    "critical_checks = [\n",
    "    serverless_results.get('environment_validation', {}).get('is_serverless', False),\n",
    "    serverless_results.get('environment_validation', {}).get('pyspark_available', False),\n",
    "    serverless_results.get('installation', {}).get('success', False),\n",
    "    serverless_results.get('pyspark_integration', {}).get('extension_loaded', False)\n",
    "]\n",
    "\n",
    "overall_success = all(critical_checks)\n",
    "serverless_results['overall_success'] = overall_success\n",
    "\n",
    "# Display comprehensive summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ DATABRICKS SERVERLESS EXTENSION TEST SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "status_icon = \"‚úÖ\" if overall_success else \"‚ùå\"\n",
    "print(f\"{status_icon} Overall Test Status: {'PASSED' if overall_success else 'FAILED'}\")\n",
    "print(f\"üïê Total Test Duration: {sum(serverless_results['performance_metrics'].values()):.2f} seconds\")\n",
    "print(f\"üè¢ Catalog: {CATALOG_NAME}.{SCHEMA_NAME}.{VOLUME_NAME}\")\n",
    "print(f\"üì¶ PyForge Version: {PYFORGE_VERSION}\")\n",
    "\n",
    "print(\"\\nüìã Environment Validation:\")\n",
    "env_val = serverless_results.get('environment_validation', {})\n",
    "print(f\"   Serverless Runtime: {'‚úÖ' if env_val.get('is_serverless') else '‚ùå'}\")\n",
    "print(f\"   PySpark Available: {'‚úÖ' if env_val.get('pyspark_available') else '‚ùå'}\")\n",
    "print(f\"   Volume Access: {'‚úÖ' if env_val.get('volume_access') else '‚ùå'}\")\n",
    "\n",
    "print(\"\\nüîå Extension Integration:\")\n",
    "pyspark_int = serverless_results.get('pyspark_integration', {})\n",
    "print(f\"   Extension Loaded: {'‚úÖ' if pyspark_int.get('extension_loaded') else '‚ùå'}\")\n",
    "print(f\"   Optimal Environment: {'‚úÖ' if pyspark_int.get('optimal_environment') else '‚ùå'}\")\n",
    "print(f\"   DataFrame Operations: {'‚úÖ' if pyspark_int.get('dataframe_operations') else '‚ùå'}\")\n",
    "\n",
    "if TEST_VOLUMES:\n",
    "    print(\"\\nüìÅ Volume Operations:\")\n",
    "    vol_ops = serverless_results.get('volume_operations', {})\n",
    "    print(f\"   Output Directory: {'‚úÖ' if vol_ops.get('output_dir_created') else '‚ùå'}\")\n",
    "    print(f\"   File Operations: {'‚úÖ' if vol_ops.get('file_operations') else '‚ùå'}\")\n",
    "    print(f\"   PySpark Conversion: {'‚úÖ' if vol_ops.get('pyspark_conversion') else '‚ùå'}\")\n",
    "\n",
    "if PERFORMANCE_TESTS:\n",
    "    print(\"\\nüöÄ Performance Results:\")\n",
    "    conv_tests = serverless_results.get('conversion_tests', {})\n",
    "    if 'data_generation' in conv_tests:\n",
    "        gen_perf = conv_tests['data_generation']\n",
    "        print(f\"   Data Generation: {gen_perf['rows']:,} rows at {gen_perf['rate']:,.0f} rows/sec\")\n",
    "    \n",
    "    if 'parquet_operations' in conv_tests:\n",
    "        parquet_perf = conv_tests['parquet_operations']\n",
    "        print(f\"   Parquet Write: {parquet_perf['write_rate_mb_s']:.1f} MB/sec\")\n",
    "        print(f\"   Parquet Read: {parquet_perf['read_rate_rows_s']:,.0f} rows/sec\")\n",
    "\n",
    "print(\"\\n‚è±Ô∏è Performance Metrics:\")\n",
    "for metric, value in serverless_results['performance_metrics'].items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"   {metric.replace('_', ' ').title()}: {value:.2f}s\")\n",
    "\n",
    "# Export results\n",
    "results_json = json.dumps(serverless_results, indent=2, default=str)\n",
    "print(\"\\nüíæ Serverless test results saved to serverless_results variable\")\n",
    "\n",
    "if VERBOSE_LOGGING:\n",
    "    print(\"\\nüìù Detailed Results:\")\n",
    "    print(results_json[:1500] + \"...\" if len(results_json) > 1500 else results_json)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üèÅ Serverless testing completed!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n   "language": "python",\n   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "version": "3.8.0"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}