{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyForge Notebook Integration - Complete Walkthrough\n",
    "## Demonstrating CLI Command Equivalents in Python Package Format\n",
    "\n",
    "This notebook demonstrates how PyForge CLI commands can be executed through a Python package interface, with intelligent Databricks/Serverless detection and optimized processing strategies.\n",
    "\n",
    "### Key Features:\n",
    "- **Environment Detection**: Automatically detects Databricks vs serverless environments\n",
    "- **Smart Format Routing**: Uses native Databricks capabilities for supported formats (CSV, JSON, XML, XLSX)\n",
    "- **Fallback Processing**: Uses PyForge converters for specialized formats (MDB, DBF, PDF)\n",
    "- **Unified API**: Same interface regardless of underlying processing engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyForge Notebook Integration (Future Package)\n",
    "# %pip install pyforge-notebook[databricks] --upgrade\n",
    "\n",
    "# For demonstration purposes, we'll simulate the package structure\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Simulate package imports (these would be real imports in production)\n",
    "print(\"üì¶ Installing PyForge Notebook Integration...\")\n",
    "print(\"‚úÖ Installation complete!\")\n",
    "print(\"\\nüîç Detecting execution environment...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Detection Simulation\n",
    "class EnvironmentDetector:\n",
    "    \"\"\"\n",
    "    Detects the execution environment and available capabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.environment = self._detect_environment()\n",
    "    \n",
    "    def _detect_environment(self):\n",
    "        env_info = {\n",
    "            'is_databricks': self._is_databricks(),\n",
    "            'spark_available': self._is_spark_available(),\n",
    "            'spark_mode': self._get_spark_mode(),\n",
    "            'runtime_version': self._get_databricks_runtime(),\n",
    "            'cluster_type': self._get_cluster_type(),\n",
    "            'processing_strategy': None\n",
    "        }\n",
    "        \n",
    "        # Determine optimal processing strategy\n",
    "        if env_info['is_databricks'] and env_info['spark_available']:\n",
    "            env_info['processing_strategy'] = 'databricks_optimized'\n",
    "        elif env_info['spark_available']:\n",
    "            env_info['processing_strategy'] = 'spark_local'\n",
    "        else:\n",
    "            env_info['processing_strategy'] = 'pandas_local'\n",
    "            \n",
    "        return env_info\n",
    "    \n",
    "    def _is_databricks(self):\n",
    "        return (\n",
    "            'DATABRICKS_RUNTIME_VERSION' in os.environ or\n",
    "            'SPARK_LOCAL_HOSTNAME' in os.environ or\n",
    "            self._check_databricks_imports()\n",
    "        )\n",
    "    \n",
    "    def _check_databricks_imports(self):\n",
    "        try:\n",
    "            import pyspark\n",
    "            from pyspark.sql import SparkSession\n",
    "            # Check if we can access dbutils (Databricks-specific)\n",
    "            spark = SparkSession.getActiveSession()\n",
    "            if spark and hasattr(spark, 'sparkContext'):\n",
    "                return 'databricks' in str(spark.sparkContext.getConf().getAll())\n",
    "        except ImportError:\n",
    "            pass\n",
    "        return False\n",
    "    \n",
    "    def _is_spark_available(self):\n",
    "        try:\n",
    "            import pyspark\n",
    "            from pyspark.sql import SparkSession\n",
    "            return SparkSession.getActiveSession() is not None\n",
    "        except ImportError:\n",
    "            return False\n",
    "    \n",
    "    def _get_spark_mode(self):\n",
    "        if not self._is_spark_available():\n",
    "            return None\n",
    "        try:\n",
    "            from pyspark.sql import SparkSession\n",
    "            spark = SparkSession.getActiveSession()\n",
    "            return spark.conf.get(\"spark.api.mode\", \"classic\")\n",
    "        except:\n",
    "            return \"classic\"\n",
    "    \n",
    "    def _get_databricks_runtime(self):\n",
    "        return os.environ.get('DATABRICKS_RUNTIME_VERSION', 'Not Databricks')\n",
    "    \n",
    "    def _get_cluster_type(self):\n",
    "        if not self._is_databricks():\n",
    "            return 'local'\n",
    "        # Simplified cluster type detection\n",
    "        return os.environ.get('CLUSTER_TYPE', 'standard')\n",
    "\n",
    "# Initialize environment detector\n",
    "env_detector = EnvironmentDetector()\n",
    "env = env_detector.environment\n",
    "\n",
    "print(\"Environment Detection Results:\")\n",
    "print(f\"üåê Environment Type: {'Databricks' if env['is_databricks'] else 'Local/Serverless'}\")\n",
    "print(f\"‚ö° Spark Available: {env['spark_available']}\")\n",
    "print(f\"üîß Spark Mode: {env['spark_mode'] or 'N/A'}\")\n",
    "print(f\"üè∑Ô∏è  Runtime Version: {env['runtime_version']}\")\n",
    "print(f\"üñ•Ô∏è  Cluster Type: {env['cluster_type']}\")\n",
    "print(f\"üéØ Processing Strategy: {env['processing_strategy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyForge Notebook Integration Class\n",
    "\n",
    "This simulates the main `PyForge` class that would be available in the `pyforge-notebook` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "class PyForgeNotebook:\n",
    "    \"\"\"\n",
    "    Main PyForge Notebook Integration Class\n",
    "    \n",
    "    Provides a unified interface for data conversion with intelligent\n",
    "    environment detection and optimization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, auto_detect_environment=True, **config):\n",
    "        self.env_detector = EnvironmentDetector() if auto_detect_environment else None\n",
    "        self.environment = self.env_detector.environment if self.env_detector else {}\n",
    "        self.config = config\n",
    "        self.last_conversion_metadata = None\n",
    "        self.conversion_stats = None\n",
    "        \n",
    "        # Initialize format processors\n",
    "        self._init_processors()\n",
    "        \n",
    "        print(f\"üöÄ PyForge initialized with {self.environment.get('processing_strategy', 'unknown')} strategy\")\n",
    "    \n",
    "    def _init_processors(self):\n",
    "        \"\"\"Initialize format-specific processors based on environment\"\"\"\n",
    "        strategy = self.environment.get('processing_strategy', 'pandas_local')\n",
    "        \n",
    "        if strategy == 'databricks_optimized':\n",
    "            self.processor = DatabricksOptimizedProcessor()\n",
    "        elif strategy == 'spark_local':\n",
    "            self.processor = SparkLocalProcessor()\n",
    "        else:\n",
    "            self.processor = PandasLocalProcessor()\n",
    "    \n",
    "    def convert(self, input_path, output_format=\"dataframe\", **options):\n",
    "        \"\"\"\n",
    "        Main conversion method - equivalent to CLI 'pyforge convert'\n",
    "        \n",
    "        Args:\n",
    "            input_path: Path to input file\n",
    "            output_format: 'dataframe', 'spark_dataframe', 'delta_table', or file extension\n",
    "            **options: Format-specific options\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame or path to output file\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Detect file format\n",
    "        file_ext = Path(input_path).suffix.lower()\n",
    "        \n",
    "        print(f\"üîÑ Converting {Path(input_path).name} ({file_ext}) to {output_format}...\")\n",
    "        \n",
    "        # Route to appropriate processor\n",
    "        result = self.processor.process_file(input_path, file_ext, output_format, **options)\n",
    "        \n",
    "        # Record conversion stats\n",
    "        end_time = datetime.now()\n",
    "        self.conversion_stats = {\n",
    "            'duration': (end_time - start_time).total_seconds(),\n",
    "            'input_file': input_path,\n",
    "            'output_format': output_format,\n",
    "            'processing_strategy': self.environment.get('processing_strategy'),\n",
    "            'success': True\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Conversion completed in {self.conversion_stats['duration']:.2f}s\")\n",
    "        return result\n",
    "    \n",
    "    def get_file_info(self, file_path):\n",
    "        \"\"\"Equivalent to CLI 'pyforge info'\"\"\"\n",
    "        path_obj = Path(file_path)\n",
    "        \n",
    "        # Simulate file analysis\n",
    "        info = {\n",
    "            'filename': path_obj.name,\n",
    "            'extension': path_obj.suffix,\n",
    "            'size_bytes': 'Unknown (simulated)',\n",
    "            'format_detected': path_obj.suffix.lstrip('.').upper(),\n",
    "            'supported': path_obj.suffix.lower() in ['.csv', '.xlsx', '.json', '.xml', '.pdf', '.mdb', '.dbf'],\n",
    "            'processing_engine': self._get_processing_engine(path_obj.suffix.lower()),\n",
    "            'estimated_memory_usage': 'Variable'\n",
    "        }\n",
    "        \n",
    "        return info\n",
    "    \n",
    "    def list_supported_formats(self):\n",
    "        \"\"\"Equivalent to CLI 'pyforge formats'\"\"\"\n",
    "        formats = [\n",
    "            {'input': 'CSV (.csv)', 'output': 'DataFrame/Parquet', 'engine': self._get_processing_engine('.csv')},\n",
    "            {'input': 'Excel (.xlsx)', 'output': 'DataFrame/Parquet', 'engine': self._get_processing_engine('.xlsx')},\n",
    "            {'input': 'JSON (.json)', 'output': 'DataFrame/Parquet', 'engine': self._get_processing_engine('.json')},\n",
    "            {'input': 'XML (.xml)', 'output': 'DataFrame/Parquet', 'engine': self._get_processing_engine('.xml')},\n",
    "            {'input': 'PDF (.pdf)', 'output': 'Text DataFrame', 'engine': self._get_processing_engine('.pdf')},\n",
    "            {'input': 'Access (.mdb)', 'output': 'DataFrame/Parquet', 'engine': self._get_processing_engine('.mdb')},\n",
    "            {'input': 'dBase (.dbf)', 'output': 'DataFrame/Parquet', 'engine': self._get_processing_engine('.dbf')}\n",
    "        ]\n",
    "        return formats\n",
    "    \n",
    "    def validate_file(self, file_path):\n",
    "        \"\"\"Equivalent to CLI 'pyforge validate'\"\"\"\n",
    "        path_obj = Path(file_path)\n",
    "        \n",
    "        errors = []\n",
    "        warnings = []\n",
    "        \n",
    "        # Basic validation\n",
    "        if not path_obj.suffix:\n",
    "            errors.append(\"File has no extension\")\n",
    "        \n",
    "        if path_obj.suffix.lower() not in ['.csv', '.xlsx', '.json', '.xml', '.pdf', '.mdb', '.dbf']:\n",
    "            warnings.append(f\"Format {path_obj.suffix} may not be fully supported\")\n",
    "        \n",
    "        status = \"valid\" if not errors else \"invalid\"\n",
    "        \n",
    "        return {\n",
    "            'status': status,\n",
    "            'errors': errors,\n",
    "            'warnings': warnings,\n",
    "            'can_process': len(errors) == 0\n",
    "        }\n",
    "    \n",
    "    def _get_processing_engine(self, file_ext):\n",
    "        \"\"\"Determine which processing engine would be used for a file format\"\"\"\n",
    "        strategy = self.environment.get('processing_strategy', 'pandas_local')\n",
    "        \n",
    "        # Native Databricks support\n",
    "        if strategy == 'databricks_optimized' and file_ext in ['.csv', '.json', '.xml', '.xlsx']:\n",
    "            return 'Databricks Native'\n",
    "        \n",
    "        # PyForge converters for specialized formats\n",
    "        if file_ext in ['.pdf', '.mdb', '.dbf']:\n",
    "            return 'PyForge Converter'\n",
    "        \n",
    "        # Default pandas processing\n",
    "        return 'Pandas/PyArrow'\n",
    "    \n",
    "    def get_environment_info(self):\n",
    "        \"\"\"Return detailed environment information\"\"\"\n",
    "        return {\n",
    "            'environment_type': 'Databricks' if self.environment.get('is_databricks') else 'Local/Serverless',\n",
    "            'spark_available': self.environment.get('spark_available', False),\n",
    "            'databricks_runtime': self.environment.get('runtime_version', 'N/A'),\n",
    "            'cluster_type': self.environment.get('cluster_type', 'local'),\n",
    "            'processing_strategy': self.environment.get('processing_strategy', 'pandas_local'),\n",
    "            'spark_mode': self.environment.get('spark_mode', 'N/A')\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ PyForgeNotebook class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Processing Engine Classes\n",
    "\n",
    "These classes demonstrate how different processing strategies would be implemented based on the detected environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseProcessor:\n",
    "    \"\"\"Base class for all processors\"\"\"\n",
    "    \n",
    "    def process_file(self, input_path, file_ext, output_format, **options):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class DatabricksOptimizedProcessor(BaseProcessor):\n",
    "    \"\"\"Processor optimized for Databricks environment\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Databricks Optimized\"\n",
    "        print(f\"üîß Initialized {self.name} processor\")\n",
    "    \n",
    "    def process_file(self, input_path, file_ext, output_format, **options):\n",
    "        if file_ext in ['.csv', '.json', '.xml', '.xlsx']:\n",
    "            return self._process_with_databricks_native(input_path, file_ext, output_format, **options)\n",
    "        else:\n",
    "            return self._process_with_pyforge_fallback(input_path, file_ext, output_format, **options)\n",
    "    \n",
    "    def _process_with_databricks_native(self, input_path, file_ext, output_format, **options):\n",
    "        print(f\"‚ö° Using Databricks native processing for {file_ext}\")\n",
    "        \n",
    "        # Simulate Spark DataFrame creation\n",
    "        sample_data = self._generate_sample_data(file_ext, input_path)\n",
    "        \n",
    "        if output_format == \"spark_dataframe\":\n",
    "            print(\"üìä Returning Spark DataFrame\")\n",
    "            return sample_data  # In reality, this would be a Spark DataFrame\n",
    "        else:\n",
    "            print(\"üêº Converting to Pandas DataFrame\")\n",
    "            return sample_data  # Convert Spark DF to Pandas\n",
    "    \n",
    "    def _process_with_pyforge_fallback(self, input_path, file_ext, output_format, **options):\n",
    "        print(f\"üîÑ Using PyForge converter for {file_ext} (specialized format)\")\n",
    "        return self._generate_sample_data(file_ext, input_path)\n",
    "    \n",
    "    def _generate_sample_data(self, file_ext, input_path):\n",
    "        \"\"\"Generate sample data based on file type\"\"\"\n",
    "        if file_ext == '.csv':\n",
    "            return pd.DataFrame({\n",
    "                'id': [1, 2, 3, 4, 5],\n",
    "                'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "                'value': [10.5, 20.3, 15.7, 30.1, 25.9],\n",
    "                'date': pd.date_range('2024-01-01', periods=5)\n",
    "            })\n",
    "        elif file_ext == '.xlsx':\n",
    "            return pd.DataFrame({\n",
    "                'quarter': ['Q1', 'Q2', 'Q3', 'Q4'],\n",
    "                'revenue': [100000, 120000, 110000, 130000],\n",
    "                'expenses': [80000, 85000, 82000, 90000],\n",
    "                'profit': [20000, 35000, 28000, 40000]\n",
    "            })\n",
    "        elif file_ext == '.json':\n",
    "            return pd.DataFrame({\n",
    "                'user_id': [101, 102, 103],\n",
    "                'username': ['user1', 'user2', 'user3'],\n",
    "                'settings': ['{\"theme\": \"dark\"}', '{\"theme\": \"light\"}', '{\"theme\": \"auto\"}']\n",
    "            })\n",
    "        elif file_ext == '.xml':\n",
    "            return pd.DataFrame({\n",
    "                'product_id': ['P001', 'P002', 'P003'],\n",
    "                'product_name': ['Widget A', 'Widget B', 'Widget C'],\n",
    "                'category': ['Electronics', 'Home', 'Electronics'],\n",
    "                'price': [29.99, 45.50, 15.75]\n",
    "            })\n",
    "        elif file_ext == '.pdf':\n",
    "            return pd.DataFrame({\n",
    "                'page_number': [1, 2, 3],\n",
    "                'text_content': [\n",
    "                    'This is the content of page 1...',\n",
    "                    'This is the content of page 2...',\n",
    "                    'This is the content of page 3...'\n",
    "                ]\n",
    "            })\n",
    "        else:\n",
    "            return pd.DataFrame({'data': ['Sample data from ' + str(input_path)]})\n",
    "\n",
    "class SparkLocalProcessor(BaseProcessor):\n",
    "    \"\"\"Processor for local Spark environment\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Spark Local\"\n",
    "        print(f\"üîß Initialized {self.name} processor\")\n",
    "    \n",
    "    def process_file(self, input_path, file_ext, output_format, **options):\n",
    "        print(f\"‚ö° Using local Spark processing for {file_ext}\")\n",
    "        # Use similar logic as Databricks but without Databricks-specific optimizations\n",
    "        return DatabricksOptimizedProcessor()._generate_sample_data(file_ext, input_path)\n",
    "\n",
    "class PandasLocalProcessor(BaseProcessor):\n",
    "    \"\"\"Processor for local pandas environment\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Pandas Local\"\n",
    "        print(f\"üîß Initialized {self.name} processor\")\n",
    "    \n",
    "    def process_file(self, input_path, file_ext, output_format, **options):\n",
    "        print(f\"üêº Using pandas processing for {file_ext}\")\n",
    "        return DatabricksOptimizedProcessor()._generate_sample_data(file_ext, input_path)\n",
    "\n",
    "print(\"‚úÖ Processing engine classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize PyForge and Demonstrate CLI Command Equivalents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyForge with automatic environment detection\n",
    "forge = PyForgeNotebook(auto_detect_environment=True)\n",
    "\n",
    "# Display environment information\n",
    "env_info = forge.get_environment_info()\n",
    "print(\"\\nüåç Environment Information:\")\n",
    "for key, value in env_info.items():\n",
    "    print(f\"  {key.replace('_', ' ').title()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CLI Command Equivalents\n",
    "\n",
    "### 5.1 `pyforge info` equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent to: pyforge info data.xlsx\n",
    "print(\"üìã File Information Analysis (equivalent to 'pyforge info'):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_files = [\n",
    "    \"/path/to/sales_data.xlsx\",\n",
    "    \"/path/to/customer_data.csv\",\n",
    "    \"/path/to/report.pdf\",\n",
    "    \"/path/to/legacy_database.mdb\"\n",
    "]\n",
    "\n",
    "for file_path in test_files:\n",
    "    file_info = forge.get_file_info(file_path)\n",
    "    print(f\"\\nüìÑ {file_info['filename']}\")\n",
    "    print(f\"   Format: {file_info['format_detected']}\")\n",
    "    print(f\"   Supported: {'‚úÖ' if file_info['supported'] else '‚ùå'}\")\n",
    "    print(f\"   Processing Engine: {file_info['processing_engine']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 `pyforge formats` equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent to: pyforge formats\n",
    "print(\"\\nüìä Supported Formats (equivalent to 'pyforge formats'):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "supported_formats = forge.list_supported_formats()\n",
    "for fmt in supported_formats:\n",
    "    print(f\"  {fmt['input']:<20} ‚Üí {fmt['output']:<25} [{fmt['engine']}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 `pyforge validate` equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent to: pyforge validate\n",
    "print(\"\\nüîç File Validation (equivalent to 'pyforge validate'):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "validation_files = [\n",
    "    \"/path/to/valid_data.csv\",\n",
    "    \"/path/to/no_extension\",\n",
    "    \"/path/to/unsupported.xyz\"\n",
    "]\n",
    "\n",
    "for file_path in validation_files:\n",
    "    result = forge.validate_file(file_path)\n",
    "    status_icon = \"‚úÖ\" if result['status'] == 'valid' else \"‚ùå\"\n",
    "    print(f\"\\n{status_icon} {Path(file_path).name}: {result['status'].upper()}\")\n",
    "    \n",
    "    if result['errors']:\n",
    "        for error in result['errors']:\n",
    "            print(f\"   ‚ùå Error: {error}\")\n",
    "    \n",
    "    if result['warnings']:\n",
    "        for warning in result['warnings']:\n",
    "            print(f\"   ‚ö†Ô∏è  Warning: {warning}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Conversion Examples\n",
    "\n",
    "### 6.1 CSV Processing with Environment-Specific Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent to: pyforge convert data.csv output.parquet\n",
    "print(\"\\nüîÑ CSV Conversion Example:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "csv_df = forge.convert(\n",
    "    input_path=\"/path/to/sales_data.csv\",\n",
    "    output_format=\"dataframe\"\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Converted CSV Data (Shape: {csv_df.shape}):\")\n",
    "print(csv_df.head())\n",
    "print(f\"\\nüìà Data Types:\")\n",
    "print(csv_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Excel Multi-Sheet Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent to: pyforge convert financial_report.xlsx output.parquet --combine-sheets\n",
    "print(\"\\nüìà Excel Multi-Sheet Conversion:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "excel_df = forge.convert(\n",
    "    input_path=\"/path/to/financial_report.xlsx\",\n",
    "    output_format=\"dataframe\",\n",
    "    excel_options={\n",
    "        'combine_sheets': True,\n",
    "        'sheet_matching_strategy': 'column_signature'\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Converted Excel Data (Shape: {excel_df.shape}):\")\n",
    "print(excel_df.head())\n",
    "\n",
    "# Show conversion metadata (simulated)\n",
    "print(\"\\nüìã Sheet Processing Summary:\")\n",
    "print(\"  - Q1 Data: 1,000 rows processed\")\n",
    "print(\"  - Q2 Data: 1,200 rows processed\")\n",
    "print(\"  - Q3 Data: 950 rows processed\")\n",
    "print(\"  - Q4 Data: 1,100 rows processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 JSON Processing with Nested Structure Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent to: pyforge convert api_data.json output.parquet\n",
    "print(\"\\nüîó JSON Conversion with Nested Structures:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "json_df = forge.convert(\n",
    "    input_path=\"/path/to/api_data.json\",\n",
    "    output_format=\"dataframe\",\n",
    "    json_options={\n",
    "        'flatten_nested': True,\n",
    "        'normalize_arrays': True\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Converted JSON Data (Shape: {json_df.shape}):\")\n",
    "print(json_df.head())\n",
    "print(\"\\nüîç JSON flattening automatically handled nested objects and arrays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 XML Processing with Hierarchical Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent to: pyforge convert catalog.xml output.parquet --flatten-nested\n",
    "print(\"\\nüå≥ XML Hierarchical Data Processing:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "xml_df = forge.convert(\n",
    "    input_path=\"/path/to/product_catalog.xml\",\n",
    "    output_format=\"dataframe\",\n",
    "    xml_options={\n",
    "        'flatten_nested': True,\n",
    "        'array_detection': True,\n",
    "        'preserve_attributes': True\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Converted XML Data (Shape: {xml_df.shape}):\")\n",
    "print(xml_df.head())\n",
    "print(\"\\nüåø XML structure automatically analyzed and flattened\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 PDF Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent to: pyforge convert document.pdf output.txt --pages 1-5\n",
    "print(\"\\nüìÑ PDF Text Extraction:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "pdf_df = forge.convert(\n",
    "    input_path=\"/path/to/annual_report.pdf\",\n",
    "    output_format=\"dataframe\",\n",
    "    pdf_options={\n",
    "        'page_range': '1-5',\n",
    "        'extract_metadata': True,\n",
    "        'preserve_formatting': False\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Extracted PDF Text (Shape: {pdf_df.shape}):\")\n",
    "print(pdf_df.head())\n",
    "print(\"\\nüìñ PDF text extraction with page-level granularity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Features\n",
    "\n",
    "### 7.1 Batch Processing Multiple Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing equivalent (not available in CLI but useful for notebooks)\n",
    "print(\"\\nüîÑ Batch Processing Multiple Files:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "batch_files = [\n",
    "    \"/path/to/sales_q1.xlsx\",\n",
    "    \"/path/to/sales_q2.xlsx\", \n",
    "    \"/path/to/sales_q3.xlsx\",\n",
    "    \"/path/to/sales_q4.xlsx\"\n",
    "]\n",
    "\n",
    "batch_results = []\n",
    "for file_path in batch_files:\n",
    "    print(f\"\\nüîÑ Processing {Path(file_path).name}...\")\n",
    "    df = forge.convert(file_path, output_format=\"dataframe\")\n",
    "    batch_results.append({\n",
    "        'file': Path(file_path).name,\n",
    "        'rows': len(df),\n",
    "        'columns': len(df.columns),\n",
    "        'dataframe': df\n",
    "    })\n",
    "\n",
    "print(\"\\nüìä Batch Processing Summary:\")\n",
    "for result in batch_results:\n",
    "    print(f\"  {result['file']}: {result['rows']} rows, {result['columns']} columns\")\n",
    "\n",
    "# Combine all quarters\n",
    "combined_df = pd.concat([r['dataframe'] for r in batch_results], ignore_index=True)\n",
    "print(f\"\\nüîó Combined Dataset: {combined_df.shape[0]} total rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Environment-Specific Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚ö° Environment-Specific Processing Demonstration:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Demonstrate different processing strategies based on environment\n",
    "test_file = \"/path/to/large_dataset.csv\"\n",
    "\n",
    "if forge.environment.get('is_databricks'):\n",
    "    print(\"üè¢ Databricks Environment Detected:\")\n",
    "    print(\"  ‚úÖ Using Spark's distributed CSV reader\")\n",
    "    print(\"  ‚úÖ Leveraging cluster compute resources\")\n",
    "    print(\"  ‚úÖ Automatic partitioning and optimization\")\n",
    "    \n",
    "    # Simulate Databricks-specific processing\n",
    "    df = forge.convert(\n",
    "        test_file,\n",
    "        output_format=\"spark_dataframe\",\n",
    "        databricks_options={\n",
    "            'adaptive_query_execution': True,\n",
    "            'columnar_cache': True,\n",
    "            'partition_strategy': 'auto'\n",
    "        }\n",
    "    )\n",
    "    print(\"  üìä Result: Spark DataFrame optimized for distributed processing\")\n",
    "    \n",
    "elif forge.environment.get('spark_available'):\n",
    "    print(\"‚ö° Local Spark Environment Detected:\")\n",
    "    print(\"  ‚úÖ Using local Spark session\")\n",
    "    print(\"  ‚úÖ Memory-optimized processing\")\n",
    "    \n",
    "    df = forge.convert(test_file, output_format=\"spark_dataframe\")\n",
    "    print(\"  üìä Result: Local Spark DataFrame\")\n",
    "    \n",
    "else:\n",
    "    print(\"üêº Pandas Environment Detected:\")\n",
    "    print(\"  ‚úÖ Using pandas with chunked reading for large files\")\n",
    "    print(\"  ‚úÖ Memory-efficient processing\")\n",
    "    \n",
    "    df = forge.convert(\n",
    "        test_file,\n",
    "        output_format=\"dataframe\",\n",
    "        pandas_options={\n",
    "            'chunksize': 10000,\n",
    "            'low_memory': True\n",
    "        }\n",
    "    )\n",
    "    print(\"  üìä Result: Pandas DataFrame with memory optimization\")\n",
    "\n",
    "print(f\"\\nüìà Processing completed using {forge.environment.get('processing_strategy')} strategy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Format Detection and Automatic Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç Automatic Format Detection and Processing Route Selection:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Demonstrate how different formats are routed to appropriate processors\n",
    "test_formats = [\n",
    "    (\"/path/to/data.csv\", \"Native processing in Databricks/Spark\"),\n",
    "    (\"/path/to/spreadsheet.xlsx\", \"Hybrid processing (PyForge + Spark output)\"),\n",
    "    (\"/path/to/config.json\", \"Native JSON processing\"),\n",
    "    (\"/path/to/catalog.xml\", \"Native XML processing with flattening\"),\n",
    "    (\"/path/to/report.pdf\", \"PyForge specialized converter\"),\n",
    "    (\"/path/to/legacy.mdb\", \"PyForge specialized converter\"),\n",
    "    (\"/path/to/old_data.dbf\", \"PyForge specialized converter\")\n",
    "]\n",
    "\n",
    "for file_path, expected_processing in test_formats:\n",
    "    file_ext = Path(file_path).suffix.lower()\n",
    "    processing_engine = forge._get_processing_engine(file_ext)\n",
    "    \n",
    "    print(f\"\\nüìÑ {Path(file_path).name}\")\n",
    "    print(f\"   Extension: {file_ext}\")\n",
    "    print(f\"   Engine: {processing_engine}\")\n",
    "    print(f\"   Strategy: {expected_processing}\")\n",
    "    \n",
    "    # Show appropriate processing symbols\n",
    "    if \"Native\" in processing_engine:\n",
    "        print(\"   üöÄ Optimized for distributed processing\")\n",
    "    elif \"PyForge\" in processing_engine:\n",
    "        print(\"   üîß Specialized format converter\")\n",
    "    else:\n",
    "        print(\"   üêº Standard pandas processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Performance Statistics and Conversion Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display last conversion statistics\n",
    "if forge.conversion_stats:\n",
    "    stats = forge.conversion_stats\n",
    "    print(f\"\\n‚è±Ô∏è  Last Conversion Performance:\")\n",
    "    print(f\"   Input File: {Path(stats['input_file']).name}\")\n",
    "    print(f\"   Output Format: {stats['output_format']}\")\n",
    "    print(f\"   Processing Strategy: {stats['processing_strategy']}\")\n",
    "    print(f\"   Duration: {stats['duration']:.2f} seconds\")\n",
    "    print(f\"   Status: {'‚úÖ Success' if stats['success'] else '‚ùå Failed'}\")\n",
    "\n",
    "# Environment capabilities summary\n",
    "print(f\"\\nüåç Environment Capabilities Summary:\")\n",
    "capabilities = {\n",
    "    'Distributed Processing': forge.environment.get('spark_available', False),\n",
    "    'Databricks Native': forge.environment.get('is_databricks', False),\n",
    "    'Delta Lake Support': forge.environment.get('is_databricks', False),\n",
    "    'Large File Optimization': True,\n",
    "    'Batch Processing': True,\n",
    "    'Progress Tracking': True\n",
    "}\n",
    "\n",
    "for capability, available in capabilities.items():\n",
    "    status = \"‚úÖ\" if available else \"‚ö†Ô∏è\"\n",
    "    print(f\"   {status} {capability}\")\n",
    "\n",
    "print(f\"\\nüéØ Optimal Use Cases for Current Environment:\")\n",
    "if forge.environment.get('is_databricks'):\n",
    "    print(\"   ‚Ä¢ Large-scale data processing (GB-TB range)\")\n",
    "    print(\"   ‚Ä¢ Multi-format data ingestion pipelines\")\n",
    "    print(\"   ‚Ä¢ Real-time data transformation workflows\")\n",
    "    print(\"   ‚Ä¢ Delta Lake integration for data lakes\")\n",
    "elif forge.environment.get('spark_available'):\n",
    "    print(\"   ‚Ä¢ Medium-scale data processing (MB-GB range)\")\n",
    "    print(\"   ‚Ä¢ Local distributed processing\")\n",
    "    print(\"   ‚Ä¢ Development and testing workflows\")\n",
    "else:\n",
    "    print(\"   ‚Ä¢ Small to medium datasets (MB range)\")\n",
    "    print(\"   ‚Ä¢ Quick data exploration and analysis\")\n",
    "    print(\"   ‚Ä¢ Specialized format conversion\")\n",
    "    print(\"   ‚Ä¢ Prototype and development work\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Migration Guide: CLI to Notebook\n",
    "\n",
    "### Common CLI Commands and Their Notebook Equivalents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîÑ CLI to Notebook Migration Guide:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "migration_examples = [\n",
    "    {\n",
    "        'cli': 'pyforge convert data.csv output.parquet',\n",
    "        'notebook': 'df = forge.convert(\"data.csv\", output_format=\"dataframe\")',\n",
    "        'description': 'Basic file conversion'\n",
    "    },\n",
    "    {\n",
    "        'cli': 'pyforge info document.pdf',\n",
    "        'notebook': 'info = forge.get_file_info(\"document.pdf\")',\n",
    "        'description': 'File information and metadata'\n",
    "    },\n",
    "    {\n",
    "        'cli': 'pyforge formats',\n",
    "        'notebook': 'formats = forge.list_supported_formats()',\n",
    "        'description': 'List supported formats'\n",
    "    },\n",
    "    {\n",
    "        'cli': 'pyforge validate data.xlsx',\n",
    "        'notebook': 'result = forge.validate_file(\"data.xlsx\")',\n",
    "        'description': 'File validation'\n",
    "    },\n",
    "    {\n",
    "        'cli': 'pyforge convert report.pdf --pages 1-10',\n",
    "        'notebook': 'df = forge.convert(\"report.pdf\", pdf_options={\"page_range\": \"1-10\"})',\n",
    "        'description': 'PDF with page range'\n",
    "    },\n",
    "    {\n",
    "        'cli': 'pyforge convert data.xlsx --combine-sheets',\n",
    "        'notebook': 'df = forge.convert(\"data.xlsx\", excel_options={\"combine_sheets\": True})',\n",
    "        'description': 'Excel multi-sheet processing'\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, example in enumerate(migration_examples, 1):\n",
    "    print(f\"\\n{i}. {example['description']}:\")\n",
    "    print(f\"   CLI: {example['cli']}\")\n",
    "    print(f\"   Notebook: {example['notebook']}\")\n",
    "\n",
    "print(\"\\n‚ú® Additional Notebook-Specific Benefits:\")\n",
    "notebook_benefits = [\n",
    "    \"Direct DataFrame manipulation and analysis\",\n",
    "    \"Interactive data exploration with display()\",\n",
    "    \"Seamless integration with visualization libraries\",\n",
    "    \"Batch processing capabilities\",\n",
    "    \"Environment-aware optimizations\",\n",
    "    \"Progress tracking and real-time feedback\",\n",
    "    \"Memory-efficient processing strategies\"\n",
    "]\n",
    "\n",
    "for benefit in notebook_benefits:\n",
    "    print(f\"   ‚úÖ {benefit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ PyForge Notebook Integration Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüèóÔ∏è Architecture Highlights:\")\n",
    "print(\"   ‚úÖ Intelligent environment detection (Databricks vs local)\")\n",
    "print(\"   ‚úÖ Format-specific processing optimization\")\n",
    "print(\"   ‚úÖ Unified API regardless of backend\")\n",
    "print(\"   ‚úÖ Seamless CLI command equivalents\")\n",
    "print(\"   ‚úÖ Native Databricks integration for supported formats\")\n",
    "print(\"   ‚úÖ PyForge fallback for specialized formats\")\n",
    "\n",
    "print(\"\\nüìä Supported Workflows:\")\n",
    "print(\"   üîÑ Single file conversions\")\n",
    "print(\"   üì¶ Batch processing\")\n",
    "print(\"   üîç File analysis and validation\")\n",
    "print(\"   üìà Performance monitoring\")\n",
    "print(\"   üéõÔ∏è Environment-specific optimizations\")\n",
    "\n",
    "print(\"\\nüöÄ Implementation Roadmap:\")\n",
    "roadmap_phases = [\n",
    "    \"Phase 1: Core integration and environment detection\",\n",
    "    \"Phase 2: Databricks optimizations and native processing\",\n",
    "    \"Phase 3: Advanced features and magic commands\",\n",
    "    \"Phase 4: Production hardening and performance tuning\"\n",
    "]\n",
    "\n",
    "for phase in roadmap_phases:\n",
    "    print(f\"   üìã {phase}\")\n",
    "\n",
    "print(\"\\nüí° Key Benefits:\")\n",
    "print(\"   ‚Ä¢ Preserve PyForge's sophisticated conversion algorithms\")\n",
    "print(\"   ‚Ä¢ Leverage Databricks native capabilities where beneficial\")\n",
    "print(\"   ‚Ä¢ Provide seamless notebook integration\")\n",
    "print(\"   ‚Ä¢ Maintain performance across different environments\")\n",
    "print(\"   ‚Ä¢ Enable data scientists to focus on analysis, not conversion\")\n",
    "\n",
    "print(\"\\nüéâ Ready for production implementation!\")\n",
    "print(\"   This architecture provides a solid foundation for\")\n",
    "print(\"   bringing PyForge's powerful conversion capabilities\")\n",
    "print(\"   directly into notebook workflows with intelligent\")\n",
    "print(\"   optimization based on the execution environment.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}